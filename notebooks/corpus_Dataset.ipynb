{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa835299-8f61-4a58-a85e-cd9157614e5b",
   "metadata": {},
   "source": [
    "# Présentation du Notebook\n",
    "#### Ce notebook est un exemple de manipulation du corpus avec Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84ee03bb-3e0d-4643-8699-67cbda58abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e752d2b-2f81-4b02-9950-f7d88b0efd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du fichier TSV\n",
    "chemin_fichier = '../data/clean/donnees.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4850ec5-7e1f-4c86-b0fa-16d290e5665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset à partir du fichier TSV\n",
    "mon_dataset = load_dataset('csv', data_files=chemin_fichier, delimiter='\\t')\n",
    "#mon_dataset = load_dataset('csv', data_files=chemin_fichier, delimiter='\\t', split='train')\n",
    "\n",
    "# Charger le dataset à partir du fichier TSV\n",
    "#mon_dataset = load_dataset('tsv', data_files=chemin_fichier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7bebf3-df19-49c9-b674-4ce745014a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'language_code'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Accéder au dataset associé à la clé 'train'\n",
    "dataset_train = mon_dataset['train']\n",
    "\n",
    "#dataset_train = mon_dataset\n",
    "\n",
    "# Sélectionner les 5 premières lignes du dataset 'train'\n",
    "premieres_lignes_train = dataset_train.select(range(5))\n",
    "\n",
    "# Afficher les premières lignes du dataset 'train'\n",
    "print(premieres_lignes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd427f63-428c-419d-80e3-dda1c1e0b5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    \"เอนโทรปีของข้อมูล - วิกิพีเดียข้ามไปเนื้อหาเมนูหลักเมนูหลักย้ายเมนูไปที่แถบด้านข้างซ่อนการนำทางหน้าหลักถามคำถามเหตุการณ์ปัจจุบันสุ่มบทความเกี่ยวกับวิกิพีเดียติดต่อเราบริจาคให้วิกิพีเดียมีส่วนร่วมคำอธิบายเริ่มต้นเขียนศาลาประชาคมปรับปรุงล่าสุดดิสคอร์ดค้นหาค้นหาสร้างบัญชีเข้าสู่ระบบเครื่องมือส่วนตัวสร้างบัญชีเข้าสู่ระบบหน้าสำหรับผู้แก้ไขที่ออกจากระบบเรียนรู้เพิ่มเติมส่วนร่วมคุยสารบัญย้ายเมนูไปที่แถบด้านข้างซ่อนบทนำ1นิยามทางการ2ที่มาของสมการเอนโทรปีToggle the table of contentsเอนโทรปีของข้อมูล45 ภาษาAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaTürkçeУкраїнськаاردوTiếng Việt中文粵語แก้ไขลิงก์บทความอภิปรายไทยอ่านแก้ไขดูประวัติเครื่องมือเครื่องมือย้ายเมนูไปที่แถบด้านข้างซ่อนการกระทำอ่านแก้ไขดูประวัติทั่วไปหน้าที่ลิงก์มาการเปลี่ยนแปลงที่เกี่ยวโยงอัปโหลดไฟล์หน้าพิเศษลิงก์ถาวรสารสนเทศหน้าอ้างอิงบทความนี้รับยูอาร์แอลแบบสั้นดาวน์โหลดคิวอาร์โค้ดสิ่งนี้ในวิกิสนเทศแก้ไขลิงก์ข้ามภาษาพิมพ์/ส่งออกสร้างหนังสือดาวน์โหลดเป็น PDFรุ่นพร้อมพิมพ์ในโครงการอื่นวิกิมีเดียคอมมอนส์จากวิกิพีเดีย สารานุกรมเสรีบทความนี้ไม่มีการอ้างอิงจากแหล่งที่มาใดกรุณาช่วยปรับปรุงบทความนี้โดยเพิ่มการอ้างอิงแหล่งที่มาที่น่าเชื่อถือ เนื้อความที่ไม่มีแหล่งที่มาอาจถูกคัดค้านหรือลบออก(เรียนรู้ว่าจะนำสารแม่แบบนี้ออกได้อย่างไรและเมื่อไร)เอนโทรปีของการทดลองแบร์นูลีซึ่งเป็นฟังก์ชันของโอกาสสำเร็จในทฤษฎีข้อมูลเอนโทรปีของข้อมูลคือ เป็นลักษณะที่บ่งชี้ระดับการสุ่มของสัญญาณหรือเหตุการณ์สุ่ม ว่ามีมากน้อยเพียงใด หรือเราอาจมองอีกมุมหนึ่งว่าเป็นตัวบ่งบอกว่าสัญญาณอันหนึ่งบรรจุข้อมูลอยู่เท่าไรเอนโทรปีเป็นแนวคิดของเทอร์โมไดนามิคส์กลศาสตร์ทางสถิติและทฤษฎีข้อมูลแนวคิดของเอนโทรปีกับเรื่องของข้อมูลมีความเกี่ยวพันกันอย่างมาก อย่างไรก็ตามกว่าที่กลศาสตร์ทางสถิติและทฤษฎีข้อมูล จะพัฒนามาจนความสัมพันธ์นี้ปรากฏขึ้น ก็ใช้เวลานานทีเดียว บทความนี้เป็นบทความเกี่ยวกับเอนโทรปีของข้อมูล (กฎเกณฑ์ของเอนโทรปีที่เกี่ยวข้องกับข้อมูลเชิงทฤษฎี)ตัวอย่างเช่น พิจารณาข้อความในภาษาไทย ซึ่งประกอบด้วยตัวอักษรและเครื่องหมายต่าง ๆ (ซึ่งสัญญาณของเราในที่นี้ ก็คือลำดับของตัวอักษรและเครื่องหมายนั่นเอง) สังเกตว่าตัวอักษรบางตัวมีโอกาสปรากฏขึ้นมาน้อยมาก (เช่น ฮ) แต่บางตัวกลับปรากฏบ่อยมาก (เช่น อ) ดังนั้นข้อความภาษาไทยนั้นก็ไม่ได้เรียกว่าสุ่มซะทีเดียว (ถ้าสุ่มจริง ข้อความน่าจะออกมาเป็นคำมั่ว ๆ อ่านไม่ได้ใจความ) อย่างไรก็ตาม ถ้าเราได้คำชุดหนึ่งมา เราก็ไม่อาจคาดเดาได้ว่าคำต่อไปเป็นคำว่าอะไร แสดงว่ามันก็มี'ความสุ่ม'อยู่บ้าง ไม่ได้เที่ยงแท้ซะทีเดียว เอนโทรปีก็คือการวัดระดับความสุ่มนี้นั่นเอง โดยกำเนิดมาจากผลงานของคลาวด์ อี แชนนอนในปีพ.ศ.\",\n",
      "    \"2491(ค.ศ.\",\n",
      "    \"1948) ชื่อA Mathematical Theory of Communication[1]เก็บถาวร1998-01-31 ที่เวย์แบ็กแมชชีนแชนนอนสร้างบทนิยามของเอนโทรปีขึ้นจากข้อสมมติฐานว่าค่านี้จะต้องมีสัดส่วน (ที่ต่อเนื่อง) นั่นคือ หากเปลี่ยนค่าของความน่าจะเป็นอันหนึ่งเพียงเล็กน้อย ค่าเอนโทรปีก็ควรเปลี่ยนเพียงเล็กน้อยเช่นกันหากผลลัพธ์ (เช่น ตัวอักษรและเครื่องหมายในตัวอย่างข้างต้น) ทุกอันมีโอกาสเกิดเท่า ๆ กันแล้ว การเพิ่มจำนวนตัวอักษร (และเครื่องหมาย) จะต้องทำให้ค่าเอนโทรปีเพิ่มขึ้นด้วยเสมอเราต้องสามารถใช้วิธีเลือกผลลัพธ์ (ตัวอักษร) โดยทำเป็นสองขั้นตอน และในกรณีนี้ ค่าเอนโทรปีของผลลัพธ์สุดท้ายต้องเท่ากับเอนโทรปีของทั้งสองขั้นตอนบวกกัน (โดยมีการถ่วงน้ำหนัก)นิยามทางการ[แก้]สำหรับเหตุการณ์สุ่มแบบเต็มหน่วย x ซึ่งสมมุติให้มีสถานะ 1..n,คล็อด อี แชนนอนนิยามเอนโทรปีในเทอมของ x ว่าH(x)=∑i=1np(i)log2⁡(1p(i))=−∑i=1np(i)log2⁡p(i).^p(i)\\log _\\left(}\\right)=-\\sum _^p(i)\\log _p(i).\\,\\!\",\n",
      "    \"}นั่นคือ เอนโทรปีของเหตุการณ์ x คือ ผลรวม (บนทุกๆผลลัพธ์ i ที่เป็นไปได้) ของผลคูณของความน่าจะเป็นที่จะเกิดผลลัพธ์ i กับ ล็อกของความน่าจะเป็นนั้น นอกจากนี้ เราสามารถใช้สมการนี้กับกระจายตัวเชิงความน่าจะเป็นทั่วๆไปนอกเหนือจากเหตุการณ์แบบเต็มหน่วยได้อีกด้วยแชนนอนแสดงว่า นิยามของเอนโทรปีทุกรูปแบบที่ตรงตามเงื่อนไขของเขาจะต้องอยู่ในรูป−K∑i=1np(i)log⁡p(i).^p(i)\\log p(i).\\,\\!\",\n",
      "    \"}เมื่อ K เป็นค่าคงตัวใดๆ (และจะเห็นได้ว่ามันเป็นเพียงค่าที่เปลี่ยนไปตามหน่วยวัดเท่านั้นเอง)แชนนอนให้นิยามการวัดเอนโทรปี (H= −p1log2p1− … −pnlog2pn) ว่า เมื่อนำไปวัดที่แหล่งข้อมูล จะสามารถบ่งบอกขนาดที่เล็กที่สุดเท่าที่เป็นไปได้ ของช่องสัญญาณที่ใช้ในการส่งข้อมูลฐานสองได้อย่างถูกต้อง สูตรนี้สามารถสร้างขึ้นมาได้จากการคำนวณค่าคาดหวัง (expectation) ของปริมาณของข้อมูลที่อยู่ในแต่ละหลักของ แหล่งข้อมูล ค่าเอนโทรปีของแชนนอนนี้ได้กลายมาเป็นตัววัดความไม่แน่นอนของตัวแปรสุ่ม และดังนั้นจึงเป็นตัวบอกเกี่ยวกับข้อมูลที่บรรจุอยู่ในข้อความ เมื่อเปรียบเทียบกับส่วนของข้อความที่สามารถคาดการณ์ได้โดยโครงสร้างของมันเอง ตัวอย่างเช่น การใช้คำฟุ่มเฟือยในภาษาสื่อสาร หรือความถี่ของการเกิดตัวอักษรหรือคำแต่ละคู่หรือแต่ละชุด ดูห่วงโซ่มาร์คอฟเพิ่มเติมที่มาของสมการเอนโทรปี[แก้]ลองนึกถึงตัวอย่างของการส่งข้อมูลจากแหล่งหนึ่งไปอีกแหล่งหนึ่ง โดยที่ข้อความที่เป็นไปได้มีทั้งหมด s แบบ ซึ่งก็คือx1,x2,…,xs,x_,\\ldots ,x_}ข้อมูลแต่ละแบบมีความถี่ในการเกิดเป็นk1,k2,…,ks,k_,\\ldots ,k_}ตามลำดับ โดยที่จำนวนการเกิดของข้อมูลทั้งหมดเป็นk=k1+k2+…+ks+k_+\\ldots +k_}หากเรามีข้อมูลเพียงเท่านี้ โดยหลักการนับเบื้องต้น ความเป็นไปได้ของข้อความที่เกิดขึ้นทั้งหมดคือ(kk1,k2,…,ks)=k!k1!k2!…ks!,k_,\\ldots ,k_}=!k_!\\ldots k_!\"\n",
      "  ]\n",
      "]\n",
      "[\n",
      "  [\n",
      "    \"th\",\n",
      "    \"th\",\n",
      "    \"th\",\n",
      "    \"th\",\n",
      "    \"th\"\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Accéder aux données du dataset 'premieres_lignes_train'\n",
    "donnees = premieres_lignes_train.data\n",
    "\n",
    "# Afficher le contenu des lignes\n",
    "for ligne in donnees:\n",
    "    print(ligne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb3dbe85-2933-4065-aca0-d600d919a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "# Afficher les clés disponibles dans le Dataset\n",
    "print(mon_dataset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1460510a-a232-4e1f-828d-8b9ce4eb2c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['th', 'th', 'th', 'th', 'th', 'th', 'th', 'th', 'th', 'th', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'yue', 'yue', 'yue', 'yue', 'yue', 'yue', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'mhr', 'mhr', 'mhr', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sv', 'sv', 'sv', 'sv', 'sv', 'sv', 'sv', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ur', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'sk', 'sk', 'sk', 'tr', 'tr', 'tr', 'tr', 'lt', 'lt', 'lt', 'lt', 'lt', 'lt', 'lt', 'lt', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl']\n"
     ]
    }
   ],
   "source": [
    "# Accéder à la partition 'train' du Dataset\n",
    "partition_train = mon_dataset['train']\n",
    "\n",
    "#partition_train = mon_dataset\n",
    "\n",
    "# Accéder à la colonne 'language_code' dans la partition 'train'\n",
    "colonne_language_code = partition_train['language_code']\n",
    "\n",
    "# Afficher la colonne sélectionnée\n",
    "print(colonne_language_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d68a0439-69e2-4632-8e41-fb09a9ca82d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'language_code'],\n",
      "        num_rows: 96\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les lignes en fonction d'une condition\n",
    "nouveau_dataset = mon_dataset.filter(lambda exemple: exemple['language_code'] == 'fr')\n",
    "\n",
    "# Affiche le résultat du filtrage : le dataset filtré\n",
    "print(nouveau_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd3d0fda-ff0f-4586-b174-dff898949b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibilté avec pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee9c832-4915-40f5-8f85-960550115e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouveau_dataset = nouveau_dataset['train'].to_pandas()\n",
    "#nouveau_dataset = nouveau_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0452fca9-5b75-4388-bb8f-b788a215c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouveau_dataset = Dataset.from_pandas(nouveau_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "303366ac-372b-4b40-a535-0360dda8d4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'language_code'],\n",
      "        num_rows: 2593\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Trier le Dataset par une colonne spécifique (language_code) par ordre lexicographique croissant\n",
    "dataset_trie = mon_dataset.sort('language_code')\n",
    "print(dataset_trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfcd4da6-e205-4224-b0c7-5eb178067494",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'lt', 'lt', 'lt', 'lt', 'lt', 'lt', 'lt', 'lt', 'mhr', 'mhr', 'mhr', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'sk', 'sk', 'sk', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'sv', 'sv', 'sv', 'sv', 'sv', 'sv', 'sv', 'th', 'th', 'th', 'th', 'th', 'th', 'th', 'th', 'th', 'th', 'tr', 'tr', 'tr', 'tr', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'ur', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'yue', 'yue', 'yue', 'yue', 'yue', 'yue', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh']\n"
     ]
    }
   ],
   "source": [
    "# Accéder à la colonne 'language_code' dans la partition 'train'\n",
    "colonne_language_code_trie = dataset_trie['train']['language_code']\n",
    "#colonne_language_code_trie = dataset_trie['language_code']\n",
    "\n",
    "# Afficher la colonne sélectionnée\n",
    "print(colonne_language_code_trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "562d0eba-459c-4fdd-8100-fc9fbd9c6b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'language_code'],\n",
      "        num_rows: 2593\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Trier le Dataset par une colonne spécifique (language_code) par ordre lexicographique décroissant\n",
    "dataset_trie_descendant = mon_dataset.sort('language_code', reverse=True)\n",
    "print(dataset_trie_descendant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6d0c6fa-c4bd-4778-aadd-6597c785843c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'yue', 'yue', 'yue', 'yue', 'yue', 'yue', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'ur', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'tr', 'tr', 'tr', 'tr', 'th', 'th', 'th', 'th', 'th', 'th', 'th', 'th', 'th', 'th', 'sv', 'sv', 'sv', 'sv', 'sv', 'sv', 'sv', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sr-cyrl', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sq', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sl', 'sk', 'sk', 'sk', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pt', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pl', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'pa-guru', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'mhr', 'mhr', 'mhr', 'lt', 'lt', 'lt', 'lt', 'lt', 'lt', 'lt', 'lt', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'gl', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'fa', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'eu', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'el', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cy', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'cs', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ckb', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'ca', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bs', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bg', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'ar', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af', 'af']\n"
     ]
    }
   ],
   "source": [
    "# Accéder à la colonne 'language_code' dans la partition 'train'\n",
    "colonne_language_code_trie_desc = dataset_trie_descendant['train']['language_code']\n",
    "#colonne_language_code_trie_desc = dataset_trie_descendant['language_code']\n",
    "\n",
    "# Afficher la colonne sélectionnée\n",
    "print(colonne_language_code_trie_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b813c1-115f-496b-b9ef-cac7923d854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une nouvelle colonne calculée à partir de la concaténation de deux colonnes existantes\n",
    "mon_nouveau_dataset = mon_dataset.map(lambda exemple: {'nouvelle_colonne': exemple['language_code'] + exemple['text']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52b5a02f-394e-4c0d-8be6-8bfa1985a88b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"thเอนโทรปีของข้อมูล - วิกิพีเดียข้ามไปเนื้อหาเมนูหลักเมนูหลักย้ายเมนูไปที่แถบด้านข้างซ่อนการนำทางหน้าหลักถามคำถามเหตุการณ์ปัจจุบันสุ่มบทความเกี่ยวกับวิกิพีเดียติดต่อเราบริจาคให้วิกิพีเดียมีส่วนร่วมคำอธิบายเริ่มต้นเขียนศาลาประชาคมปรับปรุงล่าสุดดิสคอร์ดค้นหาค้นหาสร้างบัญชีเข้าสู่ระบบเครื่องมือส่วนตัวสร้างบัญชีเข้าสู่ระบบหน้าสำหรับผู้แก้ไขที่ออกจากระบบเรียนรู้เพิ่มเติมส่วนร่วมคุยสารบัญย้ายเมนูไปที่แถบด้านข้างซ่อนบทนำ1นิยามทางการ2ที่มาของสมการเอนโทรปีToggle the table of contentsเอนโทรปีของข้อมูล45 ภาษาAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaTürkçeУкраїнськаاردوTiếng Việt中文粵語แก้ไขลิงก์บทความอภิปรายไทยอ่านแก้ไขดูประวัติเครื่องมือเครื่องมือย้ายเมนูไปที่แถบด้านข้างซ่อนการกระทำอ่านแก้ไขดูประวัติทั่วไปหน้าที่ลิงก์มาการเปลี่ยนแปลงที่เกี่ยวโยงอัปโหลดไฟล์หน้าพิเศษลิงก์ถาวรสารสนเทศหน้าอ้างอิงบทความนี้รับยูอาร์แอลแบบสั้นดาวน์โหลดคิวอาร์โค้ดสิ่งนี้ในวิกิสนเทศแก้ไขลิงก์ข้ามภาษาพิมพ์/ส่งออกสร้างหนังสือดาวน์โหลดเป็น PDFรุ่นพร้อมพิมพ์ในโครงการอื่นวิกิมีเดียคอมมอนส์จากวิกิพีเดีย สารานุกรมเสรีบทความนี้ไม่มีการอ้างอิงจากแหล่งที่มาใดกรุณาช่วยปรับปรุงบทความนี้โดยเพิ่มการอ้างอิงแหล่งที่มาที่น่าเชื่อถือ เนื้อความที่ไม่มีแหล่งที่มาอาจถูกคัดค้านหรือลบออก(เรียนรู้ว่าจะนำสารแม่แบบนี้ออกได้อย่างไรและเมื่อไร)เอนโทรปีของการทดลองแบร์นูลีซึ่งเป็นฟังก์ชันของโอกาสสำเร็จในทฤษฎีข้อมูลเอนโทรปีของข้อมูลคือ เป็นลักษณะที่บ่งชี้ระดับการสุ่มของสัญญาณหรือเหตุการณ์สุ่ม ว่ามีมากน้อยเพียงใด หรือเราอาจมองอีกมุมหนึ่งว่าเป็นตัวบ่งบอกว่าสัญญาณอันหนึ่งบรรจุข้อมูลอยู่เท่าไรเอนโทรปีเป็นแนวคิดของเทอร์โมไดนามิคส์กลศาสตร์ทางสถิติและทฤษฎีข้อมูลแนวคิดของเอนโทรปีกับเรื่องของข้อมูลมีความเกี่ยวพันกันอย่างมาก อย่างไรก็ตามกว่าที่กลศาสตร์ทางสถิติและทฤษฎีข้อมูล จะพัฒนามาจนความสัมพันธ์นี้ปรากฏขึ้น ก็ใช้เวลานานทีเดียว บทความนี้เป็นบทความเกี่ยวกับเอนโทรปีของข้อมูล (กฎเกณฑ์ของเอนโทรปีที่เกี่ยวข้องกับข้อมูลเชิงทฤษฎี)ตัวอย่างเช่น พิจารณาข้อความในภาษาไทย ซึ่งประกอบด้วยตัวอักษรและเครื่องหมายต่าง ๆ (ซึ่งสัญญาณของเราในที่นี้ ก็คือลำดับของตัวอักษรและเครื่องหมายนั่นเอง) สังเกตว่าตัวอักษรบางตัวมีโอกาสปรากฏขึ้นมาน้อยมาก (เช่น ฮ) แต่บางตัวกลับปรากฏบ่อยมาก (เช่น อ) ดังนั้นข้อความภาษาไทยนั้นก็ไม่ได้เรียกว่าสุ่มซะทีเดียว (ถ้าสุ่มจริง ข้อความน่าจะออกมาเป็นคำมั่ว ๆ อ่านไม่ได้ใจความ) อย่างไรก็ตาม ถ้าเราได้คำชุดหนึ่งมา เราก็ไม่อาจคาดเดาได้ว่าคำต่อไปเป็นคำว่าอะไร แสดงว่ามันก็มี'ความสุ่ม'อยู่บ้าง ไม่ได้เที่ยงแท้ซะทีเดียว เอนโทรปีก็คือการวัดระดับความสุ่มนี้นั่นเอง โดยกำเนิดมาจากผลงานของคลาวด์ อี แชนนอนในปีพ.ศ.\", 'th2491(ค.ศ.', 'th1948) ชื่อA Mathematical Theory of Communication[1]เก็บถาวร1998-01-31 ที่เวย์แบ็กแมชชีนแชนนอนสร้างบทนิยามของเอนโทรปีขึ้นจากข้อสมมติฐานว่าค่านี้จะต้องมีสัดส่วน (ที่ต่อเนื่อง) นั่นคือ หากเปลี่ยนค่าของความน่าจะเป็นอันหนึ่งเพียงเล็กน้อย ค่าเอนโทรปีก็ควรเปลี่ยนเพียงเล็กน้อยเช่นกันหากผลลัพธ์ (เช่น ตัวอักษรและเครื่องหมายในตัวอย่างข้างต้น) ทุกอันมีโอกาสเกิดเท่า ๆ กันแล้ว การเพิ่มจำนวนตัวอักษร (และเครื่องหมาย) จะต้องทำให้ค่าเอนโทรปีเพิ่มขึ้นด้วยเสมอเราต้องสามารถใช้วิธีเลือกผลลัพธ์ (ตัวอักษร) โดยทำเป็นสองขั้นตอน และในกรณีนี้ ค่าเอนโทรปีของผลลัพธ์สุดท้ายต้องเท่ากับเอนโทรปีของทั้งสองขั้นตอนบวกกัน (โดยมีการถ่วงน้ำหนัก)นิยามทางการ[แก้]สำหรับเหตุการณ์สุ่มแบบเต็มหน่วย x ซึ่งสมมุติให้มีสถานะ 1..n,คล็อด อี แชนนอนนิยามเอนโทรปีในเทอมของ x ว่าH(x)=∑i=1np(i)log2\\u2061(1p(i))=−∑i=1np(i)log2\\u2061p(i).^p(i)\\\\log _\\\\left(}\\\\right)=-\\\\sum _^p(i)\\\\log _p(i).\\\\,\\\\!', 'th}นั่นคือ เอนโทรปีของเหตุการณ์ x คือ ผลรวม (บนทุกๆผลลัพธ์ i ที่เป็นไปได้) ของผลคูณของความน่าจะเป็นที่จะเกิดผลลัพธ์ i กับ ล็อกของความน่าจะเป็นนั้น นอกจากนี้ เราสามารถใช้สมการนี้กับกระจายตัวเชิงความน่าจะเป็นทั่วๆไปนอกเหนือจากเหตุการณ์แบบเต็มหน่วยได้อีกด้วยแชนนอนแสดงว่า นิยามของเอนโทรปีทุกรูปแบบที่ตรงตามเงื่อนไขของเขาจะต้องอยู่ในรูป−K∑i=1np(i)log\\u2061p(i).^p(i)\\\\log p(i).\\\\,\\\\!', 'th}เมื่อ K เป็นค่าคงตัวใดๆ (และจะเห็นได้ว่ามันเป็นเพียงค่าที่เปลี่ยนไปตามหน่วยวัดเท่านั้นเอง)แชนนอนให้นิยามการวัดเอนโทรปี (H= −p1log2p1− … −pnlog2pn) ว่า เมื่อนำไปวัดที่แหล่งข้อมูล จะสามารถบ่งบอกขนาดที่เล็กที่สุดเท่าที่เป็นไปได้ ของช่องสัญญาณที่ใช้ในการส่งข้อมูลฐานสองได้อย่างถูกต้อง สูตรนี้สามารถสร้างขึ้นมาได้จากการคำนวณค่าคาดหวัง (expectation) ของปริมาณของข้อมูลที่อยู่ในแต่ละหลักของ แหล่งข้อมูล ค่าเอนโทรปีของแชนนอนนี้ได้กลายมาเป็นตัววัดความไม่แน่นอนของตัวแปรสุ่ม และดังนั้นจึงเป็นตัวบอกเกี่ยวกับข้อมูลที่บรรจุอยู่ในข้อความ เมื่อเปรียบเทียบกับส่วนของข้อความที่สามารถคาดการณ์ได้โดยโครงสร้างของมันเอง ตัวอย่างเช่น การใช้คำฟุ่มเฟือยในภาษาสื่อสาร หรือความถี่ของการเกิดตัวอักษรหรือคำแต่ละคู่หรือแต่ละชุด ดูห่วงโซ่มาร์คอฟเพิ่มเติมที่มาของสมการเอนโทรปี[แก้]ลองนึกถึงตัวอย่างของการส่งข้อมูลจากแหล่งหนึ่งไปอีกแหล่งหนึ่ง โดยที่ข้อความที่เป็นไปได้มีทั้งหมด s แบบ ซึ่งก็คือx1,x2,…,xs,x_,\\\\ldots ,x_}ข้อมูลแต่ละแบบมีความถี่ในการเกิดเป็นk1,k2,…,ks,k_,\\\\ldots ,k_}ตามลำดับ โดยที่จำนวนการเกิดของข้อมูลทั้งหมดเป็นk=k1+k2+…+ks+k_+\\\\ldots +k_}หากเรามีข้อมูลเพียงเท่านี้ โดยหลักการนับเบื้องต้น ความเป็นไปได้ของข้อความที่เกิดขึ้นทั้งหมดคือ(kk1,k2,…,ks)=k!k1!k2!…ks!,k_,\\\\ldots ,k_}=!k_!\\\\ldots k_!', 'th}}}ในการที่ฝ่ายส่งข้อมูลส่งข้อความไปให้ฝ่ายรับ ฝ่ายส่งสามารถเลือกใช้การเข้ารหัสแบบใดก็ได้ แต่สิ่งสำคัญที่สุดอย่างหนึ่งที่ต้องคำนึงคือ \"ฝ่ายรับต้องสามารถสร้างข้อความที่ได้รับมาให้กลับไปอยู่ในรูปแบบเดิมได้\" นั่นก็คือ เราจะไม่สนใจการเข้ารหัสแบบแปลกประหลาดทั้งหลายที่ฝ่ายรับนำข้อมูลมาใช้ไม่ได้ ยกตัวอย่างเช่น ส่งข้อมูลทุกอย่างไปในรูปแบบของ \"111\" ฝ่ายรับไม่สามารถนำข้อมูลที่ได้รับมาไปใช้ได้เลยวิธีส่งที่ฝ่ายส่งจะสามารถทำได้แบบหนึ่งคือ ส่งข้อมูลดังต่อไปนี้ไปให้ฝ่ายรับส่งค่าของ(k1,k2,…,ks),k_,\\\\ldots ,k_)}ใช้ปริมาณข้อมูลทั้งหมดslog\\u2061kบิตส่งลำดับของข้อความที่ต้องการใน total ordering ที่นิยามบนเซ็ตความเป็นไปได้ของข้อความทั้งหมด (ordering นั้นจำเป็นที่จะต้องเป็น computable ordering, หรือพูดอีกอย่างหนึ่งก็คือ ฝ่ายรับสามารถคำนวณหาข้อความได้เมื่อรู้ลำดับของข้อความในเซ็ตนั้น) ใช้ปริมาณข้อมูลเท่ากับค่าลอกการิธึมของขนาดของเซ็ต(สังเกตอย่างหนึ่งว่าการส่งข้อมูลที่พิจารณากันในที่นี้ ไม่สนใจประสิทธิภาพของการถอดข้อมูลกลับไปเป็นรูปแบบเดิม นั่นก็คือ ผู้ส่งนั้นไม่สนใจว่าฝ่ายรับจะใช้เวลานานเพียงใดในการคำนวณหาข้อความที่ต้องการส่งจากสิ่งที่ส่งไป สิ่งที่ฝ่ายส่งสนใจมีเพียงว่า ถ้าฝ่ายรับมีชีวิตเป็นอนันต์ ซักวันคงถอดรหัสกลับมาได้)ปริมาณข้อมูลที่ต้องการใช้ทั้งหมดก็คือlog\\u2061(k!k1!k2!…ks!)≤H(X)≤log\\u2061(k!k1!k2!…ks!', 'th)+slog\\u2061k!k_!\\\\ldots k_!', 'th}})\\\\leq H(X)\\\\leq \\\\log(!k_!\\\\ldots k_!', 'th}})+s\\\\log k}ถ้าเราใช้สูตรของ Stirling ในการประมาณค่าแฟกทอเรียล และหาลิมิตของ H (X) โดยที่ค่า k เข้าใกล้อนันต์ เราจะได้ผลลัพธ์คือH(X)=−K∑i=1np(i)log\\u2061p(i).^p(i)\\\\log p(i).\\\\,\\\\!', 'th}โดยที่pi=ki/k=k_/k}เป็นความถี่ของการเกิดข้อมูลชนิดที่ iบทความคอมพิวเตอร์อุปกรณ์ต่าง ๆหรือเครือข่ายนี้ยังเป็นโครงคุณสามารถช่วยวิกิพีเดียได้โดยการเพิ่มเติมข้อมูลดคกเข้าถึงจาก \"https://th.wikipedia.org/w/index.php?title=เอนโทรปีของข้อมูล&oldid=9914656\"หมวดหมู่:บทความเกี่ยวกับ คอมพิวเตอร์ ที่ยังไม่สมบูรณ์วิทยาการสารสนเทศทฤษฎีสารสนเทศสถิติศาสตร์การสุ่มทฤษฎีทางสถิติหมวดหมู่ที่ซ่อนอยู่:บทความที่ขาดแหล่งอ้างอิงบทความทั้งหมดที่ขาดแหล่งอ้างอิงWebarchive template wayback linksบทความทั้งหมดที่ยังไม่สมบูรณ์หน้านี้แก้ไขล่าสุดเมื่อวันที่ 7 กุมภาพันธ์ 2565 เวลา 14:36 น.อนุญาตให้เผยแพร่ภายใต้สัญญาอนุญาตครีเอทีฟคอมมอนส์ แบบแสดงที่มา-อนุญาตแบบเดียวกันและอาจมีเงื่อนไขเพิ่มเติม ดูรายละเอียดที่ข้อกำหนดการใช้งานWikipedia® เป็นเครื่องหมายการค้าจดทะเบียนของมูลนิธิวิกิมีเดียองค์กรไม่แสวงผลกำไรติดต่อเรานโยบายความเป็นส่วนตัวเกี่ยวกับวิกิพีเดียข้อปฏิเสธความรับผิดชอบจรรยาบรรณผู้พัฒนาสถิตินโยบายการใช้คุกกี้มุมมองสำหรับอุปกรณ์เคลื่อนที่สลับความกว้างเนื้อหาจำกัด', 'elΕντροπία πληροφοριών - ΒικιπαίδειαΜετάβαση στο περιεχόμενοΚύριο μενούΚύριο μενούμετακίνηση στην πλαϊνή μπάρααπόκρυψηΠλοήγησηΚύρια πύληΘεματικές πύλεςΠροβεβλημένα λήμματαΤρέχοντα γεγονόταΤυχαίο λήμμαΣυμμετοχήΒοήθειαΠύλη ΚοινότηταςΑγοράΠρόσφατες αλλαγέςΕπικοινωνίαΔωρεέςΑναζήτησηΑναζήτησηΔημιουργία λογαριασμούΣύνδεσηΠροσωπικά εργαλείαΔημιουργία λογαριασμούΣύνδεσηΣελίδες για αποσυνδεμένους συντάκτεςμάθετε περισσότεραΣυνεισφορέςΣυζήτηση για αυτή την IP[κλείσιμο]Από τις 21 Μαρτίου έως τις 31 Μαΐου, λάβετε μέρος στον διαγωνισμό σύνταξης λημμάτων για τις χώρες της Κεντρικής και Ανατολικής Ευρώπης.Συνεισφέρετε γράφοντας στο CEE Spring 2024!Περιεχόμεναμετακίνηση στην πλαϊνή μπάρααπόκρυψηΑρχή1Ορισμός2ΠαραδείγματαΕναλλαγή Παραδείγματα υποενότητας2.1Δοκιμή Bernoulli2.2Ισοπίθανα γεγονότα3ΠαραπομπέςΕναλλαγή του πίνακα περιεχομένωνΕντροπία πληροφοριών45 γλώσσεςAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Επεξεργασία συνδέσμωνΛήμμαΣυζήτησηΕλληνικάΑνάγνωσηΕπεξεργασίαΕπεξεργασία κώδικαΠροβολή ιστορικούΕργαλείαΕργαλείαμετακίνηση στην πλαϊνή μπάρααπόκρυψηΕνέργειεςΑνάγνωσηΕπεξεργασίαΕπεξεργασία κώδικαΠροβολή ιστορικούΓενικάΣυνδέσεις προς εδώΣχετικές αλλαγέςΕιδικές σελίδεςΣταθερός σύνδεσμοςΠληροφορίες σελίδαςΠαραπομπήΛάβετε συντομευμένη διεύθυνση URLDownload QR codeΑντικείμενο WikidataΕκτύπωση/εξαγωγήΔημιουργία βιβλίουΚατέβασμα ως PDFΕκτυπώσιμη έκδοσηΣε άλλα εγχειρήματαWikimedia CommonsΑπό τη Βικιπαίδεια, την ελεύθερη εγκυκλοπαίδειαΗεντροπίαστηθεωρία πληροφορίαςείναι ένα «μέτρο αβεβαιότητας» που διακατέχει ένα σύστημα.Ο όροςεντροπίαχρησιμοποιήθηκε αρχικά στηθερμοδυναμική(βλ.εντροπία).', \"elΣτη θεωρία πληροφορίας εισήχθη από τονΚλωντ Σάνοντο1948και γι' αυτό ονομάζεται καιεντροπία του Σάννον.\", 'elΗ χρήση του ίδιου όρου με τη θερμοδυναμική εντροπία, παρότι μπορεί να προκαλέσει σύγχυση, υιοθετήθηκε από τον Σάνον μετά και από παρότρυνση ενός άλλου σπουδαίου μαθηματικού, τουΤζον φον Νόιμαν, ο οποίος φέρεται ότι είχε πει στον Σάνον[1]:«Πρέπει να το ονομάσεις εντροπία για δύο λόγους: Πρώτον, η συνάρτηση αυτή χρησιμοποιείται ήδη στη θερμοδυναμική με το ίδιο όνομα.', 'elΔεύτερο, και σημαντικότερο, ο περισσότερος κόσμος δεν γνωρίζει τι πραγματικά είναι η εντροπία, και αν χρησιμοποιείς τον όρο εντροπία σε ένα αντεπιχείρημα θα κερδίζεις πάντα».Η εντροπία της θερμοδυναμικής μπορεί να αντιστοιχιστεί με την εντροπία στη θεωρία πληροφορίας.Ορισμός[Επεξεργασία|επεξεργασία κώδικα]Έστω έναπείραμα τύχηςμε n πιθανά αποτελέσματα.', 'elΘεωρούμε τηντυχαία μεταβλητήXκαι τα απλά ενδεχόμεναx1,…,xn,\\\\ldots ,x_}που πραγματοποιούνται με πιθανότητεςp1,…,pn,\\\\ldots ,p_}και(∑i=1npi=1)^p_=1)}αντίστοιχα.Η εντροπία ορίζεται ως:H(X)=∑i=1npilog2\\u2061(1pi)=−∑i=1npilog2\\u2061pi^p_\\\\log _\\\\left(}}\\\\right)=-\\\\sum _^p_\\\\log _p_},με την σύμβαση0log2\\u20610=00=0}.Παραδείγματα[Επεξεργασία|επεξεργασία κώδικα]Δοκιμή Bernoulli[Επεξεργασία|επεξεργασία κώδικα]Κύριο λήμμα:Κατανομή ΜπερνούλλιΗ εντροπία σε μίαδοκιμή Bernoulliως συνάρτηση της πιθανότητας επιτυχίαςPr(X=1)=p.Έστω μία δοκιμή Bernoulli με πιθανότητα επιτυχίαςp.', 'elΣυγκεκριμένα μπορούμε να θεωρήσουμε ένα δοχείο μεNμπάλες,Npαπό τις οποίες είναι λευκές καιN(1−p)μαύρες από το οποίο επιλέγουμε τυχαία μία μπάλα.', 'elΗ εντροπία της δοκιμής δίνεται από τον τύποH(X)=−p⋅log2\\u2061p−(1−p)⋅log2\\u2061(1−p)p-(1-p)\\\\cdot \\\\log _(1-p)}.Αν όλες οι μπάλες είναι λευκές ή όλες είναι μαύρες (p=1ήp=0αντίστοιχα), τότε ξέρουμε με σιγουριά το αποτέλεσμα του πειράματος και η εντροπία είναι0.', 'elΤη μέγιστη αβεβαιότητα για το αποτέλεσμα την έχουμε όταν οι μισές μπάλες είναι λευκές και οι μισές μαύρες,p=0.5.Ισοπίθανα γεγονότα[Επεξεργασία|επεξεργασία κώδικα]Κύριο λήμμα:Διακριτή ομοιόμορφη κατανομήΈστω η τυχαία μεταβλητήXμπορεί να πάρειnτιμές που είναι ισοπίθανες μεταξύ τους, δηλαδήpi=1/n=1/n}.', 'elΗ εντροπία τότε είναι:H(X)=−∑i=1n1nlog2\\u20611n=log2\\u2061n^}\\\\log _}=\\\\log _n}.Έτσι σε μια πηγή πληροφορίας χωρίς μνήμη (η εμφάνιση ενός συμβόλου δεν εξαρτάται από την εμφάνιση κάποιου άλλου συμβόλου) και στην οποία όλα της τα σύμβολα είναι ισοπίθανα τότε έχουμε μέγιστη τιμή εντροπίαςHmax}.Παρατηρούμε ότι η εντροπία αυξάνει με τον αριθμό των καταστάσεων.Παραπομπές[Επεξεργασία|επεξεργασία κώδικα]↑L.', 'elFloridi, 2010,Information.', 'elA very short introduction.Shigeru Furuichi, Flavia-Corina Mitroi-Symeonidis, Eleutherius Symeonidis, On some properties of Tsallis hypoentropies and hypodivergences, Entropy, 16(10) (2014), 5377-5399; DOI:10.3390/e16105377Shigeru Furuichi, Flavia-Corina Mitroi, Mathematical inequalities for some divergences, Physica A 391 (2012), pp.', 'el388-400, DOI:10.1016/j.physa.2011.07.052; ISSN 0378-4371Shigeru Furuichi, Nicuşor Minculete, Flavia-Corina Mitroi, Some inequalities on generalized entropies, J. Inequal.', 'elAppl., 2012, 2012:226.', 'elDOI: 10.1186/1029-242X-2012-226Αυτό το λήμμα χρειάζεταιεπέκταση.', 'elΜπορείτε να βοηθήσετε την Βικιπαίδειαεπεκτείνοντάς το.Ανακτήθηκε από \"https://el.wikipedia.org/w/index.php?title=Εντροπία_πληροφοριών&oldid=10289906\"Κατηγορία:Θεωρία της ΠληροφορίαςΚρυμμένη κατηγορία:ΕπέκτασηΤελευταία τροποποίηση 09:34, 14 Νοεμβρίου 2023.Όλα τα κείμενα είναι διαθέσιμα υπό τηνCreative Commons Attribution-ShareAlike License· μπορεί να ισχύουν και πρόσθετοι όροι.', 'elΧρησιμοποιώντας αυτό τον ιστότοπο, συμφωνείτε στουςΠολιτική Ιδιωτικότητας.', 'elΤο Wikipedia® είναι καταχωρημένο σήμα τουWikimedia Foundation, Inc., ενός μη κερδοσκοπικού οργανισμού.Πολιτική προσωπικών δεδομένωνΓια τη ΒικιπαίδειαΑποποίηση ευθυνώνΚώδικας συμπεριφοράςΠρογραμματιστέςΣτατιστικάΔήλωση cookieΠροβολή κινητούΕναλλαγή περιορισμένου πλάτους περιεχομένου', 'yue資訊熵 - 維基百科，自由嘅百科全書跳去內容主目錄主目錄移去側欄收埋導覽頭版目錄正嘢時人時事是但一版關於維基百科聯絡處捐畀維基百科交流說明書城市論壇社區大堂最近修改查嘢搵嘢開戶口簽到個人架生開戶口簽到未簽到編者用嘅版面知多啲貢獻同呢個互聯網地址嘅匿名人傾偈目錄移去側欄收埋文頭1算式2概論3註釋4睇埋5文獻6攷開/收內容一覽資訊熵45種語言AfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文改拎文章討論粵語简閱改睇返紀錄架撐架撐移去側欄收埋動作閱改睇返紀錄基本有乜連過嚟連結頁嘅更改上載檔案專門版固定連結此版明細引用呢篇文攞短網址Download QR code維基數據項打印/匯出下載PDF印得嘅版本第啲維基項目維基同享出自維基百科，自由嘅百科全書想像家吓掟一個銀仔（睇埋伯努利試驗），幅圖Y 軸係資訊熵，而X 軸係出「公」嘅機率。由幅圖睇得出，資訊熵喺個銀仔冇出千（「公」同「字」機率都係 50%）嗰時會最大化。提示：呢篇文講嘅唔係熵。資訊熵（information entropy）係資訊理論（研究資訊嘅數學理論）上嘅核心概念。同物理學上所講嘅熵唔同，資訊論當中嘅「熵」係一個指標，用嚟量度一個有隨機性喺入面嘅變數或者過程當中帶有幾多不確定性（uncertainty）喺入面[1]。舉個例說明，想像家吓掟一個銀仔同擲一粒骰仔，假設個銀仔同粒骰仔係冇出千嘅，前者有 50%機會率係公、50% 機會率係字，而後者有 1/6 機會係擲到「1」、1/6 機會係擲到「2」... 如此類推，即係話[註 1]：掟銀仔（i表示掟銀仔結果；0 代表公、1 代表字）：p(i)=}&}i=0,\\\\\\\\}&}i=1.\\\\end}}擲骰仔（i表示擲到嘅數字）：p(i)=}&}i=1,\\\\\\\\}&}i=2,\\\\\\\\}&}i=3,\\\\\\\\...\\\\end}}後者嘅情況有更加高嘅資訊熵－後者當中有更多嘅可能性喺度，所以不確定性亦都更加大。亦都因為噉，「話俾人知掟銀仔嘅結果」所俾嘅資訊少過「話俾人知擲骰仔嘅結果」所俾嘅，因為喺後者情況當中，「提供資訊」所消除嘅資訊熵更加多。就係噉，資訊論做到將「資訊」呢一個概念量化，令到資訊成為一個喺科學上可以被研究嘅對象[1][2]。以下嘅內容係建基於概率論嘅基礎概念嘅。算式[編輯]睇埋：資訊理論俾是但一個變數，佢會有一啲可能數值，例如「某年某月某日某刻掟某一個銀仔」嘅可能數值大致上有兩個－「公」同「字」。每個數值都會有一定嘅機會率出現，而描述每一個可能數值出現嘅機會率嘅就係所謂嘅概率質量函數（probability mass function）。知道咗某一件事件嘅概率質量函數之後，件事件所帶有嘅資訊熵（information entropy；數學符號係「S」，單位係山農）可以用以下呢條式計算[1]：S=−∑ipilog2\\u2061(pi)p_\\\\log _(p_)}；[1]喺呢條式當中，pi}係指第 i 個可能性發生嘅機會率，而log2\\u2061(pi)(p_)}係pi}以 2 做基數嘅對數。成條式用日常用語講嘅話如下：「考慮一件事件嘅所有可能性，將每一個可能性嘅機會率乘嗰個可能性嘅機會率以 2 做基數嘅對數，再將呢啲得出嘅數加埋嗮一齊，再將個數負向，就會得出件事件所包含嘅資訊熵」。用呢條式計嘅話，「掟一粒冇出千嘅銀仔」（即係話「公」同「字」嘅機會率都係 50%）呢件事件當中帶嘅資訊熵（S(X)）係[3]：S(X)=−(12log2\\u2061(12)+12log2\\u2061(12))=1}\\\\log _\\\\left(}\\\\right)+}\\\\log _\\\\left(}\\\\right)\\\\right)=1}；喺直覺上，呢條式係喺度量度緊件事件含有嘅不確定性。科學家之所以會揀用呢條式嚟計資訊熵，係因為喺可能嘅算式當中，得呢條符合佢哋心目中「一條計不確定性嘅算式應有嘅特性」：例如一件肯定嘅事件係理應冇資訊熵嘅－而如果其中一個可能性機會率等如「1」（即係其他可能性機會率冚唪唥等如「0」），[1]呢條式會俾出「0」；而另一方面，喺每個可能性機會率都一樣（不確定性最大化）嗰陣，[1]俾嘅數值會最大化[4]。概論[編輯]資訊熵嘅單位係位元（bit）。有啲研究者會用 2 以外嘅數做上面條式個對數嘅基數嘅，不過道理都一樣，例如用 28= 256 做基數嘅話，條式就會俾出以位元組（byte）作為單位嘅資訊熵[5]。資訊熵算式嘅一個特殊情況涉及一個得兩個可能數值嘅隨機變數，例如「掟銀仔嘅結果」就屬於呢種變數。喺呢個情況下，兩個可能性嘅機率加埋實會等如 1（p1+p2=1+p_=1}，當中p1}同p2}係兩個可能情況分別嘅機率），而呢件事件條資訊熵式就係所謂嘅二元熵函數（binary entropy function）。條式係噉樣嘅：Sb(p)=−plog2\\u2061p−(1−p)log2\\u2061(1−p) }(p)=-p\\\\log _p-(1-p)\\\\log _(1-p)}－係將[1]應用落去「得兩個可能性」嘅情況嘅樣。想像家吓有一個資訊源，佢傳送一連串有 N 個符號嘅資訊，而每個符號都係獨立同分佈（independent and identically distributed；iid）－指一個符號係乜唔會影響嗰串嘢入面第啲符號係乜，但每個符號嘅概率分佈相同，例如一連串互不相干，每個都係隨機噉揀嘅英文字母－呢串嘢嘅資訊熵係NSi}（當中Si}係每一個個別符號嘅資訊熵）咁多位元；而如果呢串嘢係同分佈（每個符號概率分佈相同）但係唔獨立（一個符號係乜可以影響第啲符號係乜嘅機率）－例如係一篇用英文寫嘅文章噉，如果佢某一橛係「information」呢個字嘅話，噉呢橛嘅下一格好大可能係一個空位－喺呢種情況下，串嘢嘅資訊熵會細過NSi}。如果有個人傳遞 1,000 位元（0 同 1）長嘅數碼訊號，而收訊號嗰個人喺訊號傳出去之前經已完全噉知道嗮每一個位元係「1」定「0」，噉傳訊號嗰個人冇傳達到任何新資訊－因為收訊號嗰個人喺收到訊號前後嘅資訊熵都係「0」，所以呢串訊號嘅傳遞並冇消除任何資訊熵。但如果喺收訊號嗰個人眼中嗰 1,000 個位元每一個都係 50% 機率係「1」、50% 機率係「0」嘅話，噉用[1]嚟計，佢透過接收呢串訊號會收到 1,000 位元嘅資訊。註釋[編輯]↑p(i)意思係「結果係i嘅機會率」。睇埋[編輯]電腦訊號困惑度文獻[編輯]Cover, T.M., Thomas, J.A.', 'yue(2006),Elements of Information Theory - 2nd Ed., Wiley-Interscience,ISBN 978-0-471-24195-9MacKay, D.J.C.', 'yue(2003),Information Theory, Inference and Learning Algorithms, Cambridge University Press,ISBN 978-0-521-64298-9Arndt, C. (2004),Information Measures: Information and its Description in Science and Engineering, Springer,ISBN 978-3-540-40855-0攷[編輯]↑1.01.11.2Demystifying Entropy.Towards Data Science.↑Fazlollah M. Reza (1994) [1961].An Introduction to Information Theory.', 'yueDover Publications, Inc., New York.↑Fazlollah M. Reza (1994) [1961].An Introduction to Information Theory.', 'yueDover Publications, Inc., New York.↑Gray, R. M. (2011),Entropy and Information Theory, Springer.↑Norman Abramson (1963),Information theory and coding.', 'yueMcGraw-Hill.睇傾改資訊理論資訊通訊同資訊處理Signal（數碼定模擬）雜音訊號對雜訊比例Bit相似度量度資訊熵（聯合熵·條件熵·相互資訊·條件相互資訊·相對熵）熵率資訊最大化原則第啲概念概率論（不確定性·概率）編碼理論（雜訊通道編碼定理）數據壓縮（率失真理論·無雜訊編碼定理）頻道（二元對稱同二元消除·容量）相關領域電腦科學（密碼學·資訊系統）電子工程同電訊認知科學（神經科學）語言學統計學模控學資訊經濟學拉雜相關貝葉斯推論演算法熵赤池同貝葉斯資訊量準則擬亂數產生同隨機種子控制系統資訊過多認知同神經編碼睇傾改電腦科學研究運算嘅科學工程領域重要概念運算·I/O·電腦·運算邏輯·數論·二進制·數學邏輯·離散數學（圖論） ·抽象化·電腦科學詞彙表電腦理論運算理論自動機理論·可運算度理論·運算複雜度理論·圖靈機·量子運算資訊理論（資訊·訊號·資訊熵·編碼理論） ·資料類型\\u200e同數據結構·系統資源·演算法（分析） ·形式化方法電腦系統電腦架構（邏輯門·指令集架構·微架構·多元處理·作業系統） ·並行、平行同分散運算 ·性能·超級電腦電腦通訊電腦網絡網絡傳輸協議·網絡拓樸·數據傳輸·互聯網·網頁設計電腦保安網絡保安·漏洞·公共漏洞同暴露·網絡攻擊·電腦病毒·密碼學程式編寫程式語言（程式語言理論·低級同高級程式語言 ·控制流程） ·程式執行·編譯器·程式編寫範式電腦應用數碼藝術電腦圖像（2D·3D·電腦動畫·資訊可視化·VR·AR·電腦圖像學） ·電腦音樂軟件工程電腦軟件·軟件開發·軟件需求·軟件項目管理·電子遊戲製作HCI用家·易用度·用家介面（設計·圖像·命令行·腦機）其他應用資訊系統（數據庫） ·科學運算（電腦模擬） ·運算數論·電腦代數AIAI 哲學·知識表示·自動計劃·電腦視覺·NLP·機械學習（ANN·監督式學習·非監督式學習·強化學習）相關領域工程學電子工程（電訊） ·電腦工程·資訊科技·機械人學·機電工程邏輯學·數學（統計學·博弈論） ·認知科學·模控學·資訊科學·系統科學電腦科學類由「https://zh-yue.wikipedia.org/w/index.php?title=資訊熵&oldid=1963200」收屬於2類：訊息論認知科學屬於2隱類：有英文嘅文章用緊ISBN魔術鏈嘅版呢版上次改係2023年3月27號 (禮拜一) 05:14 嘅事。呢度嘅所有文字係根據Creative Commons Attribution-ShareAlike 牌照 4.0嘅條款發佈；可能會有附加嘅條款。 利用呢個網站，你同意利用條款同埋私隱政策。Wikipedia® 係Wikimedia Foundation, Inc.嘅註冊商標，一個非牟利機構。私隱政策關於維基百科免責聲明行為準則開發人員統計Cookie聲明手提版切換限制內容闊度', 'pa-guruਐਨਟ੍ਰੌਪੀ (ਇਨਫ੍ਰਮੇਸ਼ਨ ਥਿਊਰੀ) - ਵਿਕੀਪੀਡੀਆ, ਇਕ ਅਜ਼ਾਦ ਵਿਸ਼ਵਗਿਆਨਕੋਸ਼ਸਮੱਗਰੀ \\'ਤੇ ਜਾਓਮੁੱਖ ਮੀਨੂਮੁੱਖ ਮੀਨੂਸਾਈਡਬਾਰ \\'ਤੇ ਜਾਓਲੁਕਾਓਨੇਵੀਗੇਸ਼ਨਮੁੱਖ ਸਫ਼ਾਸੱਥਹਾਲੀਆ ਤਬਦੀਲੀਆਂਹਾਲੀਆ ਘਟਨਾਵਾਂਰਲ਼ਵਾਂ ਸਫ਼ਾਮਦਦਦਾਨਵਿਕੀ ਰੁਝਾਨਵਧੇਰੇ ਵੇਖੇ ਜਾਣ ਵਾਲੇ ਸਫ਼ੇਅਕਸਰ ਪੁੱਛੇ ਜਾਣ ਵਾਲੇ ਸਵਾਲਖੋਜਖੋਜਖਾਤਾ ਬਣਾਓਦਾਖਲਨਿੱਜੀ ਸੰਦਖਾਤਾ ਬਣਾਓਦਾਖਲਲੌਗ ਆਊਟ ਕੀਤੇ ਸੰਪਾਦਕਾਂ ਲਈ ਪੰਨੇਹੋਰ ਜਾਣੋਯੋਗਦਾਨਗੱਲ-ਬਾਤਸਮੱਗਰੀਸਾਈਡਬਾਰ \\'ਤੇ ਜਾਓਲੁਕਾਓਸ਼ੁਰੂਆਤ1ਜਾਣ-ਪਛਾਣ2ਪਰਿਭਾਸ਼ਾ3ਉਦਾਹਰਨ4ਦਲੀਲ5ਪਹਿਲੂਪਹਿਲੂ ਉਪਭਾਗ ਨੂੰ ਟੌਗਲ ਕਰੋ5.1ਥਰਮੋਡਾਇਨਾਮਿਕ ਐਨਟ੍ਰੌਪੀ ਪ੍ਰਤਿ ਸਬੰਧ5.2ਸੂਚਨਾ ਸਮੱਗਰੀ ਦੇ ਤੌਰ ਤੇ ਐਨਟ੍ਰੌਪੀ5.3ਵਿਭਿੰਨਤਾ ਦੇ ਨਾਪ ਦੇ ਤੌਰ ਤੇ ਐਨਟ੍ਰੌਪੀ5.4ਡਾਟਾ ਕੰਪ੍ਰੈਸ਼ਨ5.5ਜਾਣਕਾਰੀ ਦੇ ਭੰਡਾਰੀਕਰਨ ਅਤੇ ਪ੍ਰਸਾਰ ਦੀ ਸੰਸਾਰ ਦੀ ਤਕਨੀਕੀ ਸਮਰਥਾ5.6ਸੂਚਨਾ ਸਮੱਗਰੀ ਦੇ ਤੌਰ ਤੇ ਐਨਟ੍ਰੌਪੀ ਦੀਆਂ ਕਮੀਆਂ5.7ਕ੍ਰਿਪਟੋਗ੍ਰਾਫੀ ਅੰਦਰ ਐਨਟ੍ਰੌਪੀ ਦੀਆਂ ਕਮੀਆਂ5.8ਇੱਕ ਮਾਰਕੋਵ ਪ੍ਰੋਸੈੱਸ ਦੇ ਤੌਰ ਤੇ ਡਾਟਾ5.9b-ਏਰੀ ਐਨਟ੍ਰੌਪੀ6ਐਫੀਸ਼ੈਂਸੀ7ਲੱਛਣਬੱਧਤਾਲੱਛਣਬੱਧਤਾ ਉਪਭਾਗ ਨੂੰ ਟੌਗਲ ਕਰੋ7.1ਨਿਰੰਤਰਤਾ7.2ਸਮਿੱਟਰੀ7.3ਉੱਚਤਮ7.4ਜੋੜ-ਬੱਧਤਾ8ਹੋਰ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ9ਨਿਰੰਤਰ ਮਾਮਲੇ ਤੱਕ ਅਨਿਰੰਤਰ (ਡਿਸਕ੍ਰੀਟ) ਐਨਟ੍ਰੌਪੀ ਨੂੰ ਵਧਾਓਣਾਨਿਰੰਤਰ ਮਾਮਲੇ ਤੱਕ ਅਨਿਰੰਤਰ (ਡਿਸਕ੍ਰੀਟ) ਐਨਟ੍ਰੌਪੀ ਨੂੰ ਵਧਾਓਣਾ ਉਪਭਾਗ ਨੂੰ ਟੌਗਲ ਕਰੋ9.1ਡਿੱਫ੍ਰੈਂਸ਼ੀਅਲ ਐਨਟ੍ਰੌਪੀ9.2ਡਿਸਕ੍ਰੀਟ (ਅਨਿਰੰਤਰ) ਬਿੰਦੂਆਂ ਦੀ ਹੱਦਬੰਦੀ ਕਰਨ ਵਾਲੀ ਘਣਤਾ (ਡੈਂਸਟੀ)9.3ਸਾਪੇਖਿਕ (ਰਿਲੇਟਿਵ) ਐਨਟ੍ਰੌਪੀ10ਸੰਯੋਜਕਾਤਮਿਕਤਾ ਅੰਦਰ ਵਰਤੋਂਸੰਯੋਜਕਾਤਮਿਕਤਾ ਅੰਦਰ ਵਰਤੋਂ ਉਪਭਾਗ ਨੂੰ ਟੌਗਲ ਕਰੋ10.1ਲੂਮਿਸ-ਵਿਟਨੇ ਅਸਮਾਨਤਾ10.2ਬਾਇਨੌਮੀਅਲ ਗੁਣਾਂਕ (ਕੋਐਫੀਸ਼ੈਂਟ) ਪ੍ਰਤਿ ਸੰਖੇਪ ਅਨੁਮਾਨ11ਇਹ ਵੀ ਦੇਖੋ12ਹਵਾਲੇ13ਹੋਰ ਲਿਖਤਾਂਹੋਰ ਲਿਖਤਾਂ ਉਪਭਾਗ ਨੂੰ ਟੌਗਲ ਕਰੋ13.1ਇਨਫ੍ਰਮੇਸ਼ਨ ਥਿਊਰੀ ਉੱਤੇ ਟੈਕਸਟਬੁਕਾਂ14ਬਾਹਰੀ ਲਿੰਕਸਮੱਗਰੀ ਦੀ ਸਾਰਣੀ ਨੂੰ ਟੌਗਲ ਕਰੋਐਨਟ੍ਰੌਪੀ (ਇਨਫ੍ਰਮੇਸ਼ਨ ਥਿਊਰੀ)45 ਭਾਸ਼ਾਵਾਂAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語ਜੋੜ ਸੋਧੋਸਫ਼ਾਗੱਲਬਾਤਪੰਜਾਬੀਪੜ੍ਹੋਸੋਧੋਅਤੀਤ ਵੇਖੋਸੰਦਸੰਦਸਾਈਡਬਾਰ \\'ਤੇ ਜਾਓਲੁਕਾਓਕਾਰਵਾਈਆਂਪੜ੍ਹੋਸੋਧੋਅਤੀਤ ਵੇਖੋਆਮਕਿਹੜੇ ਸਫ਼ੇ ਇੱਥੇ ਜੋੜਦੇ ਹਨਸਬੰਧਤ ਤਬਦੀਲੀਆਂਖ਼ਾਸ ਸਫ਼ੇਪੱਕੀ ਲਿੰਕਸਫ਼ੇ ਬਾਬਤ ਜਾਣਕਾਰੀਇਸ ਸਫ਼ੇ ਦਾ ਹਵਾਲਾ ਦਿਉਛੋਟਾ ਯੂਆਰਐੱਲ ਪ੍ਰਾਪਤ ਕਰੋQR ਕੋਡ ਡਾਊਨਲੋਡ ਕਰੋShort URLਵਿਕੀਡਾਟਾ ਆਈਟਮਪ੍ਰਿੰਟ ਕਰੋ / ਐਕਸਪੋਰਟ ਕਰੋਇੱਕ ਕਿਤਾਬ ਬਣਾਓPDF ਵਜੋਂ ਡਾਊਨਲੋਡ ਕਰੋਛਪਣਯੋਗ ਸੰਸਕਰਣਹੋਰ ਪ੍ਰਾਜੈਕਟਾਂ ਵਿੱਚਵਿਕੀਮੀਡੀਆ ਕਾਮਨਜ਼ਵਿਕੀਪੀਡੀਆ, ਇੱਕ ਅਜ਼ਾਦ ਗਿਆਨਕੋਸ਼ ਤੋਂਇਸ ਲੇਖਨੂੰਤਸਦੀਕਲਈ ਹੋਰ ਹਵਾਲੇ ਚਾਹੀਦੇ ਹਨ।ਕ੍ਰਿਪਾ ਕਰਕੇਇਸ ਲੇਖ ਨੂੰ ਸੁਧਾਰਨ ਵਿੱਚ ਮੱਦਦ ਕਰੋ।ਗੈਰ-ਸਰੋਤ ਸਮੱਗਰੀ ਨੂੰ ਚੁਣੌਤੀ ਦਿੱਤੀ ਜਾ ਸਕਦੀ ਹੈ ਅਤੇ ਹਟਾਈ ਜਾ ਸਕਦੀ ਹੈ।Find sources:\"ਐਨਟ੍ਰੌਪੀ\" ਇਨਫ੍ਰਮੇਸ਼ਨ ਥਿਊਰੀ–news·newspapers·books·scholar·JSTOR(April 2012)(Learn how and when to remove this template message)ਐਨਟ੍ਰੌਪੀ ਦੀਆਂ 2 ਲੰਬੀਆਂ ਨਦੀਆਂ: ਦੋ ਜਾਇਜ ਸਿੱਕੇ ਦੇ ਟੌਸਾਂ ਦੇ ਮਾਮਲੇ ਵਿੱਚ, ਇਨਫ੍ਰਮੇਸ਼ਨ ਐਨਟ੍ਰੌਪੀ ਨਿਕਲਣ ਵਾਲੇ ਸੰਭਵ ਨਤੀਜਿਆਂ ਦੀ ਸੰਖਿਆ ਦਾ ਲੌਗ-ਬੇਸ-2 ਹੁੰਦੀ ਹੈ; ਦੋ ਸਿੱਕਿਆੰ ਨਾਲ ਚਾਰ ਨਤੀਜੇ ਨਿਕਲਦੇ ਹਨ, ਅਤੇ ਐਨਟ੍ਰੌਪੀ 2 ਬਿੱਟ ਹੁੰਦੀ ਹੈ। ਆਮਤੌਰ ਤੇ, ਇਨਫ੍ਰਮੇਸ਼ਨ ਐਨਟ੍ਰੌਪੀ ਸਾਰੇ ਸੰਭਵ ਨਤੀਜਿਆਂ ਦੀ ਔਸਤ ਜਾਣਕਾਰੀ ਹੁੰਦੀ ਹੈ।ਇਨਫ੍ਰਮੇਸ਼ਨ ਥਿਊਰੀਅੰਦਰ, ਸਿਸਟਮ ਇੱਕ ਟ੍ਰਾਂਸਮਿੱਟਰ, ਚੈਨਲ, ਅਤੇ ਰਿਸੀਵਰ ਰਾਹੀਂ ਮਾਡਲਬੱਧ ਕੀਤੇ ਜਾਂਦੇ ਹਨ। ਟ੍ਰਾਂਸਮਿੱਟਰ ਅਜਿਹੇ ਸੰਦੇਸ਼ ਪੈਦਾ ਕਰਦਾ ਹੈ ਜੋ ਚੈਨਲ ਰਾਹੀਂ ਗੁਜ਼ਾਰ ਕੇ ਭੇਜੇ ਜਾਂਦੇ ਹਨ। ਚੈਨਲ ਸੰਦੇਸ਼ ਨੂੰ ਕਿਸੇ ਤਰੀਕੇ ਸੋਧ ਦਿੰਦਾ ਹੈ। ਰਿਸੀਵਰ ਅਨੁਮਾਨ ਲਗਾਉਣ ਦਾ ਯਤਨ ਕਰਦਾ ਹੈ ਕਿ ਕਿਹੜਾ ਸੰਦੇਸ਼ ਭੇਜਿਆ ਗਿਆ ਸੀ। ਇਸ ਸੰਦ੍ਰਣ ਵਿੱਚ,ਐਨਟ੍ਰੌਪੀ(ਹੋਰ ਵਿਸ਼ੇਸ਼ਤੌਰ ਤੇ,ਸੈਨੋਨ ਐਨਟ੍ਰੌਪੀ) ਹਰੇਕ ਸੰਦੇਸ਼ (ਮੈਸਜ) ਵਿੱਚ ਰੱਖੀ ਜਾਣਕਾਰੀ (ਇਨਫ੍ਰਮੇਸ਼ਨ) ਦਾਉਮੀਦ ਮੁੱਲ(ਔਸਤ) ਹੁੰਦੀ ਹੈ। ਜਾਣਕਾਰੀ ਦੇ ਕਿਸੇ ਵੀ ਪ੍ਰਵਾਹ ਰਾਹੀਂ ਸੰਦੇਸ਼ਾਂ ਦੇ ਮਾਡਲ ਬਣਾਏ ਜਾ ਸਕਦੇ ਹਨ।ਇੱਕ ਹੋਰ ਜਿਆਦਾ ਤਕਨੀਕੀ ਸਮਝ ਵਿੱਚ, ਸੂਚਨਾ ਨੂੰ ਸੰਭਵ ਘਟਨਾਵਾਂ ਜਾਂ ਸੰਦੇਸ਼ਾਂ ਦੀ ਪ੍ਰੋਬੇਬਿਲਿਟੀ ਡਿਸਟ੍ਰੀਬਿਊਸ਼ਨ ਦੇ ਲੌਗਰਿਥਮ ਦੇ ਨੈਗਟਿਵ ਦੇ ਤੌਰ ਤੇ ਪਰਿਭਾਸ਼ਿਤ ਕਰਨ ਦੇ ਕਾਰਨ ਹਨ (ਜੋ ਥੱਲੇ ਲਿਖੇ ਗਏ ਹਨ)। ਹਰੇਕ ਘਟਨਾ ਦੀ ਸੂਚਨਾ ਦੀ ਮਾਤਰਾ ਇੱਕ ਮਨਚਾਹਿਆ ਵੇਰੀਏਬਲ ਰਚਦੀ ਹੈ ਜਿਸਦਾ ਉਮੀਦ ਮੁੱਲ, ਜਾਂ ਔਸਤ, ਸੈਨੌਨ ਐਨਟ੍ਰੌਪੀ ਹੁੰਦੀ ਹੈ। ਐਨਟ੍ਰੌਪੀ ਦੀਆਂ ਯੂਨਿਟਾਂਸ਼ੇਨੌਨ,ਨਾਟ, ਜਾਂਹ੍ਰਟਲੇਹਨ, ਜੋ ਇਸਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਨ ਲਈ ਵਰਤੇ ਜਾਂਦੇ ਲੌਗਰਿਥਮ ਦੇ ਬੇਸ ਤੇ ਨਿਰਭਰ ਕਰਦੀਆਂ ਹਨ, ਭਾਵੇਂ ਸ਼ੇਨੌਨ ਨੂੰ ਸਾਂਝੇ ਤੌਰ ਤੇ ਇੱਕ ਬਿਟ ਕਿਹਾ ਜਾਂਦਾ ਹੈ।ਪ੍ਰੋਬੇਬਿਲਿਟੀ ਡਿਸਟ੍ਰੀਬਿਊਸ਼ਨ ਦਾ ਲੌਗਰਿਥਮ ਐਨਟ੍ਰੌਪੀ ਦੇ ਇੱਕ ਨਾਪ ਦੇ ਤੌਰ ਤੇ ਫਾਇਦੇਮੰਦ ਰਹਿੰਦਾ ਹੈ ਕਿਉਂਕਿ ਇਹ ਸੁਤੰਤਰ ਸੋਮਿਆਂ ਵਾਸਤੇ ਜੋੜਾਤਮਿਕ ਹੁੰਦਾ ਹੈ। ਉਦਾਹਰਨ ਦੇ ਤੌਰ ਤੇ, ਕਿਸੇ ਸਿੱਕੇ ਦੇ ਉਛਾਲਨ ਤੇ ਐਨਟ੍ਰੌਪੀ 1 ਸੈਨੋਨ ਹੁੰਦੀ ਹੈ, ਜਿੱਥੇ ਕਿmਗਣਿਤ ਦੇ ਉਛਾਲਾਂ ਲਈ ਇਹmਸ਼ੈਨੋਨ ਹੁੰਦੀ ਹੈ। ਸਰਵ ਸਧਾਰਨ ਤੌਰ ਤੇ, ਕਿਸੇ ਵੇਰੀਏਬਲ ਨੂੰ ਪ੍ਰਸਤੁਤ ਕਰਨ ਲਈ ਤੁਹਾਨੂੰlog2(n)ਬਿਟਾਂ ਦੀ ਜਰੂਰਤ ਪੈਂਦੀ ਹੈ ਜੋnਮੁੱਲਾਂ ਵਿੱਚੋਂ ਇੱਕ ਮੁੱਲ ਲੈ ਸਕਦੇ ਹਨ ਜੇਕਰn, 2 ਦੀ ਇੱਕ ਘਾਤ (ਪਾਵਰ) ਹੋਵੇ। ਜੇਕਰ ਇਹ ਮੁੱਲ ਇੱਕ-ਬਰਾਬਰ ਹੀ ਸੰਭਵ (ਪਰੋਬੇਬਲ) ਹੋਣ, ਤਾਂ ਐਨਟ੍ਰੌਪੀ (ਸ਼ੈਨੋਨਾਂ ਵਿੱਚ) ਬਿੱਟਾਂ ਦੀ ਸੰਖਿਆ ਬਰਾਬਰ ਹੁੰਦੀ ਹੈ। ਬਿੱਟਾਂ ਦੀ ਗਿਣਤੀ ਅਤੇ ਸ਼ੈਨੋਨਾਂ ਦਰਮਿਆਨ ਸਮਾਨਤਾ ਸਿਰਫ ਉਦੋਂ ਲਾਗੂ ਰਹਿੰਦੀ ਹੈ ਜਦੋਂ ਸਾਰੇ ਨਿਕਲਣ ਵਾਲੇ ਨਤੀਜੇ ਇੱਕ ਸਮਾਨ ਹੀ ਪਰੋਬੇਬਲ (ਖੋਜਣਯੋਗ) ਹੋਣ। ਜੇਕਰ ਕੋਈ ਇੱਕ ਘਟਨਾ ਬਾਕੀ ਘਟਨਾਵਾਂ ਨਾਲ਼ੋਂ ਜਿਆਦਾ ਪ੍ਰੋਬੇਬਲ ਹੋਵੇ, ਤਾਂ ਉਸ ਘਟਨਾ ਦਾ ਨਿਰੀਖਣ ਘੱਟ ਸੂਚਨਾਤਮਿਕ ਹੁੰਦਾ ਹੈ। ਇਸਦੇ ਉਲਟ ਵਿਰਲੀਆਂ ਘਟਨਾਵਾਂ ਨਿਰੀਖਣ ਹੋਣ ਤੇ ਜਿਆਦਾ ਸੂਚਨਾ ਮੁਹੱਈਆ ਕਰਵਾਉਂਦੀਆਂ ਹਨ। ਕਿਉਂਕਿ ਘੱਟ ਪਰੋਬੇਬਲ ਘਟਨਾਵਾਂ ਦਾ ਨਿਰੀਖਣ ਹੋਰ ਜਿਆਦਾ ਵਿਰਲਾ ਵਾਪਰਦਾ ਹੈ, ਇਸ ਕਾਰਨ ਸ਼ੁੱਧ ਅਸਰ ਇਹ ਪੈਂਦਾ ਹੈ ਕਿ ਗੈਰ-ਇੱਕਸਾਰ ਵੰਡੇ ਆੰਕੜੇ ਤੋਂ ਪ੍ਰਾਪਤ ਐਨਟ੍ਰੌਪੀ (ਜੋ ਔਸਤ ਸੂਚਨਾ ਦੇ ਤੌਰ ਤੇ ਮੰਨੀ ਜਾਂਦੀ ਹੈ),log2(n)ਤੋਂ ਘੱਟ ਹੁੰਦੀ ਹੈ। ਜਦੋਂ ਨਤੀਜਾ ਨਿਸ਼ਚਿਤ ਹੁੰਦਾ ਹੈ ਤਾਂ ਐਨਟ੍ਰੌਪੀ ਜ਼ੀਰੋ ਰਹਿੰਦੀ ਹੈ। ਸ਼ੈਨੋਨ ਐਨਟ੍ਰੌਪੀ ਇਹਨਾਂ ਸਾਰੀਆਂ ਵਿਚਾਰਾਂ ਨੂੰ ਇਨਬਿੰਨ ਕੁਆਂਟੀਫਾਈ ਕਰਦੀ ਹੈ ਜਦੋਂ ਸੋਨਮੇ ਦੀ ਪ੍ਰੋਬੇਬਿਲਿਟੀ ਡਿਸਟ੍ਰੀਬਿਊਸ਼ਨ ਗਿਆਤ (ਪਤਾ) ਹੁੰਦੀ ਹੈ। ਨਿਰੀਖਤ ਘਟਨਾਵਾਂ (ਸੰਦੇਸ਼ਾਂ) ਦਾ ਅਰਥ ਐਨਟ੍ਰੌਪੀ ਦੀ ਪਰਿਭਾਸ਼ਾ ਵਿੱਚ ਕੋਈ ਵਾਸਤਾ ਨਹੀਂ ਰੱਖਦਾ। ਐਨਟ੍ਰੌਪੀ ਸਿਰਫ ਕਿਸੇ ਵਿਸ਼ੇਸ਼ ਘਟਨਾ ਨੂੰ ਨਿਰੀਖਣ ਕਰਨ ਦੀ ਪ੍ਰੋਬੇਬਿਲਿਟੀ ਨੂੰ ਹੀ ਲੈਂਦੀ ਹੈ, ਇਸਲਈ ਇਸਦੇ ਦੁਆਰਾ ਸਾਂਭੀ ਗਈ ਸੂਚਨਾ ਪਿੱਛੇ ਛੁਪੀ ਪ੍ਰੋਬੇਬਿਲਿਟੀ ਡਿਸਟ੍ਰੀਬਿਊਸ਼ਨ ਬਾਬਤ ਸੂਚਨਾ ਹੁੰਦੀ ਹੈ, ਨਾ ਕਿ ਖੁਦ ਘਟਨਾਵਾਂ ਦਾ ਅਰਥ ਹੀ ਹੁੰਦੀ ਹੈ।ਆਮਤੌਰ ਤੇ,ਐਨਟ੍ਰੌਪੀਅਵਿਵਸਥਾ ਜਾਂ ਅਨਿਸ਼ਚਿਤਿਤਾ ਵੱਲ ਇਸ਼ਾਰਾ ਕਰਦੀ ਹੈ। ਸ਼ੈਨੋਨ ਐਨਟ੍ਰੌਪੀਕਲਾਓਡੇ ਈ.', 'pa-guruਸ਼ੈਨੋਨਵੱਲੋਂ 1948 ਵਿੱਚ ਆਪਣੇ ਪੇਪਰ \"ਏ ਮੈਥੇਮੈਟੀਕਲ ਥਿਊਰੀ ਔਫ ਕਮਿਊਨੀਕੇਸ਼ਨ\"[1]ਵਿੱਚ ਪੇਸ਼ ਕੀਤੀ ਗਈ ਸੀ। ਸ਼ੈਨੋਨ ਐਨਟ੍ਰੌਪੀ ਕਿਸੇ ਸੂਚਨਾ ਸੋਮੇ ਦੀਹਾਨੀਹੀਣਐਨਕੋਡਿੰਗ(ਸੰਕਵੇ-ਬੱਧਤਾ) ਦੀ ਸਭ ਤੋਂ ਜਿਆਦਾ ਚੰਗੀ ਤਰਾਂ ਸੰਭਵ ਔਸਤ ਲੰਬਾਈ ਜਾਂਕੰਪ੍ਰੈਸ਼ਨਉੱਤੇ ਇੱਕ ਸ਼ੁੱਧ ਹੱਦ ਮੁਹੱਈਆ ਕਰਵਾਉਂਦੀ ਹੈ।ਰੇਨਯੀ ਐਨਟ੍ਰੌਪੀਸ਼ੈਨੋਨ ਐਨਟ੍ਰੌਪੀ ਸਦਾ ਸਰਵ ਸਧਾਰਨ ਕਰਨ ਕਰਦੀ ਹੈ। ਵਜਾਣ-ਪਛਾਣ[ਸੋਧੋ]ਐਨਟ੍ਰੌਪੀ,ਅਵਸਥਾਦੀ ਅਨਿਸ਼ਚਿਤਿਤਾ ਦਾ ਇੱਕ ਨਾਪ ਹੁੰਦਾ ਹੈ, ਜਾਂ ਇਸਦੇ ਸਮਾਨ ਕਹੀਏ ਤਾਂ, ਅਵਸਥਾ ਦੀਔਸਤਨ ਸੂਚਨਾ ਸਮੱਗਰੀਹੁੰਦੀ ਹੈ। ਇਹਨਾਂ ਸ਼ਬਦਾਂ ਦੀ ਇੱਕ ਗਹਿਰੀ ਸਮਝ ਪ੍ਰਾਪਤ ਕਰਨ ਲਈ, ਕਿਸੇ ਰਾਜਨੀਤਕ ਵੋਟ ਦੀ ਉਦਾਹਰਨ ਲਓ। ਆਮਤੌਰ ਤੇ, ਅਜਿਹੀਆਂ ਵੋਟਾਂ ਵੋਟਾਂ ਦੇ ਨਤੀਜਿਆਂ ਦੇ ਪਹਿਲਾਂ ਨਾ ਪਤਾ ਹੋਣ ਕਾਰਨ ਹੁੰਦੀਆਂ ਹਨ। ਦੂਜੇ ਸ਼ਬਦਾਂ ਵਿੱਚ, ਵੋਟਾਂ ਦਾ ਨਤੀਜਾ ਸਾਪੇਖਿ ਤੌਰ ਤੇ ਅਨਿਸ਼ਚਿਤ ਹੁੰਦਾ ਹੈ, ਅਤੇ ਸੱਚਮੁੱਚ ਵੋਟਾਂ ਦਾ ਕਾਰਜ ਕਰਨਾ ਅਤੇ ਨਤੀਜਿਆਂ ਨੂੰ ਪੜਨਾ ਕੁੱਝ ਨਵੀਂਇਨਫਰਮੇਸ਼ਨਦਿੰਦਾ ਹੈ; ਇਹ ਸਿਰਫ ਇਹ ਕਹਿਣ ਦੇ ਵੱਖਰੇ ਤਰੀਕੇ ਹਨ ਕਿ ਵੋਟ ਨਤੀਜਿਆਂ ਦੀ ਇੱਕ ਪੂਰਵ ਐਨਟ੍ਰੌਪੀ ਜਿਆਦਾ ਹੁੰਦੀ ਹੈ। ਹੁਣ, ਓਸ ਮਾਮਲੇ ਤੇ ਵਿਚਾਰ ਕਰੋ ਕਿ ਪਹਿਲੀ ਚੋਣ ਤੋਂ ਕੁੱਝ ਦੇਰ ਬਾਦ ਫੇਰ ਤੋਂ ਓਹੀ ਵੋਟਾਂ ਪੁਆਈਆਂ ਜਾਂਦੀਆੰ ਹਨ। ਕਿਉਂਕਿ ਵੋਟਾਂ ਦਾ ਨਤੀਜਾ ਪਹਿਲਾਂ ਹੀ ਪਤਾ ਹੁੰਦਾ ਹੈ, ਇਸਲਈ ਦੂਜੀ ਵਾਰ ਦੀਆਂ ਚੋਣਾਂ ਦਾ ਨਤੀਜਾ ਚੰਗੀ ਤਰਾਂ ਅਨੁਮਾਨਿਤ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ ਅਤੇ ਨਤੀਜਾ ਜਿਆਦਾ ਨਵੀਨ ਜਾਣਕਾਰੀ ਨਹੀਂ ਰੱਖਦੇ ਹੋਣਗੇ; ਇਸ ਮਾਮਲੇ ਵਿੱਚ, ਦੂਜੀ ਚੋਣ ਨਤੀਜੇ ਦੀ ਪੂਰਵ-ਐਨਟ੍ਰੌਪੀ ਪਹਿਲੀ ਤੋਂ ਸਾਪੇਖਿਕ ਤੌਰ ਤੇ ਘੱਟ ਹੋਵੇਗੀ।ਹੁਣ ਸਿੱਕੇ ਦੇ ਟੌਸ ਦੀ ਉਦਾਹਰਨ ਲਓ। ਮੰਨ ਲਓ ਹੈੱਡ ਤੇ ਟੇਲ ਆਉਣ ਦੀ ਪ੍ਰੋਬੇਬਿਲਿਟੀ ਇੱਕ ਬਰਾਬਰ ਹੀ ਹੋਵੇ, ਤਾਂ ਸਿੱਕੇ ਦੇ ਟੌਸ ਦੀ ਐਨਟ੍ਰੌਪੀ ਵੱਧ ਤੋਂ ਵੱਧ ਜਿਆਦਾ ਹੋ ਸਕਦੀ ਹੈ। ਅਜਿਹਾ ਇਸ ਕਾਰਨ ਹੁੰਦਾ ਹਰੈ ਕਿਉਂਕਿ ਵਕਤ ਤੋਂ ਅੱਗੇ ਸਿੱਕੇ ਦੇ ਟੌਸ ਦਾ ਨਤੀਜਾ ਅਨੁਮਾਨਿਤ ਕਰਨ ਦਾ ਕੋਈ ਤਰੀਕਾ ਨਹੀਂ ਹੁੰਦਾ: ਅਸੀਂ ਵੱਧ ਤੋਂ ਵੱਧ ਇਹ ਕਰ ਸਕਦੇ ਹਾਂ ਕਿ ਇਹ ਅਨੁਮਾਨ ਲਗਾ ਸਕਦੇ ਹਾਂ ਕਿ ਸਿੱਕਾ ਟੌਸ ਕਰਨ ਤੇ ਹੈੱਡ ਆਏਗਾ, ਅਤੇ ਸਾਡਾ ਅਨੁਮਾਨ ½ ਪ੍ਰੋਬੇਬਿਲਿਟੀ ਨਾਲ ਸਹੀ ਰਹਿੰਦਾ ਹੈ। ਅਜਿਹਾ ਕੋਈ ਸਿੱਕੇ ਦਾ ਟੌਸ ਐਨਟ੍ਰੌਪੀ ਦਾ ਇੱਕ ਸ਼ੈਨੌਨ ਰੱਖਦਾ ਹੈ ਕਿਉਂਕਿ ਦੋ ਸੰਭਵ ਨਤੀਜੇ ਹੁੰਦੇ ਹਨ ਜੋ ਬਰਾਬਰ ਪ੍ਰੋਬੇਬਿਲਿਟੀ ਨਾਲ ਹੁੰਦੇ ਹਨ, ਅਤੇ ਵਾਸਤਵਿਕ ਨਤੀਜੇ ਨੂੰ ਜਾਣਨਾ ਸੂਚਨਾ ਦਾ ਇੱਕ ਸ਼ੈਨੌਨ ਰੱਖਦਾ ਹੈ। ਇਸ ਦੇ ਵਿਰੁੱਧ ਤਰੀਕੇ ਵਿੱਚ, ਬਗੈਰ ਕਿਸੇ ਟੇਲ ਤੋਂ ਸਿਰਫ ਦੋਵੇਂ ਹੀ ਹੈੱਡਾਂ ਵਾਲੇ ਸਿੱਕੇ ਵਾਸਤੇ ਐਨਟ੍ਰੌਪੀ 0 ਹੁੰਦੀ ਹੈ ਕਿਉਂਕਿ ਹਮੇਸ਼ਾ ਹੀ ਸਿੱਕੇ ਦਾ ਪਾਸਾ ਹੈੱਡ ਹੀ ਦਿਖਾਏਗਾ, ਅਤੇ ਨਤੀਜਾ ਪੂਰੀ ਤਰਾਂ ਭਵਿੱਖ ਬਾਣੀ ਹੋ ਸਕਦਾ ਹੈ। ਇਸਦੇ ਤੁੱਲ, ਇੱਕ ਸਮਾਨ ਮੁੱਲਾਂ ਵਾਲੇ ਇੱਕ ਬਾਇਨਰੀ ਬਿੱਟ ਦੀlog2\\u20612=12=1}ਸ਼ੈਨੋਨ ਸ਼ੈਨੋਨ ਐਨਟ੍ਰੌਪੀ ਹੁੰਦੀ ਹੈ। ਇਸੇਤਰਾਂ, ਸਮਾਨ-ਪ੍ਰੋਬੇਬਿਲਿਟੀ ਵਾਲੇ ਇੱਕਟ੍ਰਿਟਵਿੱਚ ਸੂਚਨਾ ਦੇlog2\\u206133}(ਲੱਗਪਗ 1.58496) ਸ਼ੈਨਨ ਹੁੰਦੇ ਹਨ ਕਿਉਂਕਿ ਇਹ ਤਿੰਨ ਮੁੱਲਾਂ ਵਿੱਚੋਂ ਇੱਕ ਮੁੱਲ ਰੱਖ ਸਕਦਾ ਹੈ।ਪਰਿਭਾਸ਼ਾ[ਸੋਧੋ]ਬੋਲਟਜ਼ਮਨ ਦੀ Η-ਥਿਊਰਮ, ਤੋਂ ਬਾਦ ਸ਼ੈਨਨ ਨੇ ਐਨਟ੍ਰੌਪੀΗ(ਅਨਿਰੰਤਰ ਮਨਚਾਹੇ ਅਸਥਿਰਾਂਕਾਂXਦਾ ਗਰੀਕ ਕੈਪੀਟਲ ਅੱਖਰਈਟਾ) ਜੋ ਸੰਭਵ ਮੁੱਲਾਂ ਵਾਲਾ ਹੁੰਦਾ ਹੈ ਅਤੇਪ੍ਰੋਬੇਬਿਲਟੀ ਮਾਸ ਫੰਕਸ਼ਨP(X)ਨੂੰ ਇਸ ਤਰ੍ਹਾਂ ਦਰਸਾਇਆ:H(X)=E[I(X)]=E[−ln\\u2061(P(X))].', 'pa-guru(X)=\\\\mathrm [\\\\mathrm (X)]=\\\\mathrm [-\\\\ln(\\\\mathrm (X))].', 'pa-guru}ਉਦਾਹਰਨ[ਸੋਧੋ]ਦਲੀਲ[ਸੋਧੋ]ਪਹਿਲੂ[ਸੋਧੋ]ਥਰਮੋਡਾਇਨਾਮਿਕ ਐਨਟ੍ਰੌਪੀ ਪ੍ਰਤਿ ਸਬੰਧ[ਸੋਧੋ]ਸੂਚਨਾ ਸਮੱਗਰੀ ਦੇ ਤੌਰ ਤੇ ਐਨਟ੍ਰੌਪੀ[ਸੋਧੋ]ਵਿਭਿੰਨਤਾ ਦੇ ਨਾਪ ਦੇ ਤੌਰ ਤੇ ਐਨਟ੍ਰੌਪੀ[ਸੋਧੋ]ਡਾਟਾ ਕੰਪ੍ਰੈਸ਼ਨ[ਸੋਧੋ]ਜਾਣਕਾਰੀ ਦੇ ਭੰਡਾਰੀਕਰਨ ਅਤੇ ਪ੍ਰਸਾਰ ਦੀ ਸੰਸਾਰ ਦੀ ਤਕਨੀਕੀ ਸਮਰਥਾ[ਸੋਧੋ]ਸੂਚਨਾ ਸਮੱਗਰੀ ਦੇ ਤੌਰ ਤੇ ਐਨਟ੍ਰੌਪੀ ਦੀਆਂ ਕਮੀਆਂ[ਸੋਧੋ]ਕ੍ਰਿਪਟੋਗ੍ਰਾਫੀ ਅੰਦਰ ਐਨਟ੍ਰੌਪੀ ਦੀਆਂ ਕਮੀਆਂ[ਸੋਧੋ]ਇੱਕ ਮਾਰਕੋਵ ਪ੍ਰੋਸੈੱਸ ਦੇ ਤੌਰ ਤੇ ਡਾਟਾ[ਸੋਧੋ]b-ਏਰੀ ਐਨਟ੍ਰੌਪੀ[ਸੋਧੋ]ਐਫੀਸ਼ੈਂਸੀ[ਸੋਧੋ]ਲੱਛਣਬੱਧਤਾ[ਸੋਧੋ]ਨਿਰੰਤਰਤਾ[ਸੋਧੋ]ਸਮਿੱਟਰੀ[ਸੋਧੋ]ਉੱਚਤਮ[ਸੋਧੋ]ਜੋੜ-ਬੱਧਤਾ[ਸੋਧੋ]ਹੋਰ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ[ਸੋਧੋ]ਨਿਰੰਤਰ ਮਾਮਲੇ ਤੱਕ ਅਨਿਰੰਤਰ (ਡਿਸਕ੍ਰੀਟ) ਐਨਟ੍ਰੌਪੀ ਨੂੰ ਵਧਾਓਣਾ[ਸੋਧੋ]ਡਿੱਫ੍ਰੈਂਸ਼ੀਅਲ ਐਨਟ੍ਰੌਪੀ[ਸੋਧੋ]ਡਿਸਕ੍ਰੀਟ (ਅਨਿਰੰਤਰ) ਬਿੰਦੂਆਂ ਦੀ ਹੱਦਬੰਦੀ ਕਰਨ ਵਾਲੀ ਘਣਤਾ (ਡੈਂਸਟੀ)[ਸੋਧੋ]ਸਾਪੇਖਿਕ (ਰਿਲੇਟਿਵ) ਐਨਟ੍ਰੌਪੀ[ਸੋਧੋ]ਸੰਯੋਜਕਾਤਮਿਕਤਾ ਅੰਦਰ ਵਰਤੋਂ[ਸੋਧੋ]ਲੂਮਿਸ-ਵਿਟਨੇ ਅਸਮਾਨਤਾ[ਸੋਧੋ]ਬਾਇਨੌਮੀਅਲ ਗੁਣਾਂਕ (ਕੋਐਫੀਸ਼ੈਂਟ) ਪ੍ਰਤਿ ਸੰਖੇਪ ਅਨੁਮਾਨ[ਸੋਧੋ]ਇਹ ਵੀ ਦੇਖੋ[ਸੋਧੋ]ਹਵਾਲੇ[ਸੋਧੋ]ਹੋਰ ਲਿਖਤਾਂ[ਸੋਧੋ]ਇਨਫ੍ਰਮੇਸ਼ਨ ਥਿਊਰੀ ਉੱਤੇ ਟੈਕਸਟਬੁਕਾਂ[ਸੋਧੋ]ਬਾਹਰੀ ਲਿੰਕ[ਸੋਧੋ]ਇਸ ਲੇਖਦੁਆਰਾਬਾਹਰੀ ਲਿੰਕਾਂਦੀਵਰਤੋਂ ਹੋ ਸਕਦਾ ਹੈ ਵਿਕੀਪੀਡੀਆ ਦੀਆਂ ਪੋਲੀਸੀਆਂ ਜਾਂ ਦਿਸ਼ਾ-ਨਿਰਦੇਸ਼ ਨਾ ਪੂਰੇ ਕਰਦੇ ਹੋਣ.ਕਿਰਪਾ ਕਰਕੇਫਾਲਤੂਜਾਂਅਢੁਕਵੇਂਬਾਹਰੀ ਲਿੱਕਾਂ ਨੂੰ ਹਟਾ ਕੇ , ਅਤੇਫੁਟਨੋਟ ਹਵਾਲਿਆਂਵਿੱਚ ਵਰਤੋਯੋਗ ਲਿੰਕਾਂ ਨੂੰ ਤਬਦੀਲ ਕਰਕੇਇਸ ਲੇਖ ਵਿੱਚ ਸੋਧ ਕਰੋ.', 'pa-guru(June 2015)Library resourcesaboutਐਨਟ੍ਰੌਪੀ (ਇਨਫ੍ਰਮੇਸ਼ਨ ਥਿਊਰੀ)Online booksResources in your libraryResources in other librariesHazewinkel, Michiel, ed.', 'pa-guru(2001),\"Entropy\",ਗਣਿਤ ਦਾ ਵਿਸ਼ਵਕੋਸ਼, ਸਪਰਿੰਗਰ,ISBN978-1-55608-010-4Introduction to entropy and informationonPrincipia Cybernetica WebEntropyan interdisciplinary journal on all aspect of the entropy concept.', 'pa-guruOpen access.Description of information entropy from \"Tools for Thought\" by Howard RheingoldArchived15 May 2011[Date mismatch]at theWayback Machine.A java applet representing Shannon\\'s Experiment to Calculate the Entropy of EnglishSlides on information gain and entropyArchived7 February 2019[Date mismatch]at theWayback Machine.An Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science– a wikibook on the interpretation of the concept of entropy.A Light Discussion and Derivation of EntropyNetwork Event Detection With Entropy Measures, Dr. Raimund Eimann, University of Auckland, PDF; 5993 kB – a PhD thesis demonstrating how entropy measures may be used in network anomaly detection.Rosetta Coderepository of implementations of Shannon entropy in different programming languages.', 'pa-guruInformation Theory for Intelligent PeopleArchived13 June 2020[Date mismatch]at theWayback Machine.. Short introduction to the axioms of information theory, entropy, mutual information, Kullback–Liebler divergence, and Jensen–Shannon distance.ਫਰਮਾ:Compression MethodsAuthority control databasesInternationalFASTNationalSpainFranceBnF dataGermanyIsraelUnited StatesJapanCzech Republic↑Shannon, Claude E.(ਜੁਲਾਈ–ਅਕਤੂਬਰ 1948).', 'pa-guruA Mathematical Theory of Communication.Bell System Technical Journal.27(3): 379–423.doi:10.1002/j.1538-7305.1948.tb01338.x.', 'pa-guru(PDF, archived fromhere)\"https://pa.wikipedia.org/w/index.php?title=ਐਨਟ੍ਰੌਪੀ_(ਇਨਫ੍ਰਮੇਸ਼ਨ_ਥਿਊਰੀ)&oldid=616866\" ਤੋਂ ਲਿਆਸ਼੍ਰੇਣੀਆਂ:Articles needing additional references from April 2012Articles with invalid date parameter in templateAll articles needing additional referencesUse dmy datesWikipedia external links cleanup from June 2015Wikipedia spam cleanup from June 2015Webarchive template warningsArticles with FAST identifiersPages with authority control identifiers needing attentionArticles with BNE identifiersArticles with BNF identifiersArticles with BNFdata identifiersArticles with GND identifiersArticles with J9U identifiersArticles with NDL identifiersArticles with NKC identifiersਐਨਟ੍ਰੌਪੀ ਅਤੇ ਸੂਚਨਾਇਨਫ੍ਰਮੇਸ਼ਨ ਥਿਊਰੀਸਟੈਟਿਸਟੀਕਲ ਰੈਂਡੱਮਨੈੱਸਲੁਕਵੀਂਆਂ ਸ਼੍ਰੇਣੀਆਂ:Webarchive template wayback linksArticles with LCCN identifiersਇਸ ਸਫ਼ੇ ਵਿੱਚ ਆਖ਼ਰੀ ਸੋਧ 4 ਅਕਤੂਬਰ 2022 ਨੂੰ 19:57 ਵਜੇ ਹੋਈ।ਇਹ ਲਿਖਤCreative Commons Attribution/Share-Alike Licenseਦੇ ਤਹਿਤ ਉਪਲਬਧ ਹੈ; ਵਾਧੂ ਸ਼ਰਤਾਂ ਲਾਗੂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਤਫ਼ਸੀਲ ਲਈਵਰਤਣ ਦੀਆਂ ਸ਼ਰਤਾਂਵੇਖੋ।Wikipedia® ਮੁਨਾਫ਼ਾ ਨਾ ਕਮਾਉਣ ਵਾਲ਼ੀਵਿਕੀਮੀਡੀਆ ਫ਼ਾਊਂਡੇਸ਼ਨ, ਇਨਕੌਰਪੋਰੇਟਡਦਾ ਰਜਿਸਟ੍ਰਡ ਟ੍ਰੇਡਮਾਰਕ ਹੈ।ਪਰਦਾ ਨੀਤੀਵਿਕੀਪੀਡੀਆ ਬਾਰੇਦਾਅਵੇਕੋਡ ਆਫ਼ ਕੰਡਕਟਵਿਕਾਸਕਾਰਅੰਕੜੇਕੂਕੀ ਤਫਸੀਲਾਂਮੋਬਾਈਲੀ ਦਿੱਖਸੀਮਤ ਸਮੱਗਰੀ ਚੌੜਾਈ ਨੂੰ ਟੌਗਲ ਕਰੋ', 'bgЕнтропия на Шанън – УикипедияНаправо към съдържаниетоГлавно менюГлавно менюпреместване към страничната лентаскриванеНавигацияНачална страницаСлучайна статияНаправете дарениеПолезноПоследни промениОбщи разговориОбсъждани статииАдминистраториИзтриванияЗа контактиВключете се!Защо?ПомощКартинкиПоведениеИзпробванеНова статияОбщувайтеБлог на общносттаФейсбук страницаФейсбук група🎮 ДискордТелеграмIRCGitHubТърсенеТърсенеСъздаване на сметкаВлизанеЛични инструментиСъздаване на сметкаВлизанеСтраници за излезли от системата редакторинаучете повечеПриносиБеседаСъдържаниепреместване към страничната лентаскриванеНачало1Минимум и максимум на ентропията (липсата на информация)2Връзка с ентропията3Смисъл на константата C4Източници5Външни препраткиСкриване/показване на съдържаниетоЕнтропия на Шанън45 езикаAfrikaansالعربيةBoarischBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語РедактиранеСтатияБеседабългарскиПрегледРедактиранеРедактиране на кодаИсторияИнструментиИнструментипреместване към страничната лентаскриванеДействияПрегледРедактиранеРедактиране на кодаИсторияОсновниКакво сочи насамСвързани промениКачване на файлСпециални странициПостоянна препраткаИнформация за страницатаЦитиране на статиятаКратък URL адресИзтегляне на QR кодОбект в УикиданниПодстранициОтпечатване/изнасянеСъздаване на книгаИзтегляне като PDFВерсия за печатВ други проектиОбщомедияот Уикипедия, свободната енциклопедияВ рамките натеорията на информацията, централен обект на изследване е функциятаентропия на Шанънилилипса на информация, която ще обозначаваме сS.Нека обозначим вероятностите за различните събития от даденовероятностно пространствосPi}.', 'bgЕнтропията на Шанън, която понякога се нарича и „липса на информация“, встатистическата механикае еквивалентна наентропията.', 'bgФункцията е дефинирана по следния начин:S=−C∑iNPiln\\u2061Pi=C⟨ln\\u2061Pi⟩^P_\\\\ln P_=C\\\\langle \\\\ln P_\\\\rangle }, където C е константа, чийто смисъл ще стане ясен по-късно, а N е броят на възможните резултати (в статистическата механика – броят на възможнитемикросъстояния)Нека например разгледаме ситуацията залагане на зарове.', 'bgПритежателят на зара е измамник, който е обработил зара така, че да пада на определено число.', 'bgПознаването на това число намалява стойността на функцията липса на информация, която понякога, за по-кратко се нарича и ентропия.Минимум и максимум на ентропията (липсата на информация)[редактиране|редактиране на кода]Ако зарътвинагипада на една страна, да кажем 4, то тогава вероятносттаP4}е равна на 1, а всички други вероятности са 0.', 'bgВ този случай, функцията липса на информация става равна на:S=−C∑iNPiln\\u2061Pi=−C(P1ln\\u2061P1+P2ln\\u2061P2+P3ln\\u2061P3+P4ln\\u2061P4+P5ln\\u2061P5+P6ln\\u2061P6)^P_\\\\ln P_=-C(P_\\\\ln P_+P_\\\\ln P_+P_\\\\ln P_+P_\\\\ln P_+P_\\\\ln P_+P_\\\\ln P_)}S=−C(0ln\\u2061P1+0ln\\u2061P2+0ln\\u2061P3+1ln\\u20611+0ln\\u2061P5+0ln\\u2061P6)+0\\\\ln P_+0\\\\ln P_+1\\\\ln 1+0\\\\ln P_+0\\\\ln P_)}и тъй като всички членове на това уравнение са равни нанула(заб.limx→0xln\\u2061x=0x\\\\ln x=0}), имаме:S=0.Тоест, липсата на информация е нулева, когато със сигурност знаем изхода от даден опит, което, както виждаме, може да се изрази математически с така дефинираната функция липса на информация.', 'bgОбратно, функцията липса на информация има максимум, когато отделните вероятности са равни.', 'bgЗа да докажем това твърдение ще използваме метода смножителите на Лагранж:От условието за нормировка следва:∑iPi=1P_=1}(сборът на всички вероятности е единица).', 'bgСледователно, прибавянето на тази сума към ентропията и изваждането на единица не променя нищо:S=S+λ(∑iNPi−1)^P_-1)}Търсим точките, в частнатапроизводнапоPi}се анулира:∂S∂Pi=−C(ln\\u2061Pi−1)+λPi=0}=-C(\\\\ln P_-1)+\\\\lambda P_=0}илиPi=e−C+λλ=e^}тоест, виждаме, че стойността на кое да еPi}зависи само от константи, т.е.', 'bgсамото то е константа, която може да обозначим, например, сμ.', 'bgСледователно, от условието за нормировка следва:∑iNPi=∑iNμ=μN=1^P_=\\\\sum _^\\\\mu =\\\\mu N=1}илиμ=Pi=1N∀i=\\\\,\\\\,\\\\,\\\\,\\\\forall i}Видяхме, че когато липсата на информация е в максимум, вероятностите за различните изходи са равни помежду си.', 'bgВ рамките на статистическата механика това се интерпретира по следния начин: когато системата е в равновесие, ентропията е в максимум.', 'bgСлед повторно изкарване от равновесие, съгласно втория принцип на термодинамиката, ентропията ще нарасне, докато отново не достигне максимум.Връзка с ентропията[редактиране|редактиране на кода]Както видяхме, когато ентропията (липса на информация) е максимална, всичкиPi}-та са равни на1N}}.', 'bgТогава:S=−C∑iN1Nln\\u20611N^\\\\ln }S=C1Nln\\u2061N∑iN=Cln\\u2061N\\\\ln N\\\\sum _^=C\\\\ln N}Ако заместим C с kBполучаваме формулата наБолцманза ентропията, която показва връзката между ентропията и липсата на информация и оправдава наименованието „ентропия на Шанън“ за функцията „липса на информация“.Смисъл на константата C[редактиране|редактиране на кода]Този път измамникът играе не на зарове, а на ези-тура.', 'bgАко нямаме информация за системата, вероятността монетата на падне на ези е равна на вероятността монетата да падне на тура,Pheads=Ptails=12=P_=}}.', 'bgАко знаем на коя страна ще падне монетата, липсата на информация е нулева.', 'bgВ теорията на информацията, Константата С е дефинирана така, че разликата в информацията за тези два случая да е 1, т.е.', 'bg:1=−C(12ln\\u206112+12ln\\u206112)=C×2ln\\u20612}\\\\ln }+}\\\\ln })=C\\\\times 2\\\\ln 2}C=12ln\\u20612}}Виждаме, че C играе ролята на „бит информация“, т.е.', 'bgв теорията на информацията един бит е12ln\\u20612}}.', 'bgКакто видяхме, в статистическата механика, ролята на С се играе отkB}, така че един „бит ентропия“ в статистическата механика еконстантата на Болцман,kB}Концепцията е представена за пръв път отКлод Шанънпрез1948г.', 'bgв неговата статия „Математическа теория на комуникациите“.Източници[редактиране|редактиране на кода]Mouhanna, D; Sator, N., Cours et TDs de Mécanique statistique, Université Pierre et Marie Curie, 2008Външни препратки[редактиране|редактиране на кода]((en))Математическа теория на комуникациитеАрхив на оригинала от2017-12-15 вWayback Machine.– оригиналната публикация на Шанън от1948г.', 'bgПосетена на 31 юли 2008.Взето от „https://bg.wikipedia.org/w/index.php?title=Ентропия_на_Шанън&oldid=11629600“.Категории:Дискретна математикаИнформатикаМатематически функцииСкрита категория:Шаблон Webarchive с препратки към Wayback MachineПоследната промяна на страницата е извършена на 28 декември 2022 г. в 06:57 ч.Текстът е достъпен под лицензаCreative Commons Признание-Споделяне на споделеното; може да са приложени допълнителни условия.', 'bgЗа подробности вижтеУсловия за ползване.ПоверителностЗа контакт с УикипедияПредупреждениеКодекс на поведениеЗа разработчициСтатистикаИзползване на „бисквитки“Мобилен изгледПревключване на ограничена дължина на съдържанието', 'glEntropía da información - Wikipedia, a enciclopedia libreSaltar ao contidoMenú principalMenú principalmover á barra lateralagocharNavegaciónPortadaPortal da comunidadeA TabernaActualidadeCambios recentesArtigos de calidadePáxina ao chouAxudaDoazónsProcuraProcurarCrear unha contaAcceder ao sistemaFerramentas persoaisCrear unha contaAcceder ao sistemaPáxinas para os editores sen a sesión iniciadamáis informaciónContribuciónsConversaContidosmover á barra lateralagocharInicio1Relación coa entropía termodinámica2Concepto intuitivo3Definición formalMostrar ou agochar a subsección \"Definición formal\"3.1Exemplos3.2Información mutua4Propiedades5Codificador óptimoMostrar ou agochar a subsección \"Codificador óptimo\"5.1Exemplo6Entropía condicionalMostrar ou agochar a subsección \"Entropía condicional\"6.1Aplicación en criptoanálise6.1.1Exemplo7Entropía dun proceso estocásticoMostrar ou agochar a subsección \"Entropía dun proceso estocástico\"7.1Cociente de entropía8Notas9Véxase taménMostrar ou agochar a subsección \"Véxase tamén\"9.1Bibliografía9.2Outros artigos9.3Ligazóns externasMostrar ou agochar a táboa de contidosEntropía da información45 linguasAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Editar as ligazónsArtigoConversagalegoLerEditarEditar a fonteVer o historialFerramentasFerramentasmover á barra lateralagocharAcciónsLerEditarEditar a fonteVer o historialXeralPáxinas que ligan con estaCambios relacionadosPáxinas especiaisLigazón permanenteInformación da páxinaCitar esta páxinaXerar URL acurtadoDownload QR codeElemento de WikidataImprimir/exportarCrear un libroDescargar como PDFVersión para imprimirNoutros proxectosWikimedia CommonsNa Galipedia, a Wikipedia en galego.Nateoría da informaciónaentropía, tamén chamadaentropía da informacióneentropíade Shannon (en honra aClaude E. Shannon), mide a incerteza dunhafonte de información.A entropía tamén se pode considerar como a cantidade de información media que conteñen os símbolos usados.', 'glOs símbolos con menor probabilidade son os que achegan maior información; por exemplo, se se considerase como sistema de símbolos as palabras nun texto, palabras frecuentes como «que», «o», «a» achegan pouca información, mentres que palabras menos frecuentes como «corren», «neno», «can» achegan máis información.', 'glSe dun texto dado se borra un «que», seguramente non afectará á comprensión e sobreentenderase, non sendo así se se borra a palabra «neno» do mesmo texto orixinal.', 'glCando todos os símbolos son igualmenteprobables(distribución de probabilidadeplana), todos achegan información relevante e a entropía é máxima.O concepto entropía emprégase entermodinámica,mecánica estatísticaeteoría da información.', 'glEn todos os casos a entropía concíbese como unha «medida da desorde» ou a «peculiaridade de certas combinacións».', 'glA entropía pode ser considerada como unha medida da incerteza e da información necesaria para, en calquera proceso, poder acoutar, reducir ou eliminar a incerteza.', 'glO concepto de información e o de entropía están basicamente relacionados entre si, aínda que se precisaron anos de desenvolvemento damecánica estatísticae dateoría da informaciónantes de que isto fose percibido.Relación coa entropía termodinámica[editar|editar a fonte]A entropía da teoría da información está estreitamente relacionada coaentropía termodinámica.', 'glNa termodinámica estúdase un sistema de partículas cuxos estadosX(usualmente posición e velocidade) teñen unha certadistribución de probabilidade, podendo ocupar varios microestados posibles (equivalentes aos símbolos na teoría da información).', 'glA entropía termodinámica é igual á entropía da teoría da información desa distribución (medida usando ologaritmo neperiano) multiplicada polaconstante de Boltzmannk, a cal permite pasar de nats (unidade semellante ao bit) aJ/K.', 'glCando todos os microestados son igualmente probables, a entropía termodinámica toma a formaklog(N).', 'glNun sistema illado, a interacción entre as partículas tende a aumentar a súa dispersión, afectando as súas posicións e as súas velocidades, o que causa que a entropía da distribución aumente co tempo até chegar a un certo máximo (cando o mesmo sistema é o máis homoxéneo e desorganizado posible); o que se denominaSegunda Lei da Termodinámica.', 'glA diferenza entre a cantidade de entropía que ten un sistema e o máximo que pode chegar a ter denomínaseneguentropía, e representa a cantidade de organización interna que ten o sistema.', 'glA partir desta última pódese definir aenerxía libre de Gibbs, que indica a enerxía que pode liberar o sistema ao aumentar a entropía até o seu máximo e pode ser transformada entraballo(enerxía mecánica útil) usando unhamáquina ideal de Carnot.', 'glCando un sistema recibe un fluxo de calor, as velocidades das partículas aumentan, o que dispersa a distribución e fai aumentar así a entropía.', 'glAsí, o fluxo de calor produce un fluxo de entropía na mesma dirección.Concepto intuitivo[editar|editar a fonte]Entropía da información nunensaio de BernoulliX(experimento aleatorio en queXpode tomar os valores 0 ou 1).', 'glA entropía depende da probabilidade P(X=1) de queXtome o valor 1.', 'glCando P(X=1)=0.5, todos os resultados posibles son igualmente probables, polo que o resultado é pouco predicible e a entropía é máxima.O concepto básico de entropía enteoría da informaciónten moito que ver coaincertezaque existe en calquera experimento ou sinal aleatorio.', 'glÉ tamén a cantidade de «ruído» ou «desorde» que contén ou libera un sistema.', 'glDesta forma, poderemos falar da cantidade de información que leva un sinal.Como exemplo, considérese un texto escrito en galego, codificado como unha cadea de letras, espazos esignos de puntuación(o noso sinal será unha cadea de caracteres).', 'glXa que, estatisticamente, algúns caracteres non son moi comúns (por exemplo, «w»), mentres outros si o son (como o «a»), a cadea de caracteres non será tan \"aleatoria\" como podería chegar a ser.', 'glObviamente, non se pode predicir con exactitude cal será o seguinte carácter na cadea, e iso faríaa aparentemente aleatoria.', 'glPero é a entropía a encargada de medir precisamente esa aleatoriedad, e foi presentada por Shannon no seu artigo de1948,A Mathematical Theory of Communication[1](\"Unha teoría matemática da comunicación\", en inglés).Shannon ofrece unha definición de entropía que satisfai as seguintes afirmacións:A medida de información debe ser proporcional (linear continua).', 'glÉ dicir, o cambio pequeno nunha das probabilidades de aparición dun dos elementos do sinal debe cambiar pouco a entropía.Se todos os elementos do sinal son equiprobables (igual de probables) á hora de aparecer, entón a entropía será máxima.Exemplos de máxima entropía: supóñase que estamos esperando dun texto, por exemplo un cable cunha mensaxe.', 'glEn devandito cable só se reciben as letras en minúscula doaaté oz, entón se a mensaxe que nos chega é \"qalmnbphijcdgketrsfuvxyzwño\" o cal posúe unha lonxitude de 27 caracteres, pódese dicir que esta mensaxe chega a nós coa máxima entropía (ou desorde posible); xa que é pouco probable que se poida prognosticar a entrada de caracteres, pois estes non se repiten nin están ordenados nunha forma predicible.Definición formal[editar|editar a fonte]Supóñase que un evento (variable aleatoria) ten un grao de indeterminación inicial igual ak(i.e.', 'glexistenkestados posibles) e supóñanse todos os estados equiprobables.', 'glEntón a probabilidade de que se dea unha desas combinacións seráp=1/k.', 'glEntón pódese representar a expresiónci}como:[a]ci=log2\\u2061(k)=log2\\u2061[1/(1/k)]=log2\\u2061(1/p)=log2\\u2061(1)⏟=0−log2\\u2061(p)=−log2\\u2061(p)=\\\\log _(k)=\\\\log _[1/(1/k)]=\\\\log _(1/p)=\\\\underbrace (1)} _-\\\\log _(p)=-\\\\log _(p)}Se agora cada un doskestados ten unha probabilidadepi}, entón a entropía virá dada pola suma ponderada da cantidade de información:[2][b]H=−p1log2\\u2061(p1)−p2log2\\u2061(p2)−....−pklog2\\u2061(pk)=−∑i=1kpilog2\\u2061(pi)\\\\log _(p_)-p_\\\\log _(p_)-....-p_\\\\log _(p_)=-\\\\sum _^p_\\\\log _(p_)}Polo tanto, a entropía dunha mensaxeX, denotada porH(X), é o valor medio ponderado da cantidade de información dos diversos estados da mensaxe:H(X)=−∑ip(xi)log2\\u2061p(xi)p(x_)\\\\log _p(x_)}que representa unha medida da incerteza media sobre unha variable aleatoria e polo tanto da cantidade de información.Exemplos[editar|editar a fonte]A entropía dunha mensaxeMde lonxitude 1 carácter que emprega o conxunto de caracteresASCII, supondo unha equiprobabilidade nos 256 caracteres ASCII, será:H(M)=log2\\u2061(256)=8(256)=8}Supóñase que o número de estados dunha mensaxe é igual a 3,M1,M2eM3onde a probabilidade deM1é 50 %, a deM225 % e a deM325 %.', 'glPolo tanto, a entropía da información é:Información mutua[editar|editar a fonte]A entropía pode verse como caso especial dainformación mutua.', 'glA información mutua de dúasvariables aleatorias, denotado porI(X;Y), é unha cantidade que mide a dependencia mutua das dúasvariables; é dicir, mide a redución da incerteza (entropía) dunha variable aleatoria,X, debido ao coñecemento do valor doutra variable aleatoria,Y.', 'glDa definición pódese concluír que, seXeYson iguais, entónI(X;X)=H(X).', 'gl[3]Propiedades[editar|editar a fonte]A entropía ten as seguintes propiedades:A entropía é non negativa.', 'glIsto é evidente xa que ao serpi}unha probabilidade entón0<pi≤1\\\\leq 1}.', 'glEntón, pódese dicir quelog2\\u2061pi≤0p_\\\\leq 0}e polo tanto−log2\\u2061pi≥0p_\\\\geq 0}.H≤loga\\u2061(n)(n)}, é dicir, a entropíaHestá limitada superiormente (cando é máxima) e non supón perda de información.Dado un proceso con posibles resultados con probabilidades relativas p1,...,pn, a funciónH(p1,…,pn),\\\\dots ,p_)\\\\,}é máxima no caso de quep1=⋯=pn=1/n=\\\\dots =p_=1/n\\\\,}.', 'glO resultado é intuitivo xa que se ten a maior incerteza da mensaxe, cando os valores posibles da variable son equiprobables.Dado un proceso con posibles resultados con probabilidades relativas p1,...,pn, a funciónH(p1,…,pn),\\\\dots ,p_)\\\\,}é nula no caso de quepi=0=0}para todo i, agás para unha clase, tal que:pj=1=1}.', 'glDe forma intuitiva pódese pensar que cando un ou máis estados teñen unha probabilidade alta, diminúe significativamente a entropía porque, como é lóxico, existe unha menor incerteza respecto á mensaxe que se recibirá.Codificador óptimo[editar|editar a fonte]Un codificador óptimo é aquel que emprega o mínimo número de bits para codificar unha mensaxe.', 'glUn codificador óptimo usará códigos curtos para codificar mensaxes frecuentes e deixará os códigos de maior lonxitude para aquelas mensaxes que sexan menos frecuentes.', 'glDesta formaoptimízaseo rendemento da canle ou zona de almacenamento e o sistema é eficiente en termos do número de bits para representar a mensaxe.Por exemplo, ocódigo Morseaprovéitase deste principio para optimizar o número de caracteres para transmitir a partir do estudo das letras máis frecuentes do alfabeto inglés.', 'glAínda que o código Morse non é un codificador óptimo, asigna ás letras máis frecuente códigos máis curtos.', 'glOutro exemplo sería oalgoritmo de Huffmande codificación que serve para compactar información.', 'gl[4]Este método baséase no codificador óptimo.', 'glPara iso o primeiro que fai é percorrer toda a información para atopar a frecuencia dos caracteres e logo a partir desta información busca o codificador óptimo por medio de árbores binarios.', 'glAlgunhas técnicas de compresión comoLZWoudeflaciónnon usan probabilidades dos símbolos illados, senón que usan as probabilidades conxuntas de pequenas secuencias de símbolos para codificar a mensaxe, polo que poden lograr un nivel de compresión maior.Pódese construír un codificador óptimo baseándose na entropía dunha variable aleatoria de informaciónX.', 'glEn efecto, a entropía dá o número medio de bits (se se usanlogaritmosde base 2) necesarios para codificar a mensaxe a través dun codificador óptimo e polo tanto determínase o límite máximo ao que se pode comprimir unha mensaxe usando un enfoque símbolo a símbolo sen ningunha perda de información (demostrado analiticamente por Shannon), o límite de compresión (en bits) é igual á entropía multiplicada pola lonxitude da mensaxe.', 'glReescribindo a ecuación de cálculo da entropía chégase a que:H(X)=−∑ip(xi)log2\\u2061p(xi)=∑i−p(xi)log2\\u2061p(xi)=∑ip(xi)[log2(1)−log2\\u2061(p(xi))]=∑xp(x)log2\\u2061(1/p(x))p(x_)\\\\log _p(x_)=\\\\sum _-p(x_)\\\\log _p(x_)=\\\\sum _p(x_)[log_(1)-\\\\log _(p(x_))]=\\\\sum _p(x)\\\\log _(1/p(x))}Polo tanto, a información (que se atopa definida en bits, dado que a base do logaritmo é 2) que achega un determinado valor ou símboloxi\\\\,\\\\!', 'gl}dunha variable aleatoria discretaXdefínese como:I(xi)=log2\\u20611p(xi)=−log2\\u2061p(xi))=\\\\log _)}}=-\\\\log _)}}Esta expresión representa o número necesario de bits para codificar a mensaxeXno codificador óptimo e polo tanto a entropía tamén se pode considerar como unha medida da información media contida en cada símbolo da mensaxe.Exemplo[editar|editar a fonte]Supóñase que o número de estados dunha mensaxe é igual a 3M1,M2eM3onde a probabilidade deM1é 50 %, a deM225 % e a deM325 %.ParaM1tense quelog2\\u2061[1/p(M1)]=log2\\u20612=1[1/p(M_)]=\\\\log _2=1}ParaM2tense quelog2\\u2061[1/p(M2)]=log2\\u20614=2[1/p(M_)]=\\\\log _4=2}ParaM3tense quelog2\\u2061[1/p(M3)]=log2\\u20614=2[1/p(M_)]=\\\\log _4=2}Polo tanto, no codificador óptimo para transmitirM1fará falta un bit e paraM2eM3será necesario contar con dous bits.', 'glPor exemplo, poderíase codificarM1con \"0\",M2con \"10\" eM3con \"11\".', 'glUsando este convenio para codificar a mensaxeM1M2M1M1M3M1M2M3usaríase \"010001101011\" e polo tanto 12 bits.O valor da entropía sería:H(X)=1/2log2\\u2061(2)+1/4log2\\u2061(4)+1/4log2\\u2061(4)=1,5(2)+1/4\\\\log _(4)+1/4\\\\log _(4)=1,5}Polo tanto, o codificador óptimo necesita de media 1,5 bits para codificar calquera valor deX.Entropía condicional[editar|editar a fonte]Supóñase que no canto de ter unha única variable aleatoriaX, existe outra variableYdependentes entre si, é dicir o coñecemento dunha (por exemplo,Y) entrega información sobre a outra (por exemplo,X).', 'glDesde o punto de vista da entropía da información podemos dicir que a información deYdiminuirá a incerteza deX.', 'glPolo tanto, pódese dicir que a entropía deXserá condicional aY, e polo tanto:H(X,Y)=−∑x,yp(x,y)log2\\u2061p(x,y)p(x,y)\\\\log _p(x,y)}Como poloteorema de Bayestense que p(x,y)=p(y)p(x|y) onde p(x|y) é a probabilidade de que se dea un estado deXcoñecidaY, podemos dicir:H(X|Y)=−∑yp(y)∑xp(x|y)log2\\u2061p(x|y)p(y)\\\\sum _p(x|y)\\\\log _p(x|y)}Aplicación en criptoanálise[editar|editar a fonte]O concepto de entropía condicional é moi interesante no campo docriptoanálise.Proporciona unha ferramenta para avaliar o grao de seguridade dos sistemas.', \"glPor exemplo, para un sistema decifradohai dúas entropías condicionais interesantes: Supóñase[5]Unha mensaxe 'M1é sometido a un proceso de cifrado usando a clave K1obtendo E(K1,M1)=C1.PC(K)(K)}representan a probabilidade condicional da claveKdado o criptograma recibidoC.\", 'glÁs veces tamén se denota porP(K|C).PC(M)(M)}representan a probabilidade condicional da mensaxeMdado o criptograma recibidoC.', 'glÁs veces tamén se denota porP(M|C).Entón:Pódese calcular a entropía do coñecemento da clave unha vez coñecido o texto cifrado, e polo tanto medir a equivocación da mensaxe (en inglés,message equivocation),HC(K)(K)}, tamén denotada porH(K|C), mediante a fórmula:HC(K)=−∑E,KP(E,K)logPE\\u2061(K)=−∑EP(E)∑KPE(K)logPE\\u2061(K)(K)=-\\\\sum _P(E,K)\\\\log _}(K)=-\\\\sum _P(E)\\\\sum _P_(K)\\\\log _}(K)}A primeira igualdade é pola definición da entropía condicional e a segunda por aplicación doteorema de Bayes.Obsérvese que seHC(K)=0(K)=0}significa que se poderá romper o cifrado pois xa non hai incerteza.', 'glEsta anulación introduce o concepto dedistancia de unicidade.Pódese calcular a entropía do coñecemento da mensaxe unha vez coñecido o texto cifrado, e polo tanto medir a equivocación da clave (en inglés,key equivocation),HC(M)(M)}, tamén denotada porH(M|C), mediante a fórmula:HC(M)=−∑E,MP(E,M)logPE\\u2061(M)=−∑EP(E)∑MPE(M)logPE\\u2061(M)(M)=-\\\\sum _P(E,M)\\\\log _}(M)=-\\\\sum _P(E)\\\\sum _P_(M)\\\\log _}(M)}A primeira igualdade é pola definición da entropía condicional e a segunda por aplicación do teorema de Bayes.Exemplo[editar|editar a fonte]Supóñase unha variableXcon catro estados:x1,x2,x3,x4,x_,x_,x_}todos equiprobables e polo tantop(xi)=1/4)=1/4}.Existe ademais outra variableYcon tres estados;y1,y2,y3,y_,y_}con probabilidadesp(y1)=1/2)=1/2}ep(y2)=p(y3)=1/4)=p(y_)=1/4}.', 'glCoñécense, ademais, as seguintes dependencias:SeY=y1}entón os posibles valores dexsonx1,x2,x3,x4,x_,x_,x_}SeY=y2}entón os posibles valores dexsonx2,x3,x_}SeY=y3}entón os posibles valores dexsonx3,x4,x_}Aplicando as fórmulas tense:H(X)=2H(Y)=1,5H(X/Y)=1,5Neste caso o coñecemento da dependencia deXrespecto deYreduce a entropía deXde 2 a 1,5.Entropía dun proceso estocástico[editar|editar a fonte]Unproceso estocástico\\\\}}é unha secuencia indexada de variables aleatorias.', 'gl[6]En xeral, pode haber dependencias entre as variables aleatorias.', 'glPara estudar a probabilidade de certo conxunto de valores adóitase adoptar o seguinte convenio:Pr[(X1,X2,...,Xn)=(x1,x2,...,xn)]=p(x1,x2,...,xn),X_,...,X_)=(x_,x_,...,x_)]=p(x_,x_,...,x_)}Sexai=1,..n\\\\}_}un proceso estocástico denvariables aleatorias, e sexaAn}o conxunto das posibles combinacións de valores dei=1,..n\\\\}_}.', 'glDefínese a entropía do proceso estocástico, tamén chamada entropía don-grama e denotado porHn}, como:Hn=H(X1,...,Xn)=∑s∈An−P((X1,...,Xn)=s)log\\u2061P((X1,...,Xn)=s)=H(X_,...,X_)=\\\\sum _}-P((X_,...,X_)=s)\\\\log P((X_,...,X_)=s)}Cociente de entropía[editar|editar a fonte]O cociente de entropía dunha secuencia denvariables aleatorias (proceso estocástico) caracteriza a taxa de crecemento da entropía da secuencia co crecemento den.', 'gl[6]O cociente de entropía dun proceso estocástico\\\\}}vén definida pola ecuación:H(X)=limn→∞1nH(X1,...,Xn)}H(X_,...,X_)}sempre que devandito límite exista.Notas[editar|editar a fonte]↑\"A Mathematical Theory of Communication\".', 'glArquivado dendeo orixinalo 31 de xaneiro de 1998.', 'glConsultado o 30 de xullo de 2017.↑Cuevas Agustín, Gonzalo, \"Teoría de la información, codificación y lenguajes\", Ed.', 'glSEPA (Sociedad para Estudios Pedagógicos Argentinos), Serie Informática 1986↑Dan C. Marinescu, Gabriela M. Marinescu, \"Classical and Quantum Information\",Academic Press 2012↑Huffman, D., \"A method for the Construction of Minimum-Redundancy Codes\", Proc.↑\"Applied cryptology, cryptographic protocols and computer security models\", Richard A. DeMillo et al.↑6,06,1Thomas M. Cover, Joy A. Thomas,\"Elements of Information Theory\", John Wiley & Sons.↑Obsérvese que se usa o logaritmo en base 2 porque se considera que a información se vai representar mediantecódigo binario(quérese representar conbits).', 'glSe para representar a información se usasen valores nunha baseaentón sería conveniente empregar o logaritmo en basea.↑Obsérvese que é unha cantidade adimensional, é dicir non leva unidade.Véxase tamén[editar|editar a fonte]Bibliografía[editar|editar a fonte]Jorge Ramió Aguirre,Aplicaciones criptográficas.', 'glLibro guía da materia de Seguridade Informática.', 'glEscola Universitaria de Informática.', 'glUniversidade Politécnica de Madrid.', 'glXaneiro 1998.Outros artigos[editar|editar a fonte]Entropía cruzadaPerplexidadeLigazóns externas[editar|editar a fonte]Unha teoría matemática da comunicaciónArquivado31 de xaneiro de 1998 enWayback Machine.', 'gl(en inglés)Calculadora da entropía de Shannon(en inglés)Calculadora da entropía de Shannon para ficheiros(en inglés)Control de autoridades:Q204570BNE:XX535116BNF:11985913jEBID:IDFAST:912828GND:4743861-7JSTOR:shannon-entropyLCCN:sh85044152MAG:70567897NDL:01191172NKC:ph425914Treccani:entropiaTraído desde «https://gl.wikipedia.org/w/index.php?title=Entropía_da_información&oldid=5952202»Categorías:Ciencias da informaciónMatemáticasA última edición desta páxina foi o 3 de xaneiro de 2022 ás 23:18.Todo o texto está dispoñible baixo alicenza Creative Commons recoñecemento compartir igual 4.0; pódense aplicar termos adicionais.', 'glConsulte ostermos de usopara obter máis información.Wikipedia® é unha marca rexistrada daWikimedia Foundation, Inc., unha organización sen fins lucrativos.Normas de protección de datosAcerca de WikipediaAdvertenciasCódigo de condutaDesenvolvedoresEstatísticasDeclaración de cookiesVista móbilAlterna o ancho de contido limitado', 'euEntropia (informazio-teoria) - Wikipedia, entziklopedia askea.Edukira joanMenu nagusiaMenu nagusiamugitu alboko barraraezkutatuNabigazioaAzalaTxikipediaIkusgelaTxokoaAldaketa berriakAusazko orriaLaguntzaDohaintza eginBilatuBilatuSortu kontuaHasi saioaTresna pertsonalakSortu kontuaHasi saioaIzena eman gabeko erabiltzaileentzako orrialdeakgehiago ikasiEkarpenakEztabaidaEdukiakmugitu alboko barraraezkutatu⇑ Gora1Sarrera2Definizio formalaErakutsi/ezkutatu Definizio formala azpiatal2.1Adibidea2.2Elkarrekiko informazioa3Propietateak4Kodetzaile optimoaErakutsi/ezkutatu Kodetzaile optimoa azpiatal4.1Adibidea5Baldintzazko entropia6Oharrak7Erreferentziak8Bibliografia9Ikus, gainera10Kanpo estekakEduki taularen ikusgarritasuna aldatuEntropia (informazio-teoria)45 hizkuntzaAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Aldatu loturakArtikuluaEztabaidaeuskaraIrakurriAldatuAldatu iturburu kodeaIkusi historiaTresnakTresnakmugitu alboko barraraezkutatuEkintzakIrakurriAldatuAldatu iturburu kodeaIkusi historiaOrokorraHonanzko esteka duten orriakLotutako orrietako aldaketakFitxategia igoOrri bereziakLotura iraunkorraOrri honen datuakArtikulu hau aipatuGet shortened URLDownload QR codeInprimatu/esportatuLiburu bat sortuDeskargatu PDF formatuanInprimatzeko bertsioaBeste proiektuetanWikimedia CommonsWikidata itemaWikipedia, Entziklopedia askeaBi entropia-bitInformazio-teoriarenalorreanentropiak,informazio-entropiaetaShannonen entropia(Claude E. Shannon-en omenez) izenez ere ezagutua,informazio iturribaten ziurgabetasuna neurtzen du.Entropia kontzeptuatermodinamikan,mekanika estatistikoanetainformazio-teorianerabiltzen da.', 'euKasu guztietan entropia «desordenaren neurri» edo «konbinazio jakin batzuen berezitasun» moduan ulertzen da.', 'euZiurgabetasunaren neurri bat dela edo edozein prozesutan ziurgabetasun hori mugatzeko, murrizteko edo ezabatzeko behar den informazioa dela uler daiteke.', 'euKontua da, informazioaren eta entropiaren kontzeptuek oso harreman estua dutela, nahiz eta hortaz konturatzeko mekanika estatistikoaren eta informazio-teoriaren arloetan lan handia egin behar izan zen.Sarrera[aldatu|aldatu iturburu kodea]Informazio-teoriaren oinarrizko ideia honakoa da: gai bati buruz zenbat eta gehiago jakin, orduan eta informazio berri gutxiago lortuko da.', 'euGertakari bat emateko probabilitatea handia bada, gertatzen denean ez da harrigarria eta, beraz, gertatu izanak informazio berri gutxi ematen du.', 'euGertatzeko probabilitatea txikia bada, ordea, gertakizuna jazo izana nabarmenki adierazgarriagoa da.', 'euHortaz,informazio kantitateagertakarien probabilitatearen alderantzizkoa (1/p) adierazten duen funtzio gorakorra da.', 'euGertakari bat baino gehiago badago, gertaeretako batek emango lukeen batezbesteko informazio kantitatea neurtzen du entropiak.', 'euAdibidez, dado bat jaurtitzeak txanpon bat jaurtitzeak baino entropia handiagoa duela esan nahi du horrek, dadoa jaurtitzean eman daitezkeen gertakariek txanpona jaurtitzean eman daitezkeenek baino probabilitate txikiago baitute.Beraz, entropia egoera batenziurgabetasun-neurriaedobatezbesteko informazio kantitateada.', 'euKontzeptu horiek ulertzeko adibide gisa, har dezagun bozketa politikoarena.', 'euBozketak egiten dira haien emaitza aldez aurretik ezagutzen ez delako eta, ondorioz, bozketen emaitzekinformazioberria emango dute.', 'euBeste era batera esanda:a priori, bozketaren entropia altua da.', 'euBozketa egin eta handik gutxira bozketa bera errepikatuko balitz, lehenengo bozketaren emaitza dagoeneko ezaguna denez, bigarrenean lortuko den emaitza iragarri ahalko litzateke eta emaitza berriak ez luke informazio gehigarri askorik emango; kasu horretan, bigarren bozketaren entropia,a priori, txikia da (aurrekoarekin alderatuz).Beste adibide bat txanponaren jaurtiketarena da.', 'euAurpegia eta gurutzea lortzeko probabilitateak berdinak direla suposatuz, txanpona jaurtitzearen entropiak izan dezakeen balio maximoa du.', 'euAldez aurretik jaurtiketaren emaitza zein izango den iragartzeko modurik ez dagoelako gertatzen da hori.', 'euIragarpen bat eman beharko balitz, aurpegia aterako dela esan genezake adibidez, asmatzeko probabilitatea 1/2ekoa izanik.', 'euHorrelako txanpon jaurtiketa batean entropiaren balioa 1 da, gertatzeko probabilitate bera duten bi egoera daudelako (aurpegia, gurutzea).', 'euTxanponaren bi aldeetan aurpegia egongo balitz, ordea, entropiaren balioa 0 izango litzateke, errorerik gabe iragarri ahal izango litzatekeelako txanpona jaurti eta emaitza aurpegia izango dela.', 'euHorrela, probabilitate bereko gertakari bitarren Shannon-en entropialog2\\u20612=12=1}da.', 'euModu berean, probabilitate bereko hiru aukera egongo balira, Shannon-en entropialog2\\u206133}izango litzateke.Definizio formala[aldatu|aldatu iturburu kodea]Demagunzorizko aldagaibaten hasierako indeterminazio mailakdela (kegoera posible dituela, alegia).', 'euDemagun, gainera, egoera guztiak probabilitate berekoak direla.', 'euHori hala izanik, konbinazio jakin bat gertatzeko probabilitateap=1/kizango da.', 'euBeraz, honakoa idatz daiteke:[oh 1]log2\\u2061(k)=log2\\u2061[1/(1/k)]=log2\\u2061(1/p)=log2\\u2061(1)⏟=0−log2\\u2061(p)=−log2\\u2061(p)(k)=\\\\log _[1/(1/k)]=\\\\log _(1/p)=\\\\underbrace (1)} _-\\\\log _(p)=-\\\\log _(p)}Egoera guztiak probabilitate berekoak ez badira, hau da,i.egoerapi}probabilitatez gertatzen bada, informazio-kantitateen batuketa haztatuaren bidez kalkulatuko da entropia:[1][oh 2]H=−p1log2\\u2061(p1)−p2log2\\u2061(p2)−....−pklog2\\u2061(pk)=−∑i=1kpilog2\\u2061(pi)\\\\log _(p_)-p_\\\\log _(p_)-....-p_\\\\log _(p_)=-\\\\sum _^p_\\\\log _(p_)}Beraz, X zoriozko aldagaiaren entropia,H(X)notazioaren bidez adierazten da eta aldagaiaren egoera desberdinei dagokien informazio-kantitatearen batezbesteko balio haztatua da.H(X)=−∑i=1kp(xi)log2\\u2061p(xi)^p(x_)\\\\log _p(x_)}Horrela definitutako entropiak X zoriozko aldagaiaren ziurgabetasun mailaren neurria ematen du eta ondorioz, informazio-kantitatea adierazten du.Adibidea[aldatu|aldatu iturburu kodea]Har dezagun txanpon baten jaurtiketaren adibidea.', 'euTxanpona jaurtitzean aurpegia ala gurutzea lortzen dira; txanpona zintzoa bada, bi egoerek probabilitate bera dute eta beraz, jaurtiketaren emaitzaren entropia maximoa da.', 'euZiurgabetasun maximoko egoera da, hurrengo jaurtiketaren emaitza aurreikustea ia ezinezkoa delako.Xren entropiaBernoulli-ren saiakuntzan.', 'eu(Xak 0 edo 1 balioak har ditzakeen ausazko saiakuntza).', 'euEntropia P(X=1) probabilitatearen araberakoa da.', 'euP(X=1)=0,5 denean, emaitza guztiek probabilitate bera dute, beraz, ziurgabetasun-maila altua da eta entropia maximoa.H(X)=−∑i=1np(xi)log2\\u2061p(xi)=−∑i=1212log2\\u206112=−∑i=1212⋅(−1)=1\\\\mathrm (X)&=-\\\\sum _^ (x_)\\\\log _\\\\mathrm (x_)}\\\\\\\\&=-\\\\sum _^}\\\\log _}}\\\\\\\\&=-\\\\sum _^}\\\\cdot (-1)}=1\\\\end}}Baina txanpona ez bada zintzoa, hau da, aurpegia lortzeko probabilitatea (p) eta gurutzea lortzekoa (q) desberdinak badira, (p≠q), orduanBernoulli-ren prozesubaten bidez eredutu ahal izango dugu.', 'euKasu horretan, ziurgabetasuna txikiagoa da, txanpona jaurtitzen den aldiero txanponaren alde bat lortzearen probabilitatea bestea lortzearena baino handiagoa delako.', 'euZiurgabetasuna murriztean entropia ere murrizten da.', 'euAdibidez, p=0,7 denean:H(X)=−plog2\\u2061(p)−qlog2\\u2061(q)=−0.7log2\\u2061(0.7)−0.3log2\\u2061(0.3)≈−0.7⋅(−0.515)−0.3⋅(−1.737)=0.8816<1\\\\mathrm (X)&=-p\\\\log _(p)-q\\\\log _(q)\\\\\\\\&=-0.7\\\\log _(0.7)-0.3\\\\log _(0.3)\\\\\\\\&\\\\approx -0.7\\\\cdot (-0.515)-0.3\\\\cdot (-1.737)\\\\\\\\&=0.8816<1\\\\end}}Elkarrekiko informazioa[aldatu|aldatu iturburu kodea]Entropiaelkarrekiko informazioarenkasu berezi bat bezala uler daiteke.', 'euZorizko bi aldagairen elkarrekiko informazioa I(X, Y) notazioaz adierazten da eta bi aldagaien elkarrekiko mendekotasuna neurtzen du.', 'euZorizko Y aldagai[2]baten balioa ezagutzeak Xaldagaiarenziurgabetasun maila edoentropiazenbat murrizten duen neurtzen du.', 'euBeraz, zera ondoriozta daiteke: X eta Y berdinak badira, orduan I(X; X) = H(X) betetzen da.Propietateak[aldatu|aldatu iturburu kodea]Entropiak honako propietateak betetzen ditu:Ez da negatiboa.pi}probabilitate bat denez,0<pi≤1\\\\leq 1}betetzen da.', 'euHortaz,log2\\u2061pi≤0p_\\\\leq 0}eta ondorioz−log2\\u2061pi≥0p_\\\\geq 0}betetzen dela ziurta daiteke.H≤loga\\u2061(n)(n)}.', 'euHau da, H entropiak goi-bornea du (maximoa denean) eta ez du informazio-galerarik suposatzen.Izan bitez emaitza posibleak eta p1, …, pn probabilitateak dituen prozesu bat.H(p1,…,pn),\\\\dots ,p_)\\\\,}funtzioa maximoa dap1=⋯=pn=1/n=\\\\dots =p_=1/n\\\\,}betetzen denean; emaitza guztiak probabilitate berberaz gerta daitezkeenean ematen da ziurgabetasunaren maila maximoa.Izan bitez emaitza posibleak eta p1, …, pn probabilitateak dituen prozesu bat.H(p1,…,pn),\\\\dots ,p_)\\\\,}funtzioak 0 balioa dupi=0=0}bada i-ren edozein baliorako klase jakin baterako izan ezik, nonpj=0=0}.', 'euModu intuitiboan pentsa daiteke egoera batek edo gehiagok probabilitate altua dutenean entropia nabarmenki jaisten dela, ziurgabetasuna txikitzen delako.Kodetzaile optimoa[aldatu|aldatu iturburu kodea]Kodetzaile optimoamezu bat kodetzeko bit kopuru minimoa erabiltzen duena da.', 'euKodetzaile optimo batek kode laburrak erabiliko ditu maiztasun handiz agertzen diren mezuak kodetzeko eta gutxitan agertzen diren mezuetarako kode luzeagoak erabiliko dira.', 'euHorrela, memoria gunearen errendimendua optimizatzen da eta sistema eraginkorra da, mezua adierazteko behar den bit kopuruari dagokionez.Adibidez,Morse kodeakodetzaile optimo bat ez izan arren, printzipio horretan oinarritzen da, ingelesez maiztasun handiagoz agertzen diren hizkiei kode laburragoak esleituz.Beste adibide batHuffmanen algoritmoada[3].', 'euAlgoritmo horrek informazioa trinkotzen dukodetzaile optimoanoinarrituz.', 'euLehenik, karaktereen agerpen-maiztasuna kalkulatzen du, ondoren kodetzaile optimoa bilatzen duzuhaitz bitarrenbidez.Datu-konpresioteknika batzuk sinboloen probabilitateak kalkulatu ordez sinbolo-sekuentzien probabilitateak kalkulatzen dituzte, datuen konpresio maila altuagoa lortzeko.Kodetzaile optimo bat eraiki daiteke X zorizko aldagai baten entropian oinarrituz.', 'euLogaritmoa oinarri bitarrean erabiltzen bada, mezu bat kodetzeko behar denbit kopuruaren batezbestekoalortzen da.', 'euHorrela, Shannon-ek analitikoki frogatu zuenez, mezu baten konpresioa sinboloak banaka hartuz eta informazio galerarik gabe zenbat konprima daitekeen kalkula daiteke (muga maximoa).', 'euKonpresioaren muga (bitetan neurtua), entropiaren eta mezuaren luzeraren biderkadura izango da.H(X)=−∑ip(xi)log2\\u2061p(xi)=∑i−p(xi)log2\\u2061p(xi)=p(x_)\\\\log _p(x_)=\\\\sum _-p(x_)\\\\log _p(x_)=}=∑ip(xi)[log2(1)−log2\\u2061(p(xi))]=∑xp(x)log2\\u2061(1/p(x))p(x_)[log_(1)-\\\\log _(p(x_))]=\\\\sum _p(x)\\\\log _(1/p(x))}Beraz,Xzorizko aldagai diskretu batenxi}sinbolo espezifiko batek ematen duen informazioa, bitetan neurtua, horrela definitzen da:I(xi)=log2\\u20611p(xi)=−log2\\u2061p(xi))=\\\\log _)}}=-\\\\log _)}}Adibidea[aldatu|aldatu iturburu kodea]Demagun mezu batek hiru egora izan ditzakeela.', 'euM1 egoeraren probabilitatea %50-ekoa da, M2 egoeraren probabilitatea %25-ekoa da eta M3 egoerarena %25-ekoa.M1-erakolog2\\u2061[1/p(M1)]=log2\\u20612=1[1/p(M_)]=\\\\log _2=1}lortuko dugu.M2-erakolog2\\u2061[1/p(M2)]=log2\\u20614=2[1/p(M_)]=\\\\log _4=2}lortuko dugu.M3-erakolog2\\u2061[1/p(M2)]=log2\\u20614=2[1/p(M_)]=\\\\log _4=2}lortuko dugu.Beraz, kodetzaile optimoak bit bat erabiliko du M1 transmititzeko, eta bi bit beharko dira M2 eta M3 kodetzeko.', 'euAdibidez, M1 kodetzeko \"0\" erabil dezakegu, M2 kodetzeko 10 eta M3rako 11.', 'euHala egingo bagenu, M1M2M1M1M3M1M2M3mezua “010001101011” moduan kodetuko genuke, 12 bit erabiliz.', 'euEntropiaren balioa honakoa izango litzateke:H(X)=1/2log2\\u2061(2)+1/4log2\\u2061(4)+1/4log2\\u2061(4)=1,5(2)+1/4\\\\log _(4)+1/4\\\\log _(4)=1,5}Zera ondoriozta daiteke: kodetzaile optimoak 1,5 bit beharko ditu Xren edozein balio kodetzeko.Baldintzazko entropia[aldatu|aldatu iturburu kodea]IkusBaldintzazko entropiaIzan bitez X eta Y, haien artean independenteak ez diren bi aldagai; horietako bat ezagutzeak (Y, adibidez) besteari buruzko informazioa (Xri buruzkoa) ematen digu.', 'euInformazioaren entropiari dagokionez, Y aldagaiaren informazioak X aldagaiaren ziurgabetasuna txikituko duela esan dezakegu.', 'euBeraz, X aldagaiaren entropia Y aldagaiaren menpe dagoela esan daiteke:H(X,Y)=−∑x,yp(x,y)log2\\u2061p(x,y)p(x,y)\\\\log _p(x,y)}Bayes-en teorematikp(x,y)=p(y)p(x|y) dela dakigu, non Y ezagututa X egoera gertatzearen probabilitatea p(x|y) den.', 'euHonakoa adieraz daiteke:H(X|Y)=−∑yp(y)∑xp(x|y)log2\\u2061p(x|y)p(y)\\\\sum _p(x|y)\\\\log _p(x|y)}Oharrak[aldatu|aldatu iturburu kodea]↑Ikusi daitekeenez, 2 oinarria duen logaritmoa erabiltzen da informazioa kode bitarrean adieraziko dela kontsideratzen delako.', 'euInformazioa adieraztekoaoinarria duten balioak erabiltzekotan,aoinarria duen logaritmoa erabiltzea egokia izango litzateke.↑Kontuan izan unitaterik gabeko kantitateak direla.Erreferentziak[aldatu|aldatu iturburu kodea]↑Cuevas Agustín, Gonzalo, \"Teoría de la información, codificación y lenguajes\", Ed.', 'euSEPA (Sociedad para Estudios Pedagógicos Argentinos), Serie Informática 1986↑Dan C. Marinescu, Gabriela M. Marinescu, \"Classical and Quantum Information\",Academic Press 2012↑Huffman, D., \"A method for the Construction of Minimum-Redundancy Codes\", Proc.', 'euIRE, Vol 40 1952Bibliografia[aldatu|aldatu iturburu kodea]Jorge Ramió Aguirre, Aplicaciones criptográficas.', 'euLibro guía de la asignatura de Seguridad Informática.', 'euEscuela Universitaria de Informática.', 'euUniversidad Politécnica de Madrid.', 'euEnero de 1998.Ikus, gainera[aldatu|aldatu iturburu kodea]Informazio kantitateaBaldintzazko entropiaElkarrekiko informazioaKanpo estekak[aldatu|aldatu iturburu kodea]Komunikazioaren Matematikaren teoria(ingelesez)Shannon-en entropia kalkulatzeko kalkulagailua(ingelesez)Fitxategientzako Shannon-en entropia kalkulatzeko kalkulagailua(ingelesez)Autoritate kontrolaWikimedia proiektuakDatuak:Q204570Multimedia:Entropy and information/Q204570IdentifikadoreakBNE:XX535116BNF:11985913j(data)GND:4743861-7LCCN:sh85044152NDL:01191172NKC:ph425914Hiztegiak eta entziklopediakBritannica:urlDatuak:Q204570Multimedia:Entropy and information/Q204570\"https://eu.wikipedia.org/w/index.php?title=Entropia_(informazio-teoria)&oldid=8886652\"(e)tik eskuratutaKategoriak:Informazioaren teoriaEstatistikaAusazkotasun estatistikoaEzkutuko kategoriak:Wikipedia:BNE identifikatzailea duten artikuluakWikipedia:BNF identifikatzailea duten artikuluakWikipedia:GND identifikatzailea duten artikuluakWikipedia:LCCN identifikatzailea duten artikuluakOrriaren azken aldaketa: 20 martxoa 2022, 00:02.TestuaCreative Commons Aitortu-PartekatuBerdin 4.0 lizentziarijarraituz erabil daiteke; baliteke beste klausularen batzuk ere aplikatu behar izatea.', 'euXehetasunen berri izateko, ikuserabilera-baldintzak.Pribazitate politikaWikipediari buruzLege oharraCode of ConductGaratzaileakEstatistikakCookie adierazpenaMugikorreko bistaAldatu edukiaren zabalera mugatua', 'roEntropie informațională - WikipediaSari la conținutMeniul principalMeniul principalmută în bara lateralăascundeNavigarePagina principalăSchimbări recenteCafeneaArticol aleatoriuFacebookParticipareCum încep pe WikipediaAjutorPortaluri tematiceArticole ceruteDonațiiCăutareCăutareCreare contAutentificareUnelte personaleCreare contAutentificarePagini pentru editorii neautentificațiaflați mai multContribuțiiDiscuțiiCuprinsmută în bara lateralăascundeÎnceput1Definiție2Exemplu3ProprietățiToggle Proprietăți subsection3.1Aditivitate3.2Schimbarea de bază3.3Continuitate3.4Simetrie3.5Maximum4Note5Bibliografie6Vezi și7Legături externeComută cuprinsulEntropie informațională45 limbiAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Modifică legăturileArticolDiscuțieromânăLecturăModificareModificare sursăIstoricUnelteUneltemută în bara lateralăascundeAcțiuniLecturăModificareModificare sursăIstoricGeneralCe trimite aiciSchimbări corelateTrimite fișierPagini specialeLegătură permanentăInformații despre paginăCitează acest articolObține URL scurtatDescărcați codul QRElement WikidataTipărire/exportareCreare carteDescărcare ca PDFVersiune de tipăritÎn alte proiecteWikimedia CommonsDe la Wikipedia, enciclopedia liberăÎnteoria informației,entropia Shannonsauentropia informaționalămăsoarăincertitudineaasociată cu ovariabilă aleatoare.', 'roAceastă măsură indică și cantitatea de informație conținută într-un mesaj, exprimată de obicei înbițisau în biți pe simbol.', 'roCând este exprimată în biți, ea reprezintă lungimea minimă pe care trebuie să o aibă un mesaj pentru a comunica informația.Ea mai reprezintă și o limită absolută a celei mai bunecompresiifără pierderi aplicabilă unor date comunicate: tratând un mesaj ca pe o serie de simboluri, cea mai scurtă reprezentare posibilă a mesajului are lungimea egală cu entropia Shannon în biți pe simbol înmulțită cu numărul de simboluri din mesajul original.O aruncare a monezii are entropia de un bit.', 'roDar, dacă moneda nu este echilibrată, atunci incertitudinea este mai mică (se știe că există oprobabilitatemai mare ca ea să cadă cu o anume parte a ei în sus), și astfel entropia Shannon este mai mică.', 'roUn șir lung de caractere repetate au entropia 0, deoarece fiecare caracter este previzibil.', 'roEntropia unui text înlimba englezăeste de 1,0 până la 1,5 biți pe literă.', 'ro[1]Echivalent, entropia Shannon măsoară media de conținut informațional pe care receptorul o pierde atunci când nu cunoaște valoarea variabilei aleatoare.Conceptul a fost introdus deClaude Shannonîn lucrarea sa din 1948„O teorie matematică a comunicației”.Definiție[modificare|modificare sursă]EntropiaHa uneivariabile discreteXcu valorilex1, ...,xnși funcția de probabilitatepesteH(X)=−E(logb\\u2061p(X))=−∑i=1np(xi)logb\\u2061p(xi), (\\\\log _p(X))=-\\\\sum _^)\\\\log _p(x_)},}undebeste o bază pentru logaritmi, reală și supraunitară (de obicei 2, caz în careunitatea de măsurăa informației se numeștebit, saue, caz în care ea se numeștenat).Entropia Shannon nu este definită pentru variabile continui.', 'roPrin analogie cu variabile discrete, se definește entropia diferențială astfel:H(X)=−∫−∞∞f(x)logb\\u2061(f(x))dx^f(x)\\\\log _(f(x))\\\\,dx}undefeste densitatea de repartiție luiX.', 'roÎnsă, entropia diferențială unei variabile continui nu are aceleași proprietăți ca entropia Shannon unei variabile discrete.', 'roSpre exemplu, poate fi negativă.Exemplu[modificare|modificare sursă]Presupunem evenimentul aruncării unui zar cu 6 fețe.', 'roValorile variabileiXsunt iar probabilitățile obținerii oricărei valori sunt egale.', 'roEntropia este:H(X)=−∑i=1616log2\\u2061(16)=−6⋅16log2\\u2061(16)=−log2(16)=2.58^}\\\\log _\\\\left(}\\\\right)}=-}\\\\log _\\\\left(}\\\\right)}=-\\\\left(}\\\\right)}=2.58}.Pentru o populație discretă cu valorile cu probabilitățile respectiv (aproximativ odistribuție binomialăcu p=50%) entropia calculată este:H(X) = 2.2.', 'roIncertitudinea s-a diminuat față de exemplul precendent.Proprietăți[modificare|modificare sursă]Aditivitate[modificare|modificare sursă]Logaritmul este folosit în calculul entropiei pentru a permite adunarea incertitudinii unor variabile independente.De exemplu, considerând X și Y doua evenimente independente, distribuite uniform, cunrespectivmposibile rezultate perechea (X,Y) va aveamnrezultate echiprobabiley_:i=1,\\\\cdots ,n,j=1,\\\\cdots ,m\\\\right\\\\}}.', 'roEntropia perechii (X,Y) se calculează:H(X,Y)=log2\\u2061(nm)=log2\\u2061(n)+log2\\u2061(m)=H(X)+H(Y).', 'ro(nm)=\\\\log _(n)+\\\\log _(m)=H(X)+H(Y).', 'ro}(2)Astfel, entropia perechii este egală cu suma entropiei celor două evenimente luate separat.', 'roProprietatea aditivității implică faptul că entropia se menține constantă indiferent dacă mulțimea rezultatelor/procesul este privit ca întreg sau ca sumă a unorsubmulțimi/ procese.Schimbarea de bază[modificare|modificare sursă]Entropia poate fi calculată folosind diferite baze ale logaritmului.', 'roÎnmulțirea logaritmilor are proprietatea:loga\\u2061(p)=loga\\u2061(b)∗logb\\u2061(p).', 'ro(p)=\\\\log _(b)*\\\\log _(p).', 'ro}.Entropia calculată in bazaava fi egală culoga(2)(2)}inmulțită cu entropia calculată culogaritmin baza 2.Continuitate[modificare|modificare sursă]Entropia este ofuncție continuă.', 'roUnei modificari infinitezimale a probabilităților corespunde o modificare asemănătoare a entropiei.Simetrie[modificare|modificare sursă]Valoarea entropiei rămâne neschimbată daca se schimbă ordinea variabilelorxi.Hn(p1,p2,…)=Hn(p2,p1,…)\\\\left(p_,p_,\\\\ldots \\\\right)=H_\\\\left(p_,p_,\\\\ldots \\\\right)}etc.Maximum[modificare|modificare sursă]Entropia, incertitudinea atinge o valoare maximă dacă evenimentele sunt echiprobabile.Hn(p1,…,pn)≤Hn(1n,…,1n).', 'ro(p_,\\\\ldots ,p_)\\\\leq H_\\\\left(},\\\\ldots ,}\\\\right).', 'ro}Pentru evenimente independente și echiprobabile entropia crește cu numărul posibil de rezultate.Hn(1n,…,1n⏟n)<Hn+1(1n+1,…,1n+1⏟n+1).\\\\underbrace },\\\\ldots ,}} _<H_\\\\underbrace },\\\\ldots ,}} _.', 'ro}Note[modificare|modificare sursă]^Schneier, Bruce.Applied Cryptography.', 'roJohn Wiley and Sons.', 'roEdiția a doua.', 'rop. 234.Bibliografie[modificare|modificare sursă]Silviu Guiașu, Radu TheodorescuTeoria matematică a informației,Editura Academiei RSR, 1966Shigeru Furuichi, Flavia-Corina Mitroi-Symeonidis, Eleutherius Symeonidis, On some properties of Tsallis hypoentropies and hypodivergences, Entropy, 16(10) (2014), 5377-5399; DOI:10.3390/e16105377Shigeru Furuichi, Flavia-Corina Mitroi, Mathematical inequalities for some divergences, Physica A 391 (2012), pp.', 'ro388-400, DOI:10.1016/j.physa.2011.07.052; ISSN: 0378-4371Shigeru Furuichi, Nicușor Minculete, Flavia-Corina Mitroi, Some inequalities on generalized entropies, J. Inequal.', 'roAppl., 2012, 2012:226.', 'roDOI: 10.1186/1029-242X-2012-226Vezi și[modificare|modificare sursă]Legături externe[modificare|modificare sursă]Portal MatematicăTeoria matematică a comunicației C.E.', 'roShannonArhivatîn31 ianuarie 1998, laWayback Machine.Control de autoritateBNE:XX535116BNF:cb11985913j(data)GND:4743861-7LCCN:sh85044152NDL:01191172NKC:ph425914Adus de lahttps://ro.wikipedia.org/w/index.php?title=Entropie_informațională&oldid=15986691Categorii:Entropie informaționalăInformaticăStatistică descriptivăCategorii ascunse:Webarchive template wayback linksArticole Wikipedia cu identificatori BNEArticole Wikipedia cu identificatori BNFArticole Wikipedia cu identificatori GNDArticole Wikipedia cu identificatori LCCNArticole Wikipedia cu identificatori NDLArticole Wikipedia cu identificatori NKCArticole Wikipedia cu control de autoritateUltima editare a paginii a fost efectuată la 21 decembrie 2023, ora 09:46.Acest text este disponibil sub licențaCreative Commons cu atribuire și distribuire în condiții identice; pot exista și clauze suplimentare.', 'roVedeți detalii laTermenii de utilizare.Politica de confidențialitateDespre WikipediaTermeniCod de conduităDezvoltatoriStatisticiDeclarație cookieVersiune mobilăComută lățimea limitată a conținutului', 'esEntropía (información) - Wikipedia, la enciclopedia libreIr al contenidoMenú principalMenú principalmover a la barra lateralocultarNavegaciónPortadaPortal de la comunidadActualidadCambios recientesPáginas nuevasPágina aleatoriaAyudaDonacionesNotificar un errorBuscarBuscarCrear una cuentaAccederHerramientas personalesCrear una cuentaAccederPáginas para editores desconectadosmás informaciónContribucionesDiscusiónContenidosmover a la barra lateralocultarInicio1Relación con la entropía termodinámica2Concepto intuitivo3Definición formalAlternar subsección Definición formal3.1Ejemplos3.2Información mutua4Propiedades5Codificador óptimoAlternar subsección Codificador óptimo5.1Ejemplo6Entropía condicionalAlternar subsección Entropía condicional6.1Aplicación en criptoanálisis6.2Ejemplo7Entropía de un proceso estocásticoAlternar subsección Entropía de un proceso estocástico7.1Ratio de entropía8Otros tipos de entropía9Véase también10Notas11Referencias12Bibliografía13Enlaces externosCambiar a la tabla de contenidosEntropía (información)45 idiomasAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Editar enlacesArtículoDiscusiónespañolLeerEditarVer historialHerramientasHerramientasmover a la barra lateralocultarAccionesLeerEditarVer historialGeneralLo que enlaza aquíCambios en enlazadasSubir archivoPáginas especialesEnlace permanenteInformación de la páginaCitar esta páginaObtener URL acortadoDescargar código QRElemento de WikidataImprimir/exportarCrear un libroDescargar como PDFVersión para imprimirEn otros proyectosWikimedia CommonsDe Wikipedia, la enciclopedia libreEn el ámbito de lateoría de la informaciónlaentropía, también llamadaentropía de la informaciónyentropía de Shannon(en honor aClaude E. Shannon), mide la incertidumbre de unafuente de información.La entropía también se puede considerar como la cantidad de información promedio que contienen los símbolos usados.', 'esLos símbolos con menor probabilidad son los que aportan mayor información; por ejemplo, si se considera como sistema de símbolos a las palabras en un texto, palabras frecuentes como «que», «el», «a» aportan poca información, mientras que palabras menos frecuentes como «corren», «niño», «perro» aportan más información.', 'esSi de un texto dado borramos un «que», seguramente no afectará a la comprensión y se sobreentenderá, no siendo así si borramos la palabra «niño» del mismo texto original.', 'esCuando todos los símbolos son igualmente probables (distribución de probabilidad plana), todos aportan información relevante y la entropía es máxima.El concepto de entropía es usado entermodinámica,mecánica estadística,teoría de la informaciónyseguridad entrópica.', 'esEn todos los casos la entropía se concibe como una «peculiaridad de ciertas combinaciones».', 'esLa entropía puede ser considerada como una medida de la incertidumbre y de la información necesaria para, en cualquier proceso, poder acotar, reducir o eliminar la incertidumbre.', 'esResulta que el concepto de información y el de entropía están básicamente relacionados entre sí, aunque se necesitaron años de desarrollo de lamecánica estadísticay de lateoría de la informaciónantes de que esto fuera percibido.Relación con la entropía termodinámica[editar]La entropía de la teoría de la información está estrechamente relacionada con laentropía termodinámica.', 'esEn la termodinámica se estudia un sistema de partículas cuyos estados X (usualmente posición y velocidad) tienen una ciertadistribución de probabilidad, pudiendo ocupar varios microestados posibles (equivalentes a los símbolos en la teoría de la información).', 'esLa entropía termodinámica es igual a la entropía de la teoría de la información de esa distribución (medida usando ellogaritmo neperiano) multiplicada por laconstante de Boltzmannk, la cual permite pasar de nats (unidad semejante al bit) a J/K.', 'esCuando todos los microestados son igualmente probables, la entropía termodinámica toma la forma k log(N).', 'esEn un sistema aislado, la interacción entre las partículas tiende a aumentar su dispersión, afectando sus posiciones y sus velocidades, lo que causa que la entropía de la distribución aumente con el tiempo hasta llegar a un cierto máximo (cuando el mismo sistema es lo más homogéneo y desorganizado posible); lo que es denominadosegunda ley de la termodinámica.', 'esLa diferencia entre la cantidad de entropía que tiene un sistema y el máximo que puede llegar a tener se denominaneguentropía, y representa la cantidad de organización interna que tiene el sistema.', 'esA partir de esta última se puede definir laenergía libre de Gibbs, que indica la energía que puede liberar el sistema al aumentar la entropía hasta su máximo y puede ser transformada en trabajo (energía mecánicaútil) usando unamáquina ideal de Carnot.', 'esCuando un sistema recibe un flujo de calor, las velocidades de las partículas aumentan, lo que dispersa la distribución y hace aumentar así la entropía.', 'esAsí, el flujo de calor produce un flujo de entropía en la misma dirección.Concepto intuitivo[editar]Entropía de la información en unensayo de BernoulliX(experimento aleatorio en que X puede tomar los valores 0 o 1).', 'esLa entropía depende de la probabilidad P(X=1) de que X tome el valor 1.', 'esCuando P(X=1)=0.5, todos los resultados posibles son igualmente probables, por lo que el resultado es poco predecible y la entropía es máxima.El concepto básico de entropía enteoría de la informacióntiene mucho que ver con laincertidumbreque existe en cualquier experimento o señal aleatoria.', 'esEs también la cantidad de «ruido» o «desorden» que contiene o libera un sistema.', 'esDe esta forma, podremos hablar de la cantidad de información que lleva una señal.Como ejemplo, consideremos algún texto escrito enespañol, codificado como una cadena de letras, espacios ysignos de puntuación(nuestra señal será unacadena de caracteres).', 'esYa que, estadísticamente, algunos caracteres no son muy comunes (por ejemplo, «w»), mientras otros sí lo son (como la «a»), la cadena de caracteres no será tan \"aleatoria\" como podría llegar a ser.', 'esObviamente, no podemos predecir con exactitud cuál será el siguiente carácter en la cadena, y eso la haría aparentemente aleatoria.', 'esPero la entropía es la encargada de medir precisamente esa aleatoriedad, y fue presentada por Shannon en su artículo de 1948,A Mathematical Theory of CommunicationArchivadoel 31 de enero de 1998 enWayback Machine.', 'es(\"Una teoría matemática de la comunicación\", en inglés).Shannon ofrece una definición de entropía que satisface las siguientes afirmaciones:La medida de información debe serproporcional(linealcontinua).', 'esEs decir, el cambio pequeño en una de las probabilidades de aparición de uno de los elementos de la señal debe cambiar poco la entropía.Si todos los elementos de la señal son equiprobables (igual de probables) a la hora de aparecer, entonces la entropía será máxima.Ejemplos de máxima entropía: Suponiendo que estamos a la espera de un texto, por ejemplo un cable con un mensaje.', 'esEn dicho cable solo se reciben las letras en minúscula de la a hasta la z, entonces si el mensaje que nos llega es \"qalmnbphijcdgketrsfuvxyzwño\" el cual posee una longitud de 27 caracteres, se puede decir que este mensaje llega a nosotros con la máxima entropía (o desorden posible); ya que es poco probable que se pueda pronosticar la entrada de caracteres, pues estos no se repiten ni están ordenados en una forma predecible.Definición formal[editar]Supongamos que un evento (variable aleatoria) tiene un grado de indeterminación inicial igual ak(i.e.', 'esexistenkestados posibles) y supongamos todos los estados equiprobables.', 'esEntonces la probabilidad de que se dé una de esas combinaciones seráp=1/k.', 'esLuego podemos representar la expresiónci}como:[a]\\u200bci=log2\\u2061(k)=log2\\u2061[1/(1/k)]=log2\\u2061(1/p)=log2\\u2061(1)⏟=0−log2\\u2061(p)=−log2\\u2061(p)=\\\\log _(k)=\\\\log _[1/(1/k)]=\\\\log _(1/p)=\\\\underbrace (1)} _-\\\\log _(p)=-\\\\log _(p)}Si ahora cada uno de loskestados tiene una probabilidadpi}, entonces la entropía vendrá dada por la suma ponderada de la cantidad de información:[1]\\u200b[b]\\u200bH=−p1log2\\u2061(p1)−p2log2\\u2061(p2)−....−pklog2\\u2061(pk)=−∑i=1kpilog2\\u2061(pi)\\\\log _(p_)-p_\\\\log _(p_)-....-p_\\\\log _(p_)=-\\\\sum _^p_\\\\log _(p_)}Por lo tanto, la entropía de un mensajeX, denotado porH(X), es el valor medio ponderado de la cantidad de información de los diversos estados del mensaje:H(X)=−∑ip(xi)log2\\u2061p(xi)=∑ip(xi)log2\\u2061(1/p(xi))p(x_)\\\\log _p(x_)=\\\\sum _p(x_)\\\\log _(1/p(x_))}que representa una medida de la incertidumbre media acerca de una variable aleatoria y por tanto de la cantidad de información.Ejemplos[editar]La entropía de un mensaje M de longitud 1 carácter que utiliza el conjunto de caracteres ASCII, suponiendo una equiprobabilidad en los 256 caracteres ASCII, será:H(M)=−∑i=12561256log2\\u2061(1256)=(−256256)log2\\u2061(1256)=1log2\\u2061(256)=8^}\\\\log _}=(-})\\\\log _}=1\\\\log _(256)=8}Supongamos que el número de estados de un mensaje es igual a 3, M1, M2y M3donde la probabilidad de M1es 50 %, la de M225 % y la de M325 %.', 'esPor tanto, la entropía de la información es:H(M)=1/2log2\\u2061(2)+1/4log2\\u2061(4)+1/4log2\\u2061(4)=1,5(2)+1/4\\\\log _(4)+1/4\\\\log _(4)=1,5}Información mutua[editar]La entropía puede verse como caso especial de lainformación mutua.', 'esLainformación mutuade dosvariables aleatorias, denotado por I(X;Y), es unacantidadque mide la dependencia mutua de las dosvariables; es decir, mide la reducción de la incertidumbre (entropía) de una variable aleatoria, X, debido al conocimiento del valor de otra variable aleatoria, Y.', 'es[2]\\u200b De la definición podemos concluir que, si X e Y son iguales, entonces I(X;Y)=H(X).Propiedades[editar]La entropía tiene las siguientes propiedades:La entropía es positiva.', 'esEsto es evidente ya que al serpi}una probabilidad entonces0<=pi≤1\\\\leq 1}.', 'esPor tanto, podemos decir quelog2\\u2061pi≤0p_\\\\leq 0}y por tanto−log2\\u2061pi≥0p_\\\\geq 0}H≤loga\\u2061(n)(n)}Es decir, la entropía H está acotada superiormente (cuando es máxima) y no supone pérdida de información.Dado un proceso con posibles resultados con probabilidades relativas p1,...,pn, la funciónH(p1,…,pn),\\\\dots ,p_)\\\\,}es máxima en el caso de quep1=⋯=pn=1/n=\\\\dots =p_=1/n\\\\,}.', 'esEl resultado es intuitivo ya que tenemos la mayor incertidumbre del mensaje, cuando los valores posibles de la variable son equiprobablesDado un proceso con posibles resultados con probabilidades relativas p1,...,pn, la funciónH(p1,…,pn),\\\\dots ,p_)\\\\,}es nula en el caso de quepi=0=0}para todo i, excepto para una clase, tal que:pj=1=1}.', 'esDe forma intuitiva podemos pensar que cuando uno o más estados tienen una probabilidad alta, disminuye significativamente la entropía porque, como es lógico, existe una menor incertidumbre respecto al mensaje que se recibirá.Codificador óptimo[editar]Uncodificador óptimoes aquel que utiliza el mínimo número de bits para codificar un mensaje.', 'esUn codificador óptimo usará códigos cortos para codificar mensajes frecuentes y dejará los códigos de mayor longitud para aquellos mensajes que sean menos frecuentes.', 'esDe esta forma se optimiza el rendimiento del canal o zona de almacenamiento y el sistema es eficiente en términos del número de bits para representar el mensaje.Por ejemplo, elcódigo Morsese aprovecha de este principio para optimizar el número de caracteres a transmitir a partir del estudio de las letras más frecuentes del alfabeto inglés.', 'esAunque el código Morse no es un codificador óptimo, asigna a las letras más frecuente códigos más cortos.', 'esOtro ejemplo sería elalgoritmo de Huffmande codificación que sirve para compactar información.', 'es[3]\\u200b Este método se basa en elcodificador óptimo.', 'esPara ello lo primero que hace es recorrer toda la información para encontrar la frecuencia de los caracteres y luego a partir de esta información busca el codificador óptimo por medio de árboles binarios.', 'esAlgunas técnicas de compresión comoLZWodeflaciónno usan probabilidades de los símbolos aislados, sino que usan las probabilidades conjuntas de pequeñas secuencias de símbolos para codificar el mensaje, por lo que pueden lograr un nivel de compresión mayor.Podemos construir un codificador óptimo basándonos en la entropía de una variable aleatoria de información X.', 'esEn efecto, la entropía nos da elnúmero mediode bits (si usamos logaritmos de base 2) necesarios para codificar el mensaje a través de uncodificador óptimoy por tanto nos determina el límite máximo al que se puede comprimir un mensaje usando un enfoque símbolo a símbolo sin ninguna pérdida de información (demostrado analíticamente por Shannon), el límite de compresión (en bits) es igual a la entropía multiplicada por el largo del mensaje.', 'esReescribiendo la ecuación de cálculo de la entropía llegamos a que:H(X)=−∑ip(xi)log2\\u2061p(xi)=∑i−p(xi)log2\\u2061p(xi)=∑ip(xi)[log2(1)−log2\\u2061(p(xi))]=∑xp(x)log2\\u2061(1/p(x))p(x_)\\\\log _p(x_)=\\\\sum _-p(x_)\\\\log _p(x_)=\\\\sum _p(x_)[log_(1)-\\\\log _(p(x_))]=\\\\sum _p(x)\\\\log _(1/p(x))}Por lo tanto, la información (que se encuentra definida en bits, dado que la base del logaritmo es 2) que aporta un determinado valor o símboloxi\\\\,\\\\!', 'es}de una variable aleatoria discretaXse define como:I(xi)=log2\\u20611p(xi)=−log2\\u2061p(xi))=\\\\log _)}}=-\\\\log _)}}Esta expresión representa el número necesario de bits para codificar el mensaje x en elcodificador óptimoy por tanto la entropía también se puede considerar como una medida de la información promedio contenida en cada símbolo del mensaje.Ejemplo[editar]Supongamos que el número de estados de un mensaje es igual a 3 M1, M2y M3donde la probabilidad de M1es 50 %, la de M225 % y la de M325 %.Para M1tenemos quelog2\\u2061[1/p(M1)]=log2\\u20612=1[1/p(M_)]=\\\\log _2=1}Para M2tenemos quelog2\\u2061[1/p(M2)]=log2\\u20614=2[1/p(M_)]=\\\\log _4=2}Para M3tenemos quelog2\\u2061[1/p(M3)]=log2\\u20614=2[1/p(M_)]=\\\\log _4=2}Por tanto, en el codificador óptimo para transmitir M1hará falta un bit y para M2y M3será necesario contar con dos bits.', 'esPor ejemplo, podríamos codificar M1con \"0\", M2con \"10\" y M3con \"11\".', 'esUsando este convenio para codificar el mensaje M1M2M1M1M3M1M2M3usaríamos \"010001101011\" y por tanto 12 bits.', 'esEl valor de la entropía sería:H(X)=1/2log2\\u2061(2)+1/4log2\\u2061(4)+1/4log2\\u2061(4)=1,5(2)+1/4\\\\log _(4)+1/4\\\\log _(4)=1,5}Por tanto, elcodificador óptimonecesita de media 1,5 bits para codificar cualquier valor de X.Entropía condicional[editar]Véase también artículo dedicado:Entropía condicionalSupongamos que en vez de tener una única variable aleatoria X, existe otra variable Y dependientes entre sí, es decir el conocimiento de una (por ejemplo, Y) entrega información sobre la otra (por ejemplo, X).', 'esDesde el punto de vista de la entropía de la información podemos decir que la información de Y disminuirá la incertidumbre de X. Por tanto, podemos decir que la entropía de X será condicional a Y, y por tanto:H(X,Y)=−∑x,yp(x,y)log2\\u2061p(x,y)p(x,y)\\\\log _p(x,y)}Como por elteorema de Bayestenemos que p(x,y)=p(y)p(x|y) donde p(x|y) es la probabilidad de que se dé un estado de X conocida Y, podemos decir:H(X|Y)=−∑yp(y)∑xp(x|y)log2\\u2061p(x|y)p(y)\\\\sum _p(x|y)\\\\log _p(x|y)}Aplicación en criptoanálisis[editar]El concepto de entropía condicional es muy interesante en el campo delcriptoanálisis.', 'esProporciona una herramienta para evaluar el grado de seguridad de los sistemas.', 'esPor ejemplo, para un sistema decifradohay dos entropías condicionales interesantes:[4]\\u200b SupongamosUn mensaje M1es sometido a un proceso de cifrado usando la clave K1obteniendo E(K1,M1)=C1.PC(K)(K)}representan laprobabilidad condicionalde la clave K dado el criptograma recibido C. A veces también se denota porP(K|C)PC(M)(M)}representan la probabilidad condicional del mensaje M dado el criptograma recibido C. A veces también se denota porP(M|C)Entonces:Podemos calcular la entropía del conocimiento de la clave una vez conocido el texto cifrado, y por tanto medir laequivocación del mensaje(en inglés,message equivocation),HC(K)(K)}, también denotada porH(K|C), mediante la fórmula:HC(K)=−∑E,KP(E,K)logPE\\u2061(K)=−∑EP(E)∑KPE(K)logPE\\u2061(K)(K)=-\\\\sum _P(E,K)\\\\log _}(K)=-\\\\sum _P(E)\\\\sum _P_(K)\\\\log _}(K)}La primera igualdad es por la definición de la entropía condicional y la segunda por aplicación delteorema de Bayes.Observar que siHC(K)=0(K)=0}significa que se podrá romper el cifrado pues ya no hay incertidumbre.', 'esEsta anulación nos introduce en el concepto dedistancia de unicidad.Podemos calcular la entropía del conocimiento del mensaje una vez conocido el texto cifrado, y por tanto medir laequivocación de la clave(en inglés,key equivocation),HC(M)(M)}, también denotada porH(M|C), mediante la fórmula:HC(M)=−∑E,MP(E,M)logPE\\u2061(M)=−∑EP(E)∑MPE(M)logPE\\u2061(M)(M)=-\\\\sum _P(E,M)\\\\log _}(M)=-\\\\sum _P(E)\\\\sum _P_(M)\\\\log _}(M)}La primera igualdad es por la definición de la entropía condicional y la segunda por aplicación delteorema de Bayes.Ejemplo[editar]Supongamos una variable X con cuatro estados:x1,x2,x3,x4,x_,x_,x_}todos equiprobables y por tantop(xi)=1/4)=1/4}.', 'esExiste además otra variable Y con tres estados;y1,y2,y3,y_,y_}con probabilidadesp(y1)=1/2)=1/2}yp(y2)=p(y3)=1/4)=p(y_)=1/4}.', 'esSe conocen, además, las siguientes dependencias:SiY=y1}entonces los posibles valores de x sonx1,x2,x3,x4,x_,x_,x_}SiY=y2}entonces los posibles valores de x sonx2,x3,x_}SiY=y3}entonces los posibles valores de x sonx3,x4,x_}Aplicando las fórmulas tenemos:H(X)=2H(Y)=1,5H(X/Y)=1,5En este caso el conocimiento de la dependencia de X respecto Y reduce la entropía de X de 2 a 1,5.Entropía de un proceso estocástico[editar][5]\\u200b Unproceso estocástico\\\\}}es una secuencia indexada de variables aleatorias.', 'esEn general, puede haber dependencias entre las variables aleatorias.', 'esPara estudiar la probabilidad de cierto conjunto de valores se suele adoptar el siguiente convenio:Pr[(X1,X2,...,Xn)=(x1,x2,...,xn)]=p(x1,x2,...,xn),X_,...,X_)=(x_,x_,...,x_)]=p(x_,x_,...,x_)}Seai=1,..n\\\\}_}un proceso estocástico de n variables aleatorias, y seaAn}el conjunto de la posibles combinaciones de valores dei=1,..n\\\\}_}.', 'esSe define laentropía del proceso estocástico, también llamadaentropía del n-gramay denotado porHn}, como:Hn=H(X1,...,Xn)=∑s∈An−P((X1,...,Xn)=s)log\\u2061P((X1,...,Xn)=s)=H(X_,...,X_)=\\\\sum _}-P((X_,...,X_)=s)\\\\log P((X_,...,X_)=s)}Ratio de entropía[editar]Véase también artículo dedicado:Ratio de entropía[5]\\u200b Laratio de entropíade una secuencia de n variables aleatorias (proceso estocástico) caracteriza la tasa de crecimiento de la entropía de la secuencia con el crecimiento de n.Laratio de entropíade un proceso estocástico\\\\}}viene definida por la ecuación:H(X)=limn→∞1nH(X1,...,Xn)}H(X_,...,X_)}siempre que dicho límite exista.Otros tipos de entropía[editar]Algunas veces resulta conveniente usar otras medidas de información distintas a la definición de Shannon.', 'esEntre ellas, para un conjunto de probabilidadespi}dado, se pueden definir las siguientes:Entropía lineal:L=∑ipi(1−pi)p_(1-p_)}Entropía de Rényide orden q:Rq=11−qln(∑ipiq)=}ln(\\\\sum _p_^)}Entropía de tsallisde orden q:Tq(pi)=kq−1(1−∑ipiq),(})=\\\\left(1-\\\\sum _p_^\\\\right),}Para todos estos tipos de entropía se verifica que:Todas son mayores o igual a cero.L≥0;Rq≥0\\\\geq 0};Tq≥0\\\\geq 0}para todoq≥0Toman su valor máximo si las probabilidadespi}son iguales.La entropía de ShannonHe}es mayor o igual que L. i.e.He≥L\\\\geq L}ocurriendo igualdad solo en caso de queHe=0=0}Las entropías de Rényi y Tsallis son generalizaciones de la entropía de ShannonHe}dado quelimq→1Rq=limq→1Tq=HeR_=\\\\lim _T_=H_}Véase también[editar]Seguridad entrópicaEntropía cruzadaPerplejidadCapacidad de canalNeguentropía o SintropíaAntónimo de entropíaNotas[editar]↑Obsérvese que se usa el logaritmo en base 2 porque se considera que la información se va a representar mediante código binario (se quiere representar conbits).', 'esSi para representar la información se usaran valores en una baseaentonces sería conveniente utilizar el logaritmo en basea.↑Obsérvese que es una cantidad adimensional, es decir no lleva unidad.Referencias[editar]↑Cuevas Agustín, Gonzalo, \"Teoría de la información, codificación y lenguajes\", Ed.', 'esSEPA (Sociedad para Estudios Pedagógicos Argentinos), Serie Informática 1986↑Dan C. Marinescu, Gabriela M. Marinescu, \"Classical and Quantum Information\",Academic Press 2012↑Huffman, D., \"A method for the Construction of Minimum-Redundancy Codes\", Proc.', 'esIRE, Vol 40 1952↑\"Applied cryptology, cryptographic protocols and computer security models\", Richard A. DeMillo et al.', 'esAmerican Mathematical Society 1983↑abThomas M. Cover, Joy A. Thomas,\"Elements of Information Theory\", John Wiley & Sons.', 'esSecond Edition 2006Bibliografía[editar]Jorge Ramió Aguirre, Aplicaciones criptográficas.', 'esLibro guía de la asignatura deSeguridad Informática.', 'esEscuela Universitaria de Informática.Universidad Politécnica de Madrid.', 'esEnero de 1998.Enlaces externos[editar]Una Teoría Matemática de la ComunicaciónArchivadoel 31 de enero de 1998 enWayback Machine.', 'es(en inglés)Calculadora de la entropía de Shannon(en inglés)Calculadora de la entropía de Shannon para archivos(en inglés)Control de autoridadesProyectos WikimediaDatos:Q204570Multimedia:Entropy and information/Q204570IdentificadoresBNE:XX535116BNF:11985913j(data)GND:4743861-7LCCN:sh85044152NDL:01191172NKC:ph425914NLI:987007550784405171Diccionarios y enciclopediasBritannica:urlTreccani:urlOntologíasNúmero IEV:171-07-15Datos:Q204570Multimedia:Entropy and information/Q204570Obtenido de «https://es.wikipedia.org/w/index.php?title=Entropía_(información)&oldid=157773026»Categorías:Entropía de la informaciónTeoría de la informaciónCategorías ocultas:Wikipedia:Artículos con identificadores BNEWikipedia:Artículos con identificadores BNFWikipedia:Artículos con identificadores GNDWikipedia:Artículos con identificadores LCCNEsta página se editó por última vez el 27 ene 2024 a las 20:20.El texto está disponible bajo laLicencia Creative Commons Atribución-CompartirIgual 4.0; pueden aplicarse cláusulas adicionales.', 'esAl usar este sitio aceptas nuestrostérminos de usoy nuestrapolítica de privacidad.Wikipedia® es una marca registrada de laFundación Wikimedia, una organización sin ánimo de lucro.Política de privacidadAcerca de WikipediaLimitación de responsabilidadCódigo de conductaDesarrolladoresEstadísticasDeclaración de cookiesVersión para móvilesActivar o desactivar el límite de anchura del contenido', 'ckbئانترۆپیی زانیاری - ویکیپیدیا، ئینسایکڵۆپیدیای ئازادبۆ ناوەڕۆک بازبدەپێڕستی سەرەکیپێڕستی سەرەکیبڕۆ بۆ شریتەلابشارەوەڕێدۆزیدەستپێکڕووداوە ھەنووکەیییەکانوتارێک بە ھەڵکەوتبەخشین بە ویکیپیدیاھەڵسوکەوتیارمەتیدەربارەی ویکیپیدیادەروازەی کۆمەڵگەدوایین گۆڕانکارییەکانپەڕەی پەیوەندیگەڕانبگەڕێھەژمار دروست بکەبچۆ ژوورەوەئامڕازە تاکەکەسییەکانھەژمار دروست بکەبچۆ ژوورەوەئەو پەڕانەی بۆ ئەو دەستکاریکەرانەن کە لەدەرەوەنزیاتر فێر بەبەشدارییەکانلێدوانناوەڕۆکەکانبڕۆ بۆ شریتەلابشارەوەدەستپێک١نموونە٢وتارە پێوەندیدارەکان٣بەستەرە دەرەکییەکان٤پێڕستخشتەی ناوەڕۆکەکان بگۆڕەئانترۆپیی زانیاریAfrikaansالعربيةBoarischБългарскиBosanskiCatalàČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語دەستکاریی گرێدانەکانوتاروتووێژکوردیخوێندنەوەدەستکاریمێژووئامرازەکانئامرازەکانبڕۆ بۆ شریتەلابشارەوەکردەوەکانخوێندنەوەدەستکاریمێژووگشتیبەستەرەکان بە ئێرەوەگۆڕانکارییە پەیوەندیدارەکانپەڕگەیەک بار بکەپەڕە تایبەتەکانبەستەری ھەمیشەییزانیاریی پەڕەئەم پەڕەیە بکە بە ژێدەربەستەری کورتکراوە بەدەست بێنەداگرتن بە کیوئاڕکۆدبەندی ویکیدراوەچاپکردن/ھەناردەکردندروستکردنی کتێبێکداگرتن بە PDFوەشانی ئامادەی چاپلە پڕۆژەکانی ترویکیمیدیا کۆمنزلە ئینسایکڵۆپیدیای ئازادی ویکیپیدیاوەئانترۆپیی زانیاری(بەئینگلیزی:Information entropy) لەچەمکەکانیتیۆریی زانیارییەکە دەبێژێت چ ڕادەزانیاریلە ڕووداوێکدا بوونی ھەیە.', 'ckb[١]بە گشتی، ھەر چی ڕووداوێک گوماناوی یان بەھەڵکەوت بێت، زانیاریی زیاترێکی لە خۆی گرتووە.', 'ckbچەمکیئانترۆپیی زانیاریلەلایەنکڵاد شاننیبیرکارەوەداھێنراوە.', 'ckb[١]نموونە[دەستکاری]با نموونەیەک بھێنینەوە.', 'ckbئەگەر بە کەسێک شتێک بوترێت کە پێشتر لێی ئاگادار بێت، ڕادەی زانیارییەک کە پێی دەدرێت زۆر کەم دەبێت.', 'ckbواتا وتنی ئەو شتە بەو کەسە تۆزێک بۆی بێ\\u200cکەڵکە.', 'ckbلەتیۆریی زانیاریدادەڵێن ئانترۆپیی ئەو زانیارییە لە خوارەوەیە.بە پیچەوانەوە، ئەگەر بە کەسێک شتێک بوترێت کە پێشتر لێی ئاگادار نییە یان زۆر کەمی لێ دەزانێت، ئەو کەسە گەلەک زانیاریی نوێ وەردەگرێت کە لەوانەیە بۆی بەکەڵک بێت.', 'ckbلەوانەیە شتی نوێ فێرببێت.', 'ckbلەتیۆریی زانیاریدادەڵێن ئانترۆپیی ئەو زانیارییە لە سەرەوەیە.وتارە پێوەندیدارەکان[دەستکاری]تیۆریی زانیاریکڵاد شاننزانیاریبەستەرە دەرەکییەکان[دەستکاری]زانیاری و ئانترۆپی جیاوازن- باسێک لەسەر بەکارھێنانی زاراوەگەلی \"زانیاری\" و \"ئانترۆپی\".', 'ckb(ئینگلیزی)پێشەکییەک لەسەر بیردۆزی زانیاری(ئینگلیزی)ئانترۆپی و بیردۆزی زانیاری(ئینگلیزی)پێڕست[دەستکاری]^ئا\"Data Science for Business [Book]\".www.oreilly.com(بە ئینگلیزی).', 'ckbRetrieved2022-11-15.کۆمنزی ویکیمیدیا، میدیای پەیوەندیدار بەئانترۆپیی زانیاریتێدایە.وەرگیراو لە «https://ckb.wikipedia.org/w/index.php?title=ئانترۆپیی_زانیاری&oldid=1161262»پۆلەکان:ئانترۆپیی زانیاریتیۆریی زانیاریپۆلە شارداوەکان:ئەو پەڕانەی ژێدەری ئینگلیزییان ھەیە (en)وتارەکان بە دەقی زمانی explicitly cited ئینگلیزیپۆلی کۆمنز بە بەستەری خۆماڵیی یەکسان لەگەڵ ویکیدراوەوتارە ھەتیوەکانئەم پەڕەیە دواجار لە \\u200f٠٣:١٠ی \\u200f١٧ی کانوونی یەکەمی ٢٠٢٣ نوێ کراوەتەوە.دەق لەژێرCreative Commons Attribution-ShareAlike Licenseلەبەردەستە؛ لەوانەیە مەرجی تریشی پێوە ببێت.', 'ckbبۆ وردەکاریمەرجەکانی بەکارھێنانببینە.سیاسەتی تایبەتێتیدەربارەی ویکیپیدیانابەرپرسییەکانCode of Conductپەرەپێدەرانئامارەکانڕوونکردنەوەی کوکیبینینی مۆبایلیپانیی ناوەڕۆکی سنووردار بگۆڕە', 'csInformační entropie – WikipediePřeskočit na obsahHlavní menuHlavní menupřesunout do postranního paneluskrýtNavigaceHlavní stranaNápovědaPotřebuji pomocNejlepší článkyNáhodný článekPoslední změnyKomunitní portálPod lípouPodpořte WikipediiHledáníHledatVytvoření účtuPřihlášeníOsobní nástrojeVytvoření účtuPřihlášeníStránky pro odhlášené editorydozvědět se vícePříspěvkyDiskuseObsahpřesunout do postranního paneluskrýt(úvod)1Úvod2Definice3Příklad4Zdůvodnění5AspektyPřepnout podsekci Aspekty5.1Vztah k termodynamické entropii5.2Entropie jako informační obsah5.3Entropie jako míra rozmanitosti5.4Komprese dat5.5Světová technologická kapacita uložených a přenášených informací5.6Omezení entropie jako informačního obsahu5.7Omezení entropie v kryptografii5.8Data jako Markovův proces5.9b-ární entropie6Efektivita7CharakterizacePřepnout podsekci Charakterizace7.1Spojitost7.2Symetrie7.3Maximální7.4Aditivita8Další vlastnosti9Rozšíření diskrétní entropie na spojitý případPřepnout podsekci Rozšíření diskrétní entropie na spojitý případ9.1Diferenciální entropie9.2Omezení hustoty diskrétních bodů9.3Relativní entropie10Použití v kombinatoricePřepnout podsekci Použití v kombinatorice10.1Loomisova–Whitneyova nerovnost10.2Aproximace binomickým koeficientem11OdkazyPřepnout podsekci Odkazy11.1Reference11.2Související články11.3Literatura11.4Externí odkazyPřepnout obsahInformační entropie45 jazykůAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Upravit odkazyČlánekDiskusečeštinaČístEditovatEditovat zdrojZobrazit historiiNástrojeNástrojepřesunout do postranního paneluskrýtAkceČístEditovatEditovat zdrojZobrazit historiiObecnéOdkazuje semSouvisející změnyNačíst souborSpeciální stránkyTrvalý odkazInformace o stránceCitovat stránkuZískat zkrácené URLStáhnout QR kódPoložka WikidatTisk/exportVytvořit knihuStáhnout jako PDFVerze k tiskuNa jiných projektechWikimedia CommonsZ Wikipedie, otevřené encyklopedieDva bity entropie: Při dvou hodech poctivou mincí je informační entropie vyjádřená v bitech logaritmus o základu 2 počtu možných výsledků; při hodu dvěma mincemi existují čtyři možné výsledky, což odpovídá dvěma bitům entropie.', 'csInformační entropie je obecně průměrné množství informace obsažené v události, pokud uvažujeme všechny možné výsledky.Informačnínebo téžshannonovská entropiejestřední hodnotamnožstvíinformacepřipadající na jeden symbol generovanýstochastickýmzdrojem dat.Míra informační entropie přiřazená ke každé možné datové hodnotě je zápornýmlogaritmempravděpodobnostní funkcedané hodnoty:S=−∑iPilog\\u2061Pi=−EP\\u2061[log\\u2061P],S=-\\\\sum _P_\\\\log }=-\\\\operatorname _[\\\\log P],kdeEP\\u2061[X]=∑iPiXi _[X]=\\\\sum _P_X_}je střední hodnota definovaná pravděpodobnostíP.', 'cs[1]Když datový zdroj vyprodukuje hodnotu (symbol), která má nízkou pravděpodobnost (tj.', 'csnastane událost s nízkou pravděpodobností), nese tato událost více „informace“ (způsobí větší „překvapení“), než vyprodukování hodnoty, která má pravděpodobnost vysokou.', 'csPokud množství informace, které nese každá událost, považujeme zanáhodnou veličinu, informační entropie je střední hodnota této náhodné události.Entropieobecně vyjadřuje neuspořádanost nebo nejistotu, a definice entropie používaná v teorii informace je přímou analogiíentropiepoužívané vestatistické termodynamice.', 'csKoncept informační entropie představilClaude Shannonv roce 1948 ve svém článku „A Mathematical Theory of Communication\".', 'cs[2]Základní model systémudatové komunikaceobsahuje tři prvky: zdroj dat,komunikační kanála přijímač a – jak vyjádřil Shannon – „základním problémem komunikace“ je, aby přijímač byl schopen ze signálu, který přijímá z přenosového kanálu, identifikovat, jaká data byla vygenerovaná zdrojem.', 'cs[3]:s.379–423 a 623–656Entropie vyjadřuje absolutní dolní mez průměrné délkybezztrátovéhokódování dat vytvářených zdrojem.', 'csPokud je entropie zdroje menší nežkapacita komunikační kanálu, je možné data generovaná zdrojem spolehlivě přenést k přijímači (alespoň teoreticky, případně se zanedbáním určitých praktických kritérií jako například času potřebného pro přenos dat a složitosti systému k tomuto účelu).Informační entropie se nejčastěji měří vbitech(kterým se v této souvislosti také říká „shannons“), někdy v „přirozených jednotkách“ (natech) nebo v desítkových číslicích (nazývaný „dits“, „bans“ nebo „hartleys“).', 'csJednotka měření závisí na základu logaritmu použitého pro výpočet entropie.Logaritmus rozdělení pravděpodobnosti je vhodný jako míra entropie, protože pro nezávislé zdroje je aditivní.', 'csNapříklad entropie vrhu poctivou mincí je 1 bit a entropiemvrhů jembitů.', 'csV přímočaré reprezentaci je potřebalog2(n)bitů pro reprezentaci proměnné, která může nabývat jedné znhodnot, jestliženje mocninou dvojky, tj.n= 2k.', 'csPokud jsou všechny hodnoty stejně pravděpodobné, entropie (v bitech) budelog2(n)=k.', 'csPokud se jedna z hodnot bude objevovat častěji než ostatní, pozorování této hodnoty je méně informativní, než kdyby došlo k méně obvyklému výsledku.', 'csNaopak pozorování vzácnější události poskytuje více informace.', 'csProtože pozorování méně pravděpodobných událostí se objevují méně často, způsobují, že entropie (braná jako průměrná informace) pocházející z nerovnoměrně distribuovaných dat je vždy menší nebo rovnalog2(n).', 'csEntropie je nulová, pokud je jisté, že nastane jedna určitá událost.', 'csEntropie je veličina, která kvantifikuje tato kritéria pro známé rozdělení pravděpodobnosti zdrojových dat.', 'csEntropie nezávisí naobsahupozorovaných událostí (významuzprávy), ale bere v úvahu pouze pravděpodobnosti jejich výskytu.', 'csTo znamená, že entropie závisí pouze na podkladovém rozdělení pravděpodobnosti, nikoli na významu událostí samých.Úvod[editovat|editovat zdroj]Základní myšlenkou teorie informace je, že „informační hodnota“ předávané zprávy závisí na míře, jak je obsah zprávy překvapivý.', 'csNení žádným překvapením, když dojde k události, která je velmi pravděpodobná.', 'csNaopak, v případě málo pravděpodobných událostí, je jejich výskyt mnohem informativnější.', 'csNapříklad znalost, že určité číslonevyhrajev loterii, nese velmi malou informaci, protože jakékoli určité číslo téměř určitě nevyhraje.', 'csAle znalost, že určité číslo v loteriivyhraje, má vysokou informační hodnotu, protože informuje o události s velmi nízkou pravděpodobností.Informační obsah(také nazývanýsurprisal– překvapení) událostiEje rostoucí funkcí převrácené hodnoty pravděpodobnostip(E)události, přesnějiI(E)=−log2\\u2061(p(E))=log2\\u2061(1/p(E))(p(E))=\\\\log _(1/p(E))}.', 'csEntropie měří očekávané (tj.', 'csprůměrné) množství informace obsažené v informaci o výsledku náhodného pokusu.', 'csZ toho vyplývá, že vrh kostkou má vyšší entropii než házení mincí, protože každý z výsledků vrhu kostkou má menší pravděpodobnost (p=16}}) než každý z výsledků vrhu mincí (p=12}}).Entropie je míranepředvídatelnostivýsledku, jehoprůměrného informačního obsahu.', 'csChceme-li získat intuitivní porozumění těmto termínům, můžeme si představit průzkum veřejného mínění.', 'csTakové průzkumy se provádějí, aby se zjistila informace, která není známá.', 'csTo znamená, že výsledek průzkumu je těžkopředpověditelnýa provedení průzkumu přináší nějakounovou informaci; to je jen jiný způsob jak říct, žeapriorníentropie výsledků průzkumu je velká.', 'csPokud by se stejný průzkum prováděl znovu, krátce po prvním průzkumu, když je výsledek prvního průzkum už známý, bylo by možné výsledek druhého průzkum dobře předvídat, a jeho výsledky sotva obsahují mnoho nové informace; to znamená, žeapriorníentropie výsledku druhého průzkumu je relativně malá v porovnání s prvním průzkumem.Uvažujme vrh mincí.', 'csPokud pravděpodobnost padnutí panny je stejná jako pravděpodobnost padnutí orla, pak entropie vrhu mincí je stejně vysoká jako u libovolného pokusu se dvěma stejně pravděpodobnými výsledky.', 'csVýsledek vrhu mincí nelze předpovědět: pokud si máme vybrat, neexistuje žádná průměrná výhoda předvídat určitý výsledek, protože každá předpověď bude správná s pravděpodobností1/2.', 'csTakový vrh mincí má jeden bit entropie, protože existují dva možné výsledky se stejnou pravděpodobností, a víme, že výsledek má jeden bit informace.', 'csNaproti tomu, vrh mincí, které by měla dvě panny a žádného orla má nulovou entropii, protože při vrhu takovou mincí vždy padne panna a výsledek lze dokonale předpovědět.', 'csLibovolná událost se stejně pravděpodobnými výsledky má Shannonovu entropiilog2\\u20612=12=1}bit.', 'csPodobně každý pokus setřemistejně pravděpodobnými hodnotami obsahujelog2\\u206133}(asi 1,58496) bitů informace, protože může nabývat jednu ze tří hodnot.Pokud budeme anglický text považovat za řetězec znaků, má docela nízkou entropii; to znamená, že je docela dobře předvídatelný.', \"csI v případě, že nevíme přesně, jaký znak bude následovat, můžeme si být jisti, že například 'e' bude mnohem obvyklejší než 'z', nebo že kombinace 'qu' bude mnohem obvyklejší než 'q' následované jiným písmenem, a že i kombinace 'th' bude obvyklejší než 'z', 'q' nebo 'qu'.\", 'csZbytek slova můžeme často odhadnout z několika prvních písmen.', 'csAnglický text má entropii 0,6 až 1,3 bitů na znak zprávy.', 'cs[4]:s.234Pokud jekomprimačníschéma bezztrátové – takové, že původní zprávu lze vždy dekomprimovat – pak komprimovaná zpráva obsahuje stejné množství informace jako původní, ale je zakódována méně znaky.', 'csObsahuje tedy více informace na znak (má vyšší entropii).', 'csKomprimovaná zpráva má menšíredundanci.Shannonova věta o zdrojovém kódováníříká, že bezztrátové komprimační schéma nemůže v průměru komprimovat zprávy, aby mělyvícenež jeden bit informace na bit zprávy, ale, že jakoukoli hodnotumenšínež jeden bit informace na bit zprávy lze dosáhnout použitím vhodného kódovacího schématu.', \"csEntropie zprávy na bit znásobená délkou zprávy je mírou, kolik informace zpráva celkem obsahuje.Uvažujme, že bychom měli vysílat posloupnosti obsahující 4 znaky 'A', 'B', 'C' a 'D' – například zprávu 'ABADDCAB'.\", 'csTeorie informace říká, jak spočítat nejmenší možné množství informace, kterou je třeba přenést.', \"csJsou-li všechna 4 písmena stejně pravděpodobná (25 %), není možné najít lepší kódovací schéma (pro binární kanál), než zakódování každého písmene dvěma bity: 'A' například jako '00', 'B' jako '01', 'C' jako '10' a 'D' jako '11'.\", \"csPokud se však 'A' objevuje s 70% pravděpodobností, 'B' s 26%, a 'C' i 'D' s 2% pravděpodobností, bylo by možné přiřadit jednotlivým znakům kódy s různou délkou, takže přijetí '1' by znamenalo, že je třeba se podívat na jiný bit pokud žádná posloupnost 2 bitů jedniček už byla přijata.\", \"csV tomto případě může být 'A' kódované jako '0' (jeden bit), 'B' jako '10' a 'C' a 'D' jako '110' a '111'.\", 'csJe snadno vidět, že v 70 % případů stačí jeden bit informace, v 26 % případů dvou bitů a pouze ve 4 % případů 3 bitů.', \"csPrůměrně stačí méně než 2 bity, protože entropie je nižší (kvůli vysoký prevalenci symbolů 'A' následovaných 'B' – současně 96 % znaků).\", 'csTento vliv zachycuje výpočet sumy pravděpodobnosti vážené logaritmem míry pravděpodobnosti.Ze Shannonovy věty také vyplývá, že žádné bezztrátové komprimační schéma nemůže zkrátitúplně všechnyzprávy.', 'csPokud některé zprávy vyjdou kratší, alespoň jedna musí vyjít delší kvůliDirichletově principu.', 'csPři praktickém použití to obecně není problém, protože typicky přenášíme pouze určitý typ zpráv, například dokumenty v angličtině nikoli náhodné řetězce znaků, nebo digitální fotografie a nikoli šum, a proto není důležité, že komprimační algoritmus nějaké nepravděpodobné nebo nezajímavé posloupnosti prodlužuje.Definice[editovat|editovat zdroj]Graf binární entropie (v intervalu <0;1>)Shannon definoval entropiiΗdiskrétní náhodné proměnnéXs možnými hodnotami,\\\\ldots ,x_\\\\right\\\\}}apravděpodobnostní funkcíP(X) (X)}takto:H(X)=E\\u2061[I\\u2061(X)]=E\\u2061[−log\\u2061(P(X))].', 'cs(X)=\\\\operatorname [\\\\operatorname (X)]=\\\\operatorname [-\\\\log(\\\\mathrm (X))].', 'cs}Pro entropii používá znakΗ(velké řecké písmenoéta), podleBoltzmannovy Η-věty.E }jeoperátor střední hodnotyaIjeinformační obsahX.', 'cs[5]:s.11[6]:s.19–20I(X)je také náhodná proměnná.Entropii můžeme explicitně zapsat jakoH(X)=−∑i=1nP(xi)logb\\u2061P(xi) (X)=-\\\\sum _^ (x_)\\\\log _\\\\mathrm (x_)}}kdebjezákladpoužitéhologaritmu.', 'csV tomto případě se nejčastěji jako základbpoužívá hodnota 2, méně častoEulerovo čísloe, nebo 10.', 'csEntropie je prob= 2vyjádřena vbitech, prob=ev jednotkách nazývanýchnata prob= 10v jednotkách nazývanýchban, hartley nebo dit.', 'cs[7]PokudP(xi) = 0pro nějakéi, bere se hodnota odpovídajícího členu0 logb(0)nulová, což odpovídálimitělimp→0+plog\\u2061(p)=0.}p\\\\log(p)=0.', 'cs}Můžeme také definovatpodmíněnou entropiidvou událostíXaY, které nabývají hodnotxi}ayj}, vzorcemH(X|Y)=−∑i,jp(xi,yj)log\\u2061p(xi,yj)p(yj) (X|Y)=-\\\\sum _p(x_,y_)\\\\log ,y_)})}}}kdep(xi,yj),y_)}je pravděpodobnost, žeX=xi}aY=yj}.', 'csTuto hodnotu chápeme jako množství náhodnosti v náhodné proměnnéXpro danou hodnotu náhodné proměnnéY.Příklad[editovat|editovat zdroj]EntropieΗ(X)(tj.střednípřekvapení) vrhu mincí měřené v bitech vynesené podle poctivosti mincePr(X= 1), kdeX= 1reprezentuje padnutí panny.Entropie je v tomto případě nejvýše 1 bit a předání výsledku vrhu mincí (se dvěma možnými výsledky) vyžaduje průměrně nejvýše 1 bit informace (pro poctivou minci je to přesně 1 bit).', 'csZakódování vrhu poctivou kostkou (6 hodnot se stejnou pravděpodobností) vyžaduje průměrně log26 bitů.Podrobnější informace naleznete v článcíchBinární funkce entropieaBernoulliho proces.Uvažujme házení mincí se známými, ne nutně poctivými pravděpodobnostmi, že padne panna nebo orel; to lze modelovatBernoulliho procesem.Entropie neznámého výsledku dalšího vrhu mincí je nejvyšší, jestliže mince je poctivá (tj.', 'csjestliže panna i orel mají stejnou pravděpodobnost 1/2).', 'csU poctivé mince je nejobtížnější předpovídat výsledek dalšího vrhu; výsledek každého vrhu mincí přináší celý jedenbitinformace.', 'csDůvodem je, žeH(X)=−∑i=1nP(xi)logb\\u2061P(xi)=−∑i=1212log2\\u206112=−∑i=1212⋅(−1)=1\\\\mathrm (X)&=-\\\\sum _^ (x_)\\\\log _\\\\mathrm (x_)}\\\\\\\\&=-\\\\sum _^}\\\\log _}}\\\\\\\\&=-\\\\sum _^}\\\\cdot (-1)}=1\\\\end}}Pokud však víme, že mince není poctivá, ale pravděpodobnosti hodu panny případně orla jsoup, příp.q, kdep≠q, pak je nejistota menší.', 'csPři každém vrhu je pravděpodobnější, že padne jedna strana než druhá.', 'csSnížení nejistoty je kvantifikováno nižší entropií: každý vrh mincí přináší v průměru méně než jedenbitinformace.', 'csJestliže napříkladp=0,7, pakH(X)=−plog2\\u2061(p)−qlog2\\u2061(q)=−0,7log2\\u2061(0,7)−0,3log2\\u2061(0,3)≈−0,7⋅(−0,515)−0,3⋅(−1,737)=0,8816<1\\\\mathrm (X)&=-p\\\\log _(p)-q\\\\log _(q)\\\\\\\\&=-0,7\\\\log _(0,7)-0,3\\\\log _(0,3)\\\\\\\\&\\\\approx -0,7\\\\cdot (-0,515)-0,3\\\\cdot (-1,737)\\\\\\\\&=0,8816<1\\\\end}}Rovnoměrné rozdělení pravděpodobnosti má maximální nejistotu a tedy maximální entropii.', 'csEntropie se pak může pouze snížit z hodnoty odpovídající rovnoměrné pravděpodobnosti.', 'csExtrémním případem je, že mince má dvě panny, takže nikdy nemůže padnout orel, nebo dva orly, takže nikdy nemůže padnout panna.', 'csVrh takovou mincí v sobě neobsahuje žádnou nejistotu, žádnou novou informaci, protože výsledek vrhu je předem známý, takže entropie je nulová.Entropii lze normalizovat tím, že ji vydělíme délkou informace.', 'csVýsledný poměr se nazývámetrika entropiea je mírou náhodnosti informace.Zdůvodnění[editovat|editovat zdroj]Pro pochopení významu-∑pilog(pi), nejdříve definujeme informační funkciIpomocí událostiis pravděpodobnostípi.', 'csMnožství informace získané pozorováním událostiivyplývá z Shannonova řešení základníchvlastnostíinformace:[8]I(p)jemonotonně klesajícívp: zvýšení pravděpodobnosti události snižuje množství informace získané pozorováním události a naopak.I(p) ≥ 0: informace jenezápornáveličina.I(1) = 0: události, ke kterým dojde vždy, nepřinášejí žádnou informaci.I(p1p2) = I(p1) + I(p2): informacenezávislých událostíje aditivní.Klíčová je poslední vlastnost.', 'csŘíká, že sdružená pravděpodobnost nezávislých zdrojů informace přináší stejné množství informace jako obě události samostatně.', 'csSpeciálně jestliže první událost je jedním znstejně pravděpodobnýchvýsledků a druhá jedním zmstejně pravděpodobných výsledků, pak sdružená událost mámnmožných výsledků.', 'csTo znamená, že jestliže je potřebalog2(n)bitů pro zakódování první hodnoty alog2(m)bitů pro zakódování druhé, potřebujemelog2(mn) = log2(m) + log2(n)pro zakódování obou.', \"csShannon objevil, že funkce pro určení množství informace, zachovávající tuto aditivitu, je logaritmická, tj.,I(p)=log\\u2061(1p)=−log\\u2061(p): (p)=\\\\log \\\\left(}\\\\right)=-\\\\log(p):}JestližeIje informační funkce, o které předpokládáme, že je dvakrát spojitě derivovatelná, dostáváme:I(p1p2)=I(p1)+I(p2)p2I′(p1p2)=I′(p1)I′(p1p2)+p1p2I″(p1p2)=0I′(u)+uI″(u)=0(uI′(u))′=0I(p_p_)&=&I(p_)+I(p_)\\\\\\\\p_I'(p_p_)&=&I'(p_)\\\\\\\\I'(p_p_)+p_p_I''(p_p_)&=&0\\\\\\\\I'(u)+uI''(u)&=&0\\\\\\\\(uI'(u))'&=&0\\\\end}}Tatodiferenciální rovnicemá k řešeníI(u)=klog\\u2061upro jakékolik∈R }.\", 'csZ podmínky 2. vyplývá, že kk<0.klze zvolit ve tvaruk=−1/log\\u2061xprox>1, což je ekvivalentní s výběrem určitéhozákladu logaritmu.', 'csRůznéjednotky informace(bityprobinární logaritmuslog2,natypropřirozený logaritmusln,banyprodesítkový logaritmuslog10atd.)', 'csjsoukonstantní násobkyjiného.', 'csPokud například uvažujeme vrh poctivou mincí, hození panny dáválog2(2) = 1bit informace, což je přibližně 0,693 natů nebo 0,301 desítkových číslic.nvrhů přináší díky aditivitěnbitů informace, což je přibližně0,693nnatů nebo0,301ndesítkových číslic.Pokud existuje rozdělení, u kterého událostimůže nastat s pravděpodobnostípia provedemeNpokusů, při kterých výsledekinastaneni=Npikrát, celkové množství přijaté informace je∑iniI(pi)=−∑iNpilog\\u2061pi\\\\mathrm (p_)}=-\\\\sum _\\\\log }}}.Průměrnémnožství informace, kterou získáme z události je proto−∑ipilog\\u2061pi.\\\\log }}.', 'cs}Aspekty[editovat|editovat zdroj]Vztah k termodynamické entropii[editovat|editovat zdroj]Inspirace pro použití slovaentropiev teorii informace pochází z podobnosti mezi Shannonovým vzorcem a velmi podobnými vzorci známými zestatistické mechaniky.Nejobecnější vzorec pro termodynamickouentropiiStermodynamického systémuvestatistické termodynamicejeGibbsova entropie:S=−kB∑piln\\u2061pi}\\\\sum p_\\\\ln p_\\\\,}kdekBjeBoltzmannova konstantaapije pravděpodobnostmikrostavu.Gibbsovu entropiidefinovalJ.', 'csWillard Gibbsv roce 1878, krátce po publikaci dílaLudwiga Boltzmannav roce 1872.', 'cs[9]Gibbsovu entropii lze použít téměř nezměněnou ve světěkvantové fyziky, kde dávávon Neumannovu entropii, kterou zavedlJohn von Neumannv roce 1927:S=−kBTr(ρln\\u2061ρ)}\\\\,}(\\\\rho \\\\ln \\\\rho )\\\\,}kde ρ jematice hustotykvantově mechanického systému a Tr jestopa matice.Při každodenním praktickém používání není spojitost mezi informační entropií a termodynamickou entropií evidentní.', 'csFyzici a chemici se spíše zajímají ozměnyentropie, protože systém se vyvíjí spontánně ze svých počátečních podmínek ve shodě sdruhým zákonem termodynamiky, než o neměnné rozdělení pravděpodobnosti.', 'csMalá hodnotaBoltzmannovy konstantykBukazuje, že změnyS/kBi pro velmi malé množství látky v chemických a fyzikálních procesech reprezentuje entropii, která je extrémně velká v porovnání s čímkoli přikompresi datnebozpracování signálu.', 'csV klasické termodynamice je entropie definována pomocí makroskopických měření bez odkazů na nějaké rozdělení pravděpodobnosti, což je hlavním aspektem pro definici informační entropie.Spojení mezi termodynamikou a tím, co nyní nazýváme teorie informace, si poprvé všimlLudwig Boltzmann, který je vyjádřil svouproslulou rovnicí:S=kBln\\u2061(W)}\\\\ln(W)}kdeSje termodynamická entropie určitého makroskopického stavu (definovaná termodynamickými parametry jako například teplotou, objemem, energií, atd.', 'cs),Wje počet mikroskopických stavů (různých kombinací částic v různých energetických stavech), které mohou dávat daný makroskopický stav akBjeBoltzmannova konstanta.', 'csPředpokládá se, že každý mikroskopický stav je stejně pravděpodobný, takže pravděpodobnost daného mikroskopického stavu jepi= 1/W.', 'csPokud tyto pravděpodobnosti dosadíme do výše uvedeného výrazu pro Gibbsovu entropii (nebo ekvivalentněkBkrát Shannonovu entropii), dostaneme Boltzmannovu rovnici.', 'csPoužijeme-li termíny z teorie informace, je informační entropie systému rovna množství „chybějící“ informace potřebné pro určení mikroskopického stavu pro daný makroskopický stav.V přístupuEdwina Thompsona Jaynese(1957) se na termodynamickou entropii, jak ji vysvětlujestatistická mechanika, musí nahlížet jako naaplikaciShannonovy teorie informace: termodynamická entropie se interpretuje jako množství další Shannonovy informace potřebné pro určení konkrétního mikroskopického stavu systému, který při popisu systému výhradně makroskopickými proměnnými klasické termodynamiky zůstává skrytý, přičemž konstanta úměrnosti je právěBoltzmannova konstanta.', 'csDodáváním tepla do systému se zvyšuje jeho termodynamická entropie, protože se zvyšuje počet možných mikroskopických stavů systému, které jsou konzistentní s měřitelnou hodnotou své makroskopické proměnné, což způsobuje, že jakýkoli úplný popis stavu je komplikovanější.', 'cs(Viz článek:Termodynamika maximální entropie).Maxwellův demonmůže (hypoteticky) snižovat termodynamickou entropii systému použitím informací o stavech jednotlivých molekul; ale jak ukázal v roce 1961Rolf Landauerse svými spolupracovníky, aby démon sám fungoval, musí sám zvyšovat termodynamickou entropii v procesu, alespoň o množství Shannonovy informace, kterou potřebuje nejdřív získat a uložit; kvůli tomu se celková termodynamická entropie nesnižuje (což vysvětluje paradox).Landauerův principurčuje mimo jiné nejnižší množství tepla, které musí počítač vyprodukovat při zpracování určitého množství informací.', 'csAni moderní počítače se k této efektivitě ani zdaleka neblíží.Entropie jako informační obsah[editovat|editovat zdroj]Podrobnější informace naleznete v článkuShannonova věta o zdrojovém kódování.Entropie je definovaná v kontextu pravděpodobnostního modelu.', 'csNezávislý vrh poctivou mincí má entropii 1 bit na hod.', \"csZdroj, které vždy generuje dlouhý řetězec znaků B má entropii 0, protože následující znak bude vždy 'B'.Poměr entropiedatového zdroje je průměrný početbitůna symbol potřebných pro kódování tohoto zdroje.\", 'csShannonovy pokusy s lidmi jako prediktory ukazují, že anglický text má informační poměr 0,6 až 1,3 bitů na znak.', 'cs[10]Komprimační algoritmusPrediction by partial matching(PPM) může na anglickém textu dosáhnout komprimačního poměru 1,5 bitu na znak.V předchozím příkladě jsou důležité tyto body:Entropie vyjádřená v bitech nemusí být celé číslo.Mnoho bitů dat nenese žádné informace.', 'csDatové struktury často obsahují redundantní informace nebo mají identické části bez ohledu na to, jaké informace obsahují.Pokud je Shannonova definice entropie aplikována na zdroj informace, může určovat minimální kapacitu kanálu nezbytnou pro spolehlivý přenos dat ze zdroje, pokud jsou zakódovány jako binární hodnoty (viz varování níže v kurzívě).', 'csVzorec pro tuto kapacitu může být odvozen výpočtem matematického očekávánímnožství informaceobsažené v každé číslici přenášených dat.Viz takéShannonova–Hartleyova věta.Shannonova entropie měří informaci obsaženou ve zprávě jako protiklad k části zprávy, která je pevně určena (nebo předvídatelná).K dalším příkladům patří redundance ve struktuře jazyka nebo statistické vlastnosti týkající se frekvence výskytu dvojic, trojic, atd., písmen nebo slov.VizMarkovův řetězec.Entropie jako míra rozmanitosti[editovat|editovat zdroj]Podrobnější informace naleznete v článkuIndex rozmanitosti.Entropie je jedním ze způsobů, jak měřit rozmanitost.', 'csKonkrétně Shannonova entropie je logaritmus1D,skutečný rozmanitostindex s parametr rovné 1.Komprese dat[editovat|editovat zdroj]Podrobnější informace naleznete v článkuKomprimace dat.Entropie efektivně limituje výkonnost nejsilnější bezztrátové komprimační metody, což lze teoreticky provést pomocítypické množinynebo v praxi pomocíHuffmanova kódování,Lempel–Zivneboaritmetického kódování.', 'csViz takéKolmogorovova složitost.', 'csV praxi komprimační algoritmy úmyslně zahrnují nějakou rozumnou redundanci ve forměkontrolního součtupro zabezpečení proti chybám.Světová technologická kapacita uložených a přenášených informací[editovat|editovat zdroj]Studie z roku 2011 v časopisuScienceodhaduje světovou technologickou kapacitu pro uložení a přenos optimálně komprimovaných informací normalizovanou na nejefektivnější komprimační algoritmy dostupné v roce 2007, je vlastně odhadem entropie technologicky dostupný zdrojů.', 'cs[11]:s.60–65Hodnoty v entropicky komprimovanýchexabytechTyp Informace19862007Uložení2,6295Vysílání4321900Telekomunikace0,28165Autoři odhadli technologickou kapacitu pro uložení (plně entropicky komprimovaných) informací, kterou má lidstvo k dispozici, v roce 1986 a 2007.', 'csInformace rozdělují do tří kategorií: objem datových médií, která byla k dispozici pro ukládání informací, množství informací předaných pomocí jednosměrnýchvysílacíchsítí a výměnu informací pomocí obousměrnýchtelekomunikačnísítí.', 'cs[11]Omezení entropie jako informačního obsahu[editovat|editovat zdroj]Existuje několik dalších konceptů příbuzných entropii, které určitým způsobem matematicky kvantifikují informační obsah:Vlastní informacejednotlivé zprávy nebo symbolu vzatého z daného rozdělení pravděpodobnosti,entropiedaného rozdělení pravděpodobnosti zpráv nebo symbolů apoměr entropiestochastického procesu.', 'cs(pro určité posloupnosti zpráv nebo symbolů generované daným stochastickým procesem může také definovat „poměr vlastní informace“, který je vždy roven poměru entropie v případěstacionárního procesu.)', 'csJinámnožství informacese také používají pro porovnávání různých zdrojů informace.Výše uvedené koncepty si nesmíme plést.', 'csČasto je zřejmé pouze z kontextu, který koncept se myslí.', 'csKdyž například někdo říká, že „entropie“ angličtiny je asi 1 bit na znak, jedná se o modelování angličtiny jako stochastického procesu a mluví se opoměruentropie.', 'csI sám Shannon používal termín podobně.Při použití velmi velkých bloků může být odhad entropie na jeden znak uměle nízký, protože rozdělení pravděpodobnosti posloupnosti není známo přesně; je to pouze odhad.', 'csPokud bychom uvažovali text každé dosud publikované knihy jako posloupnost, a text celé knihy bychom považovali za symbol, a jestliže je celkemNpublikovaných knih a každá kniha je publikovaná pouze jednou, odhad pravděpodobnosti každé knihy je1/Na entropie (v bitech) je−log2(1/N) = log2(N).', 'csJako praktický kód to odpovídá přiřazeníjednoznačného identifikátorukaždé knize a používat ho místo textu knihy, když se chceme odkázat na text knihy.', 'csTo je mimořádně užitečné pro mluvení o knihách, ale není to tak užitečné pro charakterizaci informačního obsahu jednotlivých knih nebo jazyka obecně: z identifikátoru knihy nelze rekonstruovat obsah knihy bez znalosti rozdělení pravděpodobnosti, tj.', 'csúplného textu všech knih.', 'csKlíčovou myšlenkou je, že musíme uvažovat složitost pravděpodobnostního modelu.Kolmogorovova složitostje teoretické zobecnění této myšlenky, která umožňuje zvážení informačního obsahu posloupnosti nezávislý na jakýkoli určitý pravděpodobnost model; uvažuje nejkratšíprogramprouniverzální počítač, který vytvoří požadovanou posloupnost.', 'csKód, který dosahuje poměru entropie posloupnosti pro daný model, plus kódová kniha (tj.', 'cspravděpodobnostní model), je jedním takovým programem, ale nemusí být nejkratší.Fibonacciho posloupnost je 1, 1, 2, 3, 5, 8, 13, .... pokud uvažujeme posloupnost jako zprávu a každé číslo jako symbol, existuje téměř stejně symbolů jako znaků ve zprávě, což dává entropii přibližnělog2(n).', 'csPrvních 128 symbolů Fibonacciho posloupnost má entropii přibližně 7 bitů/symbol, ale posloupnost lze vyjádřit pomocí vzorce [F(n) = F(n−1) + F(n−2)pron= 3, 4, 5, …,F(1) =1,F(2) = 1] a toto vzorec má mnohem nižší entropie a je použita na jakýkoli délka Fibonacciho posloupnosti.Omezení entropie v kryptografii[editovat|editovat zdroj]Vkryptoanalýzese entropie často používá jako přibližná míra nepředvídatelnosti kryptografického klíče, jehož reálnánejistotaje neměřitelná.', 'csNapříklad 128bitový klíč, který je rovnoměrně a náhodně generovaný, má entropii 128 bitů.', 'csTaké je třeba (v průměru)2128−1}pokusů pro prolomení šifry hrubou silou.', 'csEntropie však selhává, jestliže možné klíče nejsou voleny rovnoměrně.', 'cs[12][13]Místo toho lze použít míru nazývanouguessworkpro změření úsilí potřebného pro útok hrubou silou.', 'cs[14]V kryptografii se mohou objevit další problémy při používání entropie kvůli nerovnoměrnému rozdělení.', 'csNapříkladjednorázová šifra, která šifruje text pomocí funkce xor s miliónem binárních číslic.', 'csPokud má tabulka entropii milión bitů, je šifrování dokonalé.', 'csPokud má šifra entropii jen 999 999 bitů, rovnoměrně distribuovaný (každý jednotlivý bit jednorázového hesla má entropii 0,999999 bitů) může stále poskytovat dobrou bezpečnost.', 'csAle jestliže jednorázové heslo má entropii 999 999 bitů, přičemž první bit je pevný a zbývajících 999 999 bitů je naprosto náhodných, první bit šifrového textu nebude vůbec zašifrovaný.Data jako Markovův proces[editovat|editovat zdroj]Obvyklý způsob, jak definovat entropii textu, vychází zMarkovova modelutextu.', 'csPro zdroj 0. řádu (každý znak je vybraný nezávisle na posledních znacích) je binární entropie:H(S)=−∑pilog\\u2061pi, (})=-\\\\sum p_\\\\log p_,}kdepije pravděpodobnosti.', 'csProMarkovův zdrojprvního řádu (zdroj, u něhož je pravděpodobnost výběru znak závislá pouze na předchozím znaku), jepoměr entropie(anglickyentropy rate):H(S)=−∑ipi∑jpi(j)log\\u2061pi(j), (})=-\\\\sum _p_\\\\sum _\\\\ p_(j)\\\\log p_(j),}kdeijestav(anglickystate) (určitý předchozí znaky) api(j)(j)}je pravděpodobnostjdanýijako předchozí znak.Pro Markovův zdroj druhého řádu je poměr entropieH(S)=−∑ipi∑jpi(j)∑kpi,j(k)log\\u2061pi,j(k).', 'cs(})=-\\\\sum _p_\\\\sum _p_(j)\\\\sum _p_(k)\\\\ \\\\log \\\\ p_(k).', 'cs}b-ární entropie[editovat|editovat zdroj]Obecněb-ární entropiezdrojeS}}= (S,P)sezdrojovou abecedouS= adiskrétním rozdělením pravděpodobnostiP= , kdepije pravděpodobnostai(značímepi=p(ai))je definovaný vztahem:Hb(S)=−∑i=1npilogb\\u2061pi, _(})=-\\\\sum _^p_\\\\log _p_,}Hodnotabv „b-ární entropie“ je počet různých symbolůideální abecedypoužívané jako standardní měřítko pro měření zdrojové abecedy.', 'csV teorii informace je pro kódování informacínutné a postačující, aby abeceda obsahovala dva symboly.', 'csProto implicitně pracujeme sb= 2(„binární entropie“).', 'csEntropie zdrojové abecedy spolu s daným empirickým rozdělením pravděpodobnosti je číslo rovné počtu (případně zlomkovému) symbolů „ideální abecedy“ s optimálním rozdělením pravděpodobnosti nezbytných pro kódování každého symbolu zdrojové abecedy.', 'csPřitom „optimální rozdělení pravděpodobnosti“ zde znamenárovnoměrné rozdělení: zdrojová abeceda snsymboly má nejvyšší možnou entropii (mezi abecedami snsymboly), je-li rozdělení pravděpodobnosti abecedy rovnoměrné.', 'csUkazuje se, že tato optimální entropie jelogb(n).Efektivita[editovat|editovat zdroj]Zdrojová abeceda s nerovnoměrným rozdělením bude mít menší entropii, než kdyby její symboly měly rovnoměrné rozdělení („optimalizovaná abeceda“).', 'csTento nedostatek entropie lze vyjádřit poměrem nazývaným efektivita:η(X)=−∑i=1np(xi)logb\\u2061(p(xi))logb\\u2061(n)^)\\\\log _(p(x_))}(n)}}}Použitím základních vlastností logaritmu lze tuto hodnotu také vyjádřit jako:η(X)=−∑i=1np(xi)logb\\u2061(p(xi))logb\\u2061(n)=∑i=1nlogb\\u2061(p(xi)−p(xi))logb\\u2061(n)=∑i=1nlogn\\u2061(p(xi)−p(xi))=logn\\u2061(∏i=1np(xi)−p(xi))^)\\\\log _(p(x_))}(n)}}=\\\\sum _^(p(x_)^)})}(n)}}=\\\\sum _^\\\\log _(p(x_)^)})=\\\\log _(\\\\prod _^p(x_)^)})}Efektivita je vhodnou mírou pro využitíkomunikačního kanálu.', 'csUvedená veličina se také označuje normalizovaná entropie, protože entropie je dělena maximální entropiílogb\\u2061(n)(n)}}.', 'csEfektivita je navíc nezávislá na volbě základub, jak je ukázáno výše.Charakterizace[editovat|editovat zdroj]Shannonova entropie jecharakterizovánamalým počtem dále uvedených kritérií.', 'csJakákoli definice entropie, která těmto kritériím vyhovuje, má tvar−K∑i=1npilog\\u2061(pi)^p_\\\\log(p_)}kdeKje konstanta, která závisí na volbě jednotek měření.V dalším textu budeme psátpi= Pr(X=xi)aΗn(p1, …,pn) = Η(X).Spojitost[editovat|editovat zdroj]Míra musí býtspojitá, takže změna hodnoty pravděpodobnosti o velmi malou hodnotu způsobí pouze malou změnu entropie.Symetrie[editovat|editovat zdroj]Míra nesmí záviset na pořadí výsledkůxi.Hn(p1,p2,…)=Hn(p2,p1,…) _\\\\left(p_,p_,\\\\ldots \\\\right)=\\\\mathrm _\\\\left(p_,p_,\\\\ldots \\\\right)}atd.Maximální[editovat|editovat zdroj]Míra musí být maximální, jestliže všechny výsledky jsou stejně pravděpodobně (nejistota je nejvyšší, když všechny možné události mají stejnou pravděpodobnost).Hn(p1,…,pn)≤Hn(1n,…,1n)=logb\\u2061(n).', 'cs_(p_,\\\\ldots ,p_)\\\\leq \\\\mathrm _\\\\left(},\\\\ldots ,}\\\\right)=\\\\log _(n).', 'cs}Pro stejně pravděpodobné události se entropie musí zvyšovat s počtem výsledků.Hn(1n,…,1n⏟n)=logb\\u2061(n)<logb\\u2061(n+1)=Hn+1(1n+1,…,1n+1⏟n+1).', 'cs_\\\\underbrace },\\\\ldots ,}} _=\\\\log _(n)<\\\\log _(n+1)=\\\\mathrm _\\\\underbrace },\\\\ldots ,}} _.', 'cs}Rozdělení, které má maximálnídiferenciální entropiípro spojité náhodné proměnné, jevícerozměrné normální rozdělení.Aditivita[editovat|editovat zdroj]Velikost entropie musí být nezávislá na tom, jak je proces rozdělen do složek.Tato poslední funkcionální podmínka charakterizuje entropii systému se subsystémy.', 'csVyžaduje, aby entropii systému bylo možné spočítat z entropií jeho subsystémů, jestliže interakce mezi subsystémy jsou známé.Je-li dán soubornrovnoměrně distribuovaných prvků, které jsou rozděleny dokpodsystémů, každý sb1, ...,bkprvky, entropie celého systému se musí rovnat sumě entropií jednotlivých podsystémů a entropií jednotlivých boxů, každý vážený s pravděpodobností, že je v příslušném podsystému.Prokladná celá číslabikdeb1+ … +bk=n,Hn(1n,…,1n)=Hk(b1n,…,bkn)+∑i=1kbinHbi(1bi,…,1bi).', 'cs_\\\\left(},\\\\ldots ,}\\\\right)=\\\\mathrm _\\\\left(}},\\\\ldots ,}}\\\\right)+\\\\sum _^}}\\\\,\\\\mathrm _}\\\\left(}},\\\\ldots ,}}\\\\right).', 'cs}Pokud zvolímek=n,b1= … =bn= 1, z toto vyplývá, že entropie určitého výsledku je nulová:Η1(1) = 0.', 'csZ toho vyplývá, že efektivitu zdrojové abecedy snsymboly lze definovat jednoduše jako jejín-ární entropii.', 'csViz takéRedundance (teorie informace).Další vlastnosti[editovat|editovat zdroj]Shannonova entropie splňuje další vlastnosti; pro některé z nich je užitečné interpretovat entropii jako množství dodané informace (nebo odstraněné nejistoty), kterou přinese zjištění hodnoty náhodné proměnnéX:Přidání nebo odstranění události s nulovou pravděpodobností nepřispívá k entropii:Hn+1(p1,…,pn,0)=Hn(p1,…,pn) _(p_,\\\\ldots ,p_,0)=\\\\mathrm _(p_,\\\\ldots ,p_)}.Entropie diskrétní náhodné proměnné je nezáporné číslo:H(X)≥0 (X)\\\\geq 0}.', 'cs[15]:s.15UžitímJensenovy nerovnostilze dokázat, žeH(X)=E\\u2061[logb\\u2061(1p(X))]≤logb\\u2061(E\\u2061[1p(X)])=logb\\u2061(n) (X)=\\\\operatorname \\\\left[\\\\log _\\\\left(}\\\\right)\\\\right]\\\\leq \\\\log _\\\\left(\\\\operatorname \\\\left[}\\\\right]\\\\right)=\\\\log _(n)}[15].', 'cs:s.29Této maximální entropielogb(n)se efektivně dosáhne takovou zdrojovou abecedou, která má rovnoměrné rozdělení pravděpodobnosti: nejistota je maximální, když jsou všechny možné události stejně pravděpodobné.Entropie nebo množství informace získané vyhodnocením(X,Y)(tj.', 'csvyhodnocováníXaYsoučasně) se rovná informaci získané provedením dvou pokusů po sobě: nejdříve vyhodnotíme hodnotuY, potom hodnotuXse znalostí hodnotyY.', 'csTo lze napsat jakoH(X,Y)=H(X|Y)+H(Y)=H(Y|X)+H(X).', 'cs(X,Y)=\\\\mathrm (X|Y)+\\\\mathrm (Y)=\\\\mathrm (Y|X)+\\\\mathrm (X).', 'cs}PokudY=f(X)kdefje funkce, pakH(f(X)|X)=0.', 'csPoužití předchozího vzorce naH(X,f(X))dáváH(X)+H(f(X)|X)=H(f(X))+H(X|f(X)), (X)+\\\\mathrm (f(X)|X)=\\\\mathrm (f(X))+\\\\mathrm (X|f(X)),}takžeH(f(X))≤H(X), entropie jedné náhodné proměnné se může snížit pouze tehdy, když na druhou proměnnou aplikujeme nějakou funkci.PokudXaYjsou dvě nezávislé náhodné proměnné, pak znalost hodnotyYneovlivňuje naši znalost hodnotyX(protože tyto proměnné se vzájemně neovlivňují):H(X|Y)=H(X).', 'cs(X|Y)=\\\\mathrm (X).', 'cs}Entropie dvou současných událostí není větší než suma entropií každé z událostí, přičemž rovnost nastane, jsou-li události nezávislé.', 'csKonkrétněji, jestližeXaYjsou dvě náhodné proměnné na stejném pravděpodobnostním prostoru a(X,Y)označuje jejich kartézský součin, pakH(X,Y)≤H(X)+H(Y).', 'cs(X,Y)\\\\leq \\\\mathrm (X)+\\\\mathrm (Y).', 'cs}EntropieH(p) (p)}jekonkávnív pravděpodobnostní funkcip, tj.H(λp1+(1−λ)p2)≥λH(p1)+(1−λ)H(p2) (\\\\lambda p_+(1-\\\\lambda )p_)\\\\geq \\\\lambda \\\\mathrm (p_)+(1-\\\\lambda )\\\\mathrm (p_)}pro všechny pravděpodobnostní hmotové funkcep1,p2,p_}a0≤λ≤1[15].', 'cs:s.32Rozšíření diskrétní entropie na spojitý případ[editovat|editovat zdroj]Diferenciální entropie[editovat|editovat zdroj]Podrobnější informace naleznete v článkuDiferenciální entropie.Shannonova entropie je definována pouze pro náhodné proměnné nabývající diskrétních hodnot.', 'csOdpovídající vzorec pro spojitou náhodnou proměnnou shustotou pravděpodobnostif(x)s konečným nebo nekonečným nosičemX }na reálné ose je možné definovat analogicky použitím výše uvedeného tvaru entropie jako střední hodnoty:h[f]=E\\u2061[−ln\\u2061(f(x))]=−∫Xf(x)ln\\u2061(f(x))dx.', 'cs[-\\\\ln(f(x))]=-\\\\int _ }f(x)\\\\ln(f(x))\\\\,dx.', 'cs}Tato veličina se obvykle nazýváspojitá entropienebodiferenciální entropie.', 'csPředchůdcem spojité entropieh[f]je výraz pro funkcionálΗvBoltzmannověH-větě.Přestože analogie mezi oběma funkcemi je sugestivní, je třeba položit otázku, zda je diferenciální entropie povoleným rozšířením Shannonovy diskrétní entropie.', 'csDiferenciální entropie postrádá některé vlastnosti Shannonovy diskrétní entropie – může být mimo jiné záporná – proto byly doporučovány opravy, předevšímomezující hustota diskrétních bodů.Aby bylo možné odpovědět na tuto otázku, je nutné ukázat spojení mezi těmito dvěma funkcemi:Pro získání obecně konečné míry, když sevelikost intervalů, na které je rozsah hodnot rozdělen, blíží k nule.', 'csV diskrétním případě je velikost intervalu (implicitní) šířkou každého zn(konečných nebo nekonečných) intervalů, jejíchž pravděpodobnosti jsou označenypn.', 'csKdyž uvažujeme rozšíření na spojitý případ, šířka intervalu musí být stanovena explicitně.Vyjdeme od spojité funkcefdiskretizované na intervaly velikostiΔ.', 'csPodle věty o střední hodnotě existuje hodnotaxiv každém intervalu tak, žef(xi)Δ=∫iΔ(i+1)Δf(x)dx)\\\\Delta =\\\\int _^f(x)\\\\,dx}integrál funkceflze aproximovat (v Riemannovském smyslu) jako∫−∞∞f(x)dx=limΔ→0∑i=−∞∞f(xi)Δ^f(x)\\\\,dx=\\\\lim _\\\\sum _^f(x_)\\\\Delta }kde tato limita odpovídá tomu, že „velikost intervalu se blíží k nule“.OznačímeHΔ:=−∑i=−∞∞f(xi)Δlog\\u2061(f(xi)Δ) ^:=-\\\\sum _^f(x_)\\\\Delta \\\\log \\\\left(f(x_)\\\\Delta \\\\right)}a rozepsáním logaritmu dostávámeHΔ=−∑i=−∞∞f(xi)Δlog\\u2061(f(xi))−∑i=−∞∞f(xi)Δlog\\u2061(Δ).', 'cs^=-\\\\sum _^f(x_)\\\\Delta \\\\log(f(x_))-\\\\sum _^f(x_)\\\\Delta \\\\log(\\\\Delta ).', 'cs}Pro Δ → 0 máme∑i=−∞∞f(xi)Δ→∫−∞∞f(x)dx=1∑i=−∞∞f(xi)Δlog\\u2061(f(xi))→∫−∞∞f(x)log\\u2061f(x)dx.\\\\sum _^f(x_)\\\\Delta &\\\\to \\\\int _^f(x)\\\\,dx=1\\\\\\\\\\\\sum _^f(x_)\\\\Delta \\\\log(f(x_))&\\\\to \\\\int _^f(x)\\\\log f(x)\\\\,dx.\\\\end}}Pamatujte, želog(Δ) → −∞proΔ → 0, vyžaduje speciální definici diferenciální nebo spojité entropie:h[f]=limΔ→0(HΔ+log\\u2061Δ)=−∫−∞∞f(x)log\\u2061f(x)dx,\\\\left(\\\\mathrm ^+\\\\log \\\\Delta \\\\right)=-\\\\int _^f(x)\\\\log f(x)\\\\,dx,}čemuž říkáme, jak již bylo řečeno,diferenciální entropie.', 'csTo znamená, že diferenciální entropienenílimitou Shannonovy entropie pron→ ∞.', 'csNaopak, liší se od limity Shannonovy entropie o nekonečné posunutí (viz také článekinformační rozměr).Omezení hustoty diskrétních bodů[editovat|editovat zdroj]Podrobnější informace naleznete v článkuOmezení hustoty diskrétních bodů.Ukazuje se, že diferenciální entropie, na rozdíl od Shannonovy entropie, obecněnenídobrou mírou nejistoty nebo informace.', 'csDiferenciální entropie může být například záporná; také není invariantní vůči spojité transformaci souřadnic.', 'csTento problém může být ilustrován změnou jednotek, pokud byxbyla veličina, která má fyzikální rozměr.f(x)pak bude mít rozměr1/x.', 'csArgument logaritmu musí být bezrozměrný, jinak je nevlastní, takže diferenciální entropie definovaná výše bude nevlastní.', 'csPokudΔje nějaká „standardní“ hodnotax(tj.', 'cs„velikost intervalu“) a proto má stejný rozměr, pak lze modifikovanou diferenciální entropii zapsat v patřičném tvaru jako:H=∫−∞∞f(x)log\\u2061(f(x)Δ)dx^f(x)\\\\log(f(x)\\\\,\\\\Delta )\\\\,dx}Přičemž výsledek bude stejný pro jakoukoli volbu jednotek veličinyx.', 'csTotiž limita diskrétní entropie proN→∞by také obsahovala členlog\\u2061(N), který by obecně byl nekonečný.', 'csTo se dalo očekávat, protože po změně spojitých proměnných na diskrétní obvykle vyjde entropie nekonečná.Omezující hustota diskrétních bodůje skutečně mírou, o kolik snazší je popsat nějaké rozdělení než rozdělení, které je rovnoměrně přes své kvantizační schéma.Relativní entropie[editovat|editovat zdroj]Podrobnější informace naleznete v článkuZobecněná relativní entropie.Další užitečnou mírou entropie, která funguje stejně dobře pro diskrétní i spojitý případ, jerelativní entropierozdělení.', 'csJe definovaná jakoKullbackova–Leiblerova divergencez rozdělení na reference míramtakto.', 'csPředpokládejme, že rozdělení pravděpodobnostipjeabsolutně spojitévzhledem k mířem, tj.', 'csmá tvarp(dx) =f(x)m(dx)pro nějakou nezápornoum-integrovatelnou funkcifsm-integrálem 1, pak lze relativní entropii definovat jakoDKL(p‖m)=∫log\\u2061(f(x))p(dx)=∫f(x)log\\u2061(f(x))m(dx).', 'cs}(p\\\\|m)=\\\\int \\\\log(f(x))p(dx)=\\\\int f(x)\\\\log(f(x))m(dx).', 'cs}V tomto tvaru relativní entropie zobecňuje (až na změnu znaménka) jak diskrétní entropii, kde míramjepočítací míra, tak diferenciální entropii, kde míramjeLebesgueova míra.', 'csPokud míramje samotné rozdělení pravděpodobnosti, relativní entropie je nezáporná, a je nulová, jestližep=mjako míry.', 'csJe definována pro jakýkoli prostor s mírou, je tedy nezávislá na volbě souřadnic a invariantní vůči změně souřadnic, jestliže správně bereme v úvahu transformaci mírym.', 'csRelativní entropie a implicitně entropie a diferenciální entropie, závisí na „referenční“ mířem.Použití v kombinatorice[editovat|editovat zdroj]Entropie se stala užitečnou veličinou vkombinatorice.Loomisova–Whitneyova nerovnost[editovat|editovat zdroj]Jejím jednoduchým příkladem je alternativní důkazLoomisovy–Whitneyovy nerovnosti: pro každou podmnožinuA⊆Zd, máme|A|d−1≤∏i=1d|Pi(A)|\\\\leq \\\\prod _^|P_(A)|}kdePijeortogonální projekcevi-té souřadnici:Pi(A)=.', 'cs(A)=\\\\,\\\\ldots ,x_,x_,\\\\ldots ,x_):(x_,\\\\ldots ,x_)\\\\in A\\\\}.', 'cs}Důkaz je jednoduchým důsledkemShearerovy nerovnosti:, jestližeX1, …,Xdjsou náhodné proměnné aS1, …,Snjsou podmnožiny množiny takové, že každé celé číslo mezi 1 adleží přesně vrtěchto podmnožinách, pakH[(X1,…,Xd)]≤1r∑i=1nH[(Xj)j∈Si] [(X_,\\\\ldots ,X_)]\\\\leq }\\\\sum _^\\\\mathrm [(X_)_}]}kde(Xj)j∈Si)_}}je kartézský součin náhodné proměnnéXjs indexyjvSi(takže rozměry tohoto vektoru se rovnají velikostiSi).Načrtneme, jak z uvedeného vyplývá Loomisova–Whitneyova nerovnost: NechťXje rovnoměrně distribuovaná náhodná proměnná s hodnotami vA, takže každá hodnotaAmá stejnou pravděpodobnost.', 'csPak (z dalších vlastností entropie zmíněných výše)H(X)=log\\u2061|A| (X)=\\\\log |A|}, kde|A|označujemohutnost množinyA.', 'csNechťSi= .', 'csHodnoty(Xj)j∈Si)_}}jsou obsaženy vPi(A), a tedyH[(Xj)j∈Si]≤log\\u2061|Pi(A)| [(X_)_}]\\\\leq \\\\log |P_(A)|}.', 'csTo nyní použijeme pro omezení pravé strany Shearerovy nerovnosti a aplikujeme exponenciální funkci na obě strany výsledné nerovnosti.Aproximace binomickým koeficientem[editovat|editovat zdroj]Pro celá čísla0 <k<npoložímeq=k/n.', 'csPak2nH(q)n+1≤(nk)≤2nH(q), (q)}}}\\\\leq }\\\\leq 2^ (q)},}kdeH(q)=−qlog2\\u2061(q)−(1−q)log2\\u2061(1−q).', 'cs(q)=-q\\\\log _(q)-(1-q)\\\\log _(1-q).', 'cs}[16]:s.43Zde je náčrt důkazu.', 'csUvědomíme si, že(nk)qqn(1−q)n−nq}q^(1-q)^}je jeden člen výrazu∑i=0n(ni)qi(1−q)n−i=(q+(1−q))n=1.^}q^(1-q)^=(q+(1-q))^=1.', 'cs}Přeskupení dává horní mez.', 'csPro dolní mez musíme nejdřív ukázat pomocí nějaké algebry, že se jedná o největší člen v sumě.', 'csAle pak,(nk)qqn(1−q)n−nq≥1n+1}q^(1-q)^\\\\geq }}protože suma mán+ 1členů.', 'csPřeskupení dává dolní mez.Hezkým důsledkem je, že počet binárních řetězců délkyn, které obsahují přesněkjedniček, je přibližně2nH(k/n) (k/n)}}.', 'cs[17]Odkazy[editovat|editovat zdroj]Reference[editovat|editovat zdroj]V tomto článku byl použitpřekladtextu z článkuEntropy (information theory)na anglické Wikipedii.↑PATHRIA, R. K.; BEALE, Paul.Statistical Mechanics.', 'cs3. vyd.', 'cs[s.l.', 'cs]: Academic Press, 2011.Dostupné online.ISBN978-0123821881.', 'csS. 51.↑SHANNON, Claude E.A Mathematical Theory of Communication.Bell System Technical Journal.', 'csČervenec–říjen 1948, roč.', 'cs27, čís.', 'cs3, s. 379–423.Dostupné online.DOI10.1002/j.1538-7305.1948.tb01338.x.', 'cs(PDF, archivováno způvodního zdroje)↑SHANNON, Claude E. A Mathematical Theory of Communication.Bell System Technical Journal.', 'cs1948, roč.', 'cs27, čís.', 'cs3, s. 379–423.Dostupné online.DOI10.1002/j.1538-7305.1948.tb01338.x., červenec a říjen↑SCHNEIER, B.Applied Cryptography.', 'cs2. vyd.', 'cs[s.l.', 'cs]: John Wiley and Sons, 1996.Dostupné online.↑BORDA, Monica.Fundamentals in Information Theory and Coding.', 'cs[s.l.', 'cs]: Springer, 2011.Dostupné online.ISBN978-3-642-20346-6.↑Mathematics of Information and Coding.', 'cs[s.l.', 'cs]: American Mathematical Society, 2002.Dostupné online.ISBN978-0-8218-4256-0.↑SCHNEIDER, T.D.Information theory primer with an appendix on logarithms[online].', 'csNational Cancer Institute, 2007-04-14.Dostupné online.', 'cs[nedostupný zdroj]↑CARTER, Tom.An introduction to information theory and entropy.', 'csSanta Fe: [s.n.', 'cs], březen 2014.Dostupné online.↑Srovnejte: Boltzmann, Ludwig (1896, 1898).', 'csVorlesungen über Gastheorie : 2 Volumes – Lipsko 1895/98 UB: O 5262-6. anglická verze: Lectures on gas theory.', 'csPřeklad Stephen G. Brush (1964) Berkeley: University of California Press; (1995) New York: DoverISBN0-486-68455-5↑Mark Nelson.The Hutter Prize[online].', 'cs2006-08-24 [cit.', 'cs2008-11-27].Dostupné v archivupořízeném dne 2018-03-01.↑ab\"The World\\'s Technological Capacity to Store, Communicate, and Compute Information\", Martin Hilbert a Priscila López (2011),Science, 332(6025); volný přístup k článku: [martinhilbert.net/WorldInfoCapacity.html]↑MASSEY, James.', 'csGuessing and Entropy.', 'csIn:Proc.', 'csIEEE International Symposium on Information Theory.', 'cs[s.l.', 'cs]: [s.n.', 'cs], 1994.Dostupné online.↑MALONE, David; SULLIVAN, Wayne.', 'csGuesswork is not a Substitute for Entropy.', 'csIn:Proceedings of the Information Technology & Telecommunications Conference.', 'cs[s.l.', 'cs]: [s.n.', 'cs], 2005.Dostupné online.↑PLIAM, John.', 'csGuesswork and variation distance as measures of cipher security.', 'csIn:International Workshop on Selected Areas in Cryptography.', 'cs[s.l.', 'cs]: [s.n.', 'cs], 1999.DOI10.1007/3-540-46513-8_5.↑abcElements of Information Theory.', \"csHoboken, New Jersey: Wiley, 2006-07-18.Dostupné online.ISBN978-0-471-24195-9.↑Aoki, New Approaches to Macroeconomic Modeling.↑Probability and Computing, M. Mitzenmacher and E. Upfal, Cambridge University PressTento článek obsahuje informace z článkuShannon's entropyz encyklopediePlanetMath, jejíž licence je podobná licencímCreative Commons Attribution/Share-Alike.Související články[editovat|editovat zdroj]Podmíněná entropieKřížová entropie– míra průměrného počtu bitů potřebné pro identifikaci události ze sady možností mezi dvěma rozděleními pravděpodobnostiIndex rozmanitosti– alternativa se blíží pro určení množství rozmanitost v rozdělení pravděpodobnostiEntropie (šipka času)Entropické kódování– kódovací schéma, které přiřazuje kódy na symboly tak jako vyhovovat délkám kódu s pravděpodobností symbolů.Odhad entropieNerovnost entropické sílyPoměr entropiíFisherova informaceGrafová entropieHammingova vzdálenostHistorie entropieHistorie teorie informaceInformační geometrieSdružená entropie– míra, kolik entropie je obsaženo ve sdruženém systému dvou náhodných proměnných.Kolmogorovova–Sinaiova entropievdynamických systémechLevenštejnova vzdálenostVzájemná informaceNegentropiePerplexitaKvalitativní variace– jiné míryvariabilitypronominální rozděleníKvantová relativní entropie– míra rozlišitelnosti mezi dvěma kvantovými stavy.Rényiho entropie– zobecnění Shannonovy entropie; je jedním z rodiny funkcionálů pro kvantifikaci rozmanitosti, nejistoty nebo náhodnosti systému.NáhodaShannonův indexTheilův indexTypoglykemieLiteratura[editovat|editovat zdroj]Arndt, C. (2004),Information Measures: Information and its Description in Science and Engineering, Springer,ISBN978-3-540-40855-0Cover, T. M., Thomas, J.\", 'csA.', 'cs(2006),Elements of information theory, 2.', 'csEdition.', 'csWiley-Interscience.ISBN0-471-24195-4.Gray, R. M. (2011),Entropy and Information Theory, Springer.MacKay, David J.', 'csC..Information Theory, Inference, and Learning AlgorithmsCambridge: Cambridge University Press, 2003.ISBN0-521-64298-1Mathematical Theory of Entropy.', 'cs[s.l.', 'cs]: Cambridge University Press, 2011.Dostupné online.ISBN978-0-521-17738-2.Shannon, C.E.,Weaver, W.(1949)The Mathematical Theory of Communication, Univ of Illinois Press.ISBN0-252-72548-4Stone, J. V. (2014), Chapter 1 ofInformation Theory: A Tutorial Introduction, University of Sheffield, Anglie.ISBN978-0956372857.Externí odkazy[editovat|editovat zdroj]Obrázky, zvuky či videa k tématuentropie informacínaWikimedia CommonsEntropyEncyclopædia of Mathematics, EMS Press 2004 (1994)Introduction to entropy and informationna webuPrincipia Cybernetica WebEntropyinterdisciplinární časopis o různých aspektech konceptu entropie.', 'csVolný přístup.Description of information entropy from \"Tools for Thought\" by Howard RheingoldA java applet representing Shannon\\'s Experiment to Calculate the Entropy of EnglishSlidy o nárůstu informace a entropiiAn Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science– wikibook o interpretaci konceptu entropie.Network Event Detection With Entropy Measures, Dr. Raimund Eimann, University of Auckland, PDF; 5993 kB – PhD práce, která se zabývá tím, jak lze měření entropie použít pro detekci anomálií v sítích.', 'csEntropynaRosetta Code– repozitář implementací výpočtu Shannonovy entropie v různých programovacích jazycích.', 'csInformation Theory for Intelligent PeopleArchivováno13.', 'cs6.', 'cs2020 naWayback Machine.. Krátký úvod do axiómů teorie informace, entropie, vzájemné informace, Kullbackovy–Lieblerovy divergence a Jensenovy–Shannonovu vzdálenosti.Online nástroj pro výpočet entropie (plain text)Online nástroj pro výpočet entropie (binary)Autoritní dataNKC:ph425914TDKIV:000000451BNE:XX535116BNF:cb11985913j(data)GND:4743861-7LCCN:sh85044152NDL:01191172NLI:987007550784405171Portály:MatematikaCitováno z „https://cs.wikipedia.org/w/index.php?title=Informační_entropie&oldid=23657459“Kategorie:Entropie a informaceTeorie informaceSkryté kategorie:Údržba:Články obsahující odkazy na nedostupné zdrojeMonitoring:Články přeložené z enwikiMonitoring:Články s identifikátorem NKCMonitoring:Články s identifikátorem TDKIVMonitoring:Články s identifikátorem BNEMonitoring:Články s identifikátorem BNFMonitoring:Články s identifikátorem GNDMonitoring:Články s identifikátorem LCCNMonitoring:Články s identifikátorem NDLMonitoring:Články s identifikátorem NLIStránka byla naposledy editována 13.', 'cs2.', 'cs2024 v 22:49.Text je dostupný podlicencí Creative Commons Uveďte původ – Zachovejte licenci, případně za dalších podmínek.', 'csPodrobnosti naleznete na stráncePodmínky užití.Ochrana osobních údajůO WikipediiVyloučení odpovědnostiKontaktujte WikipediiKodex chováníVývojářiStatistikyProhlášení o cookiesMobilní verzePřepnout omezenou šířku obsahu', \"caEntropia de Shannon - Viquipèdia, l'enciclopèdia lliureVés al contingutMenú principalMenú principalmou a la barra lateralamagaNavegacióPortadaArticle a l'atzarArticles de qualitatComunitatPortal viquipedistaAgenda d'actesCanvis recentsLa tavernaContacteXatDonatiusAjudaCercaCercaCrea un compteInicia la sessióEines personalsCrea un compteInicia la sessióPàgines per a editors no registratsmés informacióContribucionsDiscussió per aquest IPContingutmou a la barra lateralamagaInici1Descripció2Història3Preàmbul4Entropia d'un text comú5Utilitat pràctica6L'entropia com a mesura de similitud7Ús en aprenentatge automàtic8Referències9Bibliografia10Vegeu també11Enllaços externsCommuta la taula de continguts.Entropia de Shannon45 llengüesAfrikaansالعربيةBoarischБългарскиBosanskiکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Modifica els enllaçosPàginaDiscussiócatalàMostraModificaMostra l'historialEinesEinesmou a la barra lateralamagaAccionsMostraModificaMostra l'historialGeneralQuè hi enllaçaCanvis relacionatsPàgines especialsEnllaç permanentInformació de la pàginaCitau aquest articleObtén una URL abreujadaDescarrega el codi QRElement a WikidataImprimeix/exportaCrear un llibreBaixa com a PDFVersió per a impressoraEn altres projectesCommonsDe la Viquipèdia, l'enciclopèdia lliureDos bits d'entropia: en el cas de llançar a l'aire dues monedes justes, l'entropia d'informació en bits és el logaritme en base 2 del nombre de possibles resultats; amb dues monedes hi ha quatre possibles resultats, i dos bits d'entropia.\", \"caEn general, l'entropia d'informació és la quantitat mitjana d'informació que es transmet en un esdeveniment, considerant-ne tots els possibles resultats.L'entropia de Shannon, (formulada perClaude Shannon)[1][2]és una funció matemàtica que intuïtivament es correspon amb la quantitat d'informaciócontinguda o lliurada per una font d'informació.\", \"caAquesta font pot ser un text escrit en un idioma determinat, unsenyal elèctric, unfitxer d'ordinadoro qualsevol (col·lecció de bytes).\", \"caDes del punt de vista d'un receptor, com més informació diferent emet la font, més gran és l'entropia (o incertesa sobre el que emet la font), i viceversa.\", \"caCom més informació rep el receptor sobre el missatge transmès, més disminueix l'entropia (incertesa) respecte aquest missatge, per raó d'aquest augment d'informació.\", \"caLa definició d'entropia de Shannon és tal que com més redundant sigui la font, menys informació conté.\", \"caEn absència de restriccions particulars, l'entropia és màxima per a una font en la que tots els símbols són igualment probables (equiprobables).Donada una variable aleatòria discretaX, que pren valors en l'alfabetX}}i segueix una certa distribucióp:X→[0,1]}\\\\to [0,1]}:H(X)=−∑i=1nP(xi)log\\u2061P(xi) (X)=-\\\\sum _^ (x_)\\\\log \\\\mathrm (x_)}}onΣdenota la suma al llarg dels possibles valors de la variable aleatòria.\", \"caLa tria de base perlog, ellogaritme, varia en funció de l'apliació.\", 'caUsant la base 2, s\\'obté l\\'entropia enbits(o en \"shannons\"), mentre que la baseedóna les \"unitats naturals\"nat, i la base 10 dóna unitats de \"dits\", \"bans\", o \"hartleys\".', \"caUna definició equivalent de l'entropia és l'esperançade la auto-informació d'una variable.\", \"ca[3]Descripció[modifica]En el cas particular d'un sistema deTelecomunicacions, l'entropia de la font d'informació (l'emissor) indica la incertesa del receptor respecte del que la font transmetrà.\", 'caPer exemple, una font que es considera que sempre envia el mateix símbol, per exemple, la lletra \"a\", té una entropia zero, és a dir, mínima.', 'caDe fet, un receptor que coneix només les estadístiques de transmissió de la font assegura que el símbol següent serà un \"a\", sense por d\\'equivoca-se.', 'caEl receptor no necessita rebre cap senyal per eliminar la incertesa sobre el que ha transmès la font.', 'caD\\'altra banda, si es considera que la font envia una \"a\" la meitat de temps i una \"b\" l\\'altra meitat, el receptor no està segur que el següent caràcter a rebre sigui una \"a\".', \"caL'entropia de la font en aquest cas, per tant, no és zero (és positiva) i representa quantitativament la incertesa que reina sobre la informació que emana de la font.\", 'caDes del punt de vista del receptor, l\\'entropia indica la quantitat d\\'informació que necessita obtenir per eliminar completament la incertesa (o dubte) sobre el que ha transmès la font.Història[modifica]El 1948, mentre treballava a Bell Laboratories, l\\'enginyer elèctric Claude Shannon va formalitzar matemàticament la naturalesa estadística de la \"informació perduda\" en senyals de línia telefònica.', \"caPer això, va desenvolupar el concepte general d'entropia de la informació, fonamental en la sevateoria de la informació.\", \"ca[4]Inicialment, no sembla que Shannon tingués especial coneixement de l'estreta relació entre la seva nova mesura i el treball anterior en termodinàmica.\", \"caEl termeentropiava ser suggerit pel matemàticJohn von Neumannper la raó que aquesta noció s'assemblava a la que ja es coneixia com a entropia en física estadística, i podia afegir, per triomfar en qualsevol debat, que aquest terme s'havia entès malament.\", \"ca[5]El 1957,Edwin Thompson Jaynesdemostrà el vincle formal entre l'entropia macroscòpica introduïda per Clausius el 1847, la microscòpica introduïda per Gibbs i l'entropia matemàtica de Shannon.\", 'caAquest descobriment va ser descrit per Myron Tribus com una \"revolució passada desapercebuda\".', 'ca[6]Preàmbul[modifica]A principis de ladècada de 1940, les telecomunicacions estaven dominades pels sistemes analògics.', \"caLa transmissió de ràdio i televisió es basaven en modulacions contínues com la modulació d'amplitud (AM) i la modulació de freqüència (FM).\", \"caEls sons i les imatges es transformen en senyals elèctrics l'amplitud i / o freqüència els quals són funcions contínues, de vegades proporcionals, al senyal d'entrada.\", \"caEn el cas del so, es mesura amb un micròfon el fenomen de la pressió i la depressió que viatja a l'aire.\", \"caEn el cas de la televisió, la blancor de la imatge (la seva brillantor) és el principal senyal d'interès.\", 'caAquest procediment implica que el soroll afegit durant la transmissió produeix una degradació del senyal rebut.', \"caL'arquetip d'aquest tipus de soroll pren la forma de xerraire de ràdio i neu per a la televisió.\", \"caLa modulació analògica implica l'ús de nombres reals la dilatació decimal és infinita per representar informació (pressió sonora, intensitat de la llum, etc.).\", \"caUn soroll, per petit que sigui, tingui una conseqüència directa en el senyal.Els investigadors han admès així que una manera eficaç de protegir el soroll seria transformar el so i la imatge en nombres discrets, en lloc d'utilitzar nombres reals la precisió dels quals requereix un nombre infinit de dígits.\", 'caPer exemple, es podria utilitzar el número 0 per representar la negror total i el número 10 per a un blanc perfecte, amb tots els enters entre els dos que representen nivells successius de gris.', \"caSi 11 nivells no semblen suficients, podem utilitzar el mateix mètode per a una divisió d'intensitats tan grans com sigui necessari per satisfer l'ull.\", \"caEs pot fer un raonament similar per al so i arribem a un punt on és possible representar una pel·lícula i la seva banda sonora amb una quantitat limitada d'enters.La transmissió d'aquests enters provoca el que anomenem comunicació digital.\", 'caSi el soroll que parlem en el cas analògic es considera en una transmissió digital, es produiran errors quan aquest soroll és prou fort com per convertir un número en un altre.', 'caEn el cas analògic, fins i tot un petit soroll es converteix en errors perceptibles.', 'caEn digital, és poc probable que es produeixi un petit soroll per generar un error, però el soroll pot, però, fer-ho.', 'caEls investigadors han pensat que calia acceptar que la comunicació perfecta era impossible.', 'caÉs aquesta conjectura que va ser refutada per Shannon amb la seva teoria de la informació.', 'caVa demostrar que era possible transmetre informació sense errors utilitzant una estratègia de codificació digital mentre es consideri suficient una determinada velocitat de transmissió.', \"caPer error aquí, significa la capacitat del receptor per restaurar el missatge original fins i tot si el missatge rebut és modificat pel soroll.Entropia d'un text comú[modifica]Shannon proposa una manera senzilla de determinar l'entropia d'un text donat per a un receptor determinat:[7]A té el text i li demana a B que l'endevini lletra per lletra (espais inclosos).\", \"casi B endevina correctament la lletra, es compta 1 (perquè A, mentre respon «sí», transmet 1 bit d'informació).\", \"caSi B s'equivoca, se li dona la lletra correcta i es compta 4,75 (perquè un caràcter de 26 (és a dir, 27 - 1) representa 4.75 bits d'informació.El mètode aplicat als texts dels diaris i als lectors normals mostra que es pot endevinar una lletra de dues, la redundància del llenguatge corrent té, per tant, un factor de 2, i el contingut informatiu d'una lletra, en aquest context, és de tan sols 2,35 bits.Utilitat pràctica[modifica]L'entropia de Shannon s'utilitza en l'electrònica digital per a digitalitzar una font utilitzant el màxim nombre possible de bits sense perdre informació.\", 'caTambé quantifica el nombre mínim de bits en què es pot codificar un fitxer, mesurant així els límits que els algorismes de compressió sense pèrdua poden esperar aconseguir.', \"caTambé s'utilitza en altres camps, com, per exemple, per a seleccionar el millor punt de vista d'un objecte tridimensional.\", \"ca[8]L'entropia com a mesura de similitud[modifica]A més d'utilitzar la codificació d'entropia com a forma de comprimir dades digitals, també es pot utilitzar un codificador d'entropia per mesurar la quantitat de similitud entre fluxos de dades i classes de dades ja existents.\", \"caAixò es fa generant un codificador / compressor d'entropia per a cada classe de dades; Les dades desconegudes es classifiquen donant les dades sense comprimir a cada compressor i veient quin compressor produeix la compressió més alta.\", 'caEl codificador amb la millor compressió és probablement el codificador format sobre les dades que eren més similars a les dades desconegudes.', \"ca[8]Ús en aprenentatge automàtic[modifica]Les tècniques d'aprenentatge automàticapareixen molt sovint en estadística i en teoria de la informació.\", \"caEn general, l'entropia és una mesura de la incertesa i l'objectiu de l'aprenentatge automàtic és precisament el de minimitzar la incertesa.Els algorismes d'aprenentatge basat en arbres de decisióutilitzen l'entropia relativa per determinar les normes de decisió que governen les dades en cada node.\", \"ca[9]El guany d'informació en arbres de decisióIG(Y,X), que és igual a la diferència entre l'entropia deYi l'entropia condicional deYdonatX, quantifica l'esperança en informació, o la reducció d'entropia, de saber addicionalment el valor d'un atributX.\", \"caS'utilitza el guany d'informació per identificar quins atributs del conjunt de dades proporcionen més informació i per tant s'han d'utilitzar per dividir els nodes de l'arbre òptimament.Els models d'inferència bayesianasovint apliquen elprincipi de màxima entropiaper obtenir distribucions de probabilitata priori.\", \"ca[10]La idea és que la distribució que millor representa l'estat actual de coneixement d'un sistema és el que té més entropia, i que és per tant el més indicat a ser l'a priori.La classificació estadística, ja sigui feta ambregressió logísticao ambxarxes neuronals artificialssovint utilitza una funció de pèrdues estàndard, anomenada pèrdua d'entropia creuadaque minimitza l'entropia creuada mitjana entre la veritat i les distribucions predites.\", \"ca[11]En general, l'entropia creuada és una mesura de la diferència entre duos conjunts de dades amb divergència de Kullback-Liebler (o entropia relativa) similar.Referències[modifica]↑Shannon, Claude E.«A Mathematical Theory of Communication».Bell System Technical Journal, 27, 3, juliol 1948, pàg.\", 'ca379–423.DOI:10.1002/j.1538-7305.1948.tb01338.x.', 'ca(PDF, archived fromhere)↑Shannon, Claude E.«A Mathematical Theory of Communication».Bell System Technical Journal, 27, 4, octubre 1948, pàg.', 'ca623–656.DOI:10.1002/j.1538-7305.1948.tb00917.x.', 'ca(PDF, archived fromhere)↑Pathria, R. K.;Beale, Paul.Statistical Mechanics.', 'caThird.', 'caAcademic Press, 2011, p. 51.ISBN 978-0123821881.↑Claude E.ShannonA mathematical theory of communicationBell System Technical Journal, vol.', 'ca27, pp.', 'ca379-423 and 623-656, July and October, 1948↑M.', 'caTribus, E.C.', 'caMcIrvine, “Energy and information”, Scientific American, 224 (September 1971).↑La référence est dansce documentArxivat2006-09-07 aWayback Machine.', 'ca(PDF)↑El mesurament depèn de la \"cultura\" del receptor.', 'caUna frase com \"obtenim la següent sèrie ràpidament convergent\" proporcionarà una taxa d\\'èxit més gran per als matemàtics que per als no matemàtics.', \"caPassa el mateix, explica Brillouin, si s'utilitzen altres vocabularis molt especialitzats com mèdics, financers, polítics, etc.↑8,08,1(anglès)P.-P. Vàzquez, M. Feixas, M. Sbert, W. Heidrich,Viewpoint selection using viewpoint entropy, Proceedings of the Vision Modeling and Visualization Conference, 273-280, 2001.↑Batra, Mridula;Agrawal, Rashmi «Comparative Analysis of Decision Tree Algorithms» (en anglès).Nature Inspired Computing.\", 'caSpringer [Singapore], 652, 2018, pàg.', 'ca31–36.DOI:10.1007/978-981-10-6747-1_4.↑Jaynes, Edwin T. «Prior Probabilities».IEEE Transactions on Systems Science and Cybernetics, 4, 3, setembre 1968, pàg.', 'ca227–241.DOI:10.1109/TSSC.1968.300117.ISSN:2168-2887.↑Rubinstein, Reuven Y.;Kroese, Dirk P.The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning(en anglès).', 'caSpringer Science & Business Media, 2013-03-09.ISBN 978-1-4757-4321-0.Bibliografia[modifica]Cover, Thomas M.;Thomas, Joy A.Elements of Information Theory(en anglès).', 'caWiley, 1991-08-26.ISBN 978-0-471-06259-2.Fano, Robert M.Transmission of Information: A Statistical Theory of Communications(en anglès).', 'caM.I.T.', 'caPress, 1961.ISBN 978-0-262-06001-1.Feinstein, Amiel «A new basic theorem of information theory».Research Laboratory for Electronics - Technical Reports, 1954.MacKay, David J.', 'caC..Information Theory, Inference and Learning Algorithms(en anglès).', 'caCambridge University Press, 2003-09-25.ISBN 978-0-521-64298-9.Shannon, C. E. «A mathematical theory of communication».The Bell System Technical Journal, 27, 3, 1948-07, pàg.', 'ca379–423.', \"caArxivat de l'originalel 1998-01-31.DOI:10.1002/j.1538-7305.1948.tb01338.x.ISSN:0005-8580[Consulta: 15 març 2024].Wolfowitz, J.\", \"ca«The coding of messages subject to chance errors».Illinois Journal of Mathematics, 1, 4, 01-12-1957.DOI:10.1215/ijm/1255380682.ISSN:0019-2082.Vegeu també[modifica]Segon teorema de ShannonA Mathematical Theory of CommunicationEnllaços externs[modifica]AWikimedia Commonshi ha contingut multimèdia relatiu a:Entropia de ShannonUn applet de Java que representa l'Experiment de Shannon per calcular l'entropia de l'AnglèsRegistres d'autoritatBNE(1)BNF(1)GND(1)LCCN(1)NDL(1)NKC(1)Obtingut de «https://ca.wikipedia.org/w/index.php?title=Entropia_de_Shannon&oldid=33324747»Categories:ComunicacióProcessament de senyalsClaude ShannonCategories ocultes:Articles amb la plantilla Webarchive amb enllaç waybackPàgines amb enllaç commonscat des de WikidataControl d'autoritatsLa pàgina va ser modificada per darrera vegada el 2 abr 2024 a les 02:31.El text està disponible sota laLlicència de Creative Commons Reconeixement i Compartir-Igual; es poden aplicar termes addicionals.\", \"caVegeu lesCondicions d'ús.\", \"caWikipedia® (Viquipèdia™) és unamarca registradadeWikimedia Foundation, Inc.Política de privadesaQuant al projecte ViquipèdiaDescàrrec de responsabilitatCodi de conductaDesenvolupadorsEstadístiquesDeclaració de cookiesVersió per a mòbilsCommuta la limitació d'amplada del contingut\", 'faآنتروپی اطلاعات - ویکی\\u200cپدیا، دانشنامهٔ آزادپرش به محتوامنوی اصلیمنوی اصلیانتقال به نوار کنارینهفتنبازدید محتواصفحهٔ اصلیرویدادهای کنونیمقالهٔ تصادفیکمک مالیهمکاریتغییرات اخیرویکی\\u200cنویس شوید!راهنماتماس با ویکی\\u200cپدیاجستجوجستجوایجاد حسابورودابزارهای شخصیایجاد حسابورودصفحه\\u200cهایی برای ویرایشگرانی که از سامانه خارج شدندبیشتر بدانیدمشارکت\\u200cهابحثفهرستانتقال به نوار کنارینهفتنبخش آغازین۱مقدمه۲تعریفتغییر وضعیت زیربخش\\u200cهای تعریف۲.۱نظریه اندازه۳مثال۴توصیف صفات۵فشرده\\u200cسازی داده\\u200cها۶آنتروپی به عنوان معیاری از تنوع۷کاربرد در یادگیری ماشین۸جستارهای وابسته۹منابعتغییر وضعیت فهرست محتویاتآنتروپی اطلاعات۴۵ زبانAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語ویرایش پیوندهامقالهبحثفارسیخواندنویرایشنمایش تاریخچهابزارهاابزارهاانتقال به نوار کنارینهفتنعمل\\u200cهاخواندنویرایشنمایش تاریخچهعمومیپیوندها به این صفحهتغییرات مرتبطبارگذاری پروندهصفحه\\u200cهای ویژهپیوند پایداراطلاعات صفحهیادکرد این صفحهدریافت نشانی کوتاه\\u200cشدهدریافت کیوآر کدآیتم ویکی\\u200cدادهویرایش پیوندهای بین\\u200cزبانینسخه\\u200cبرداریبارگیری به\\u200cصورت PDFنسخهٔ قابل چاپدر پروژه\\u200cهای دیگرویکی\\u200cانباراز ویکی\\u200cپدیا، دانشنامهٔ آزادنظریه اطلاعاتمفاهیمآنتروپی اطلاعاتاطلاعات مشترکنرخ مخابرهظرفیت کانالچهره\\u200cهای مهمکلود شانونهری نایکویسترالف هارتلیتوماس کاوررابرت فانوریچارد همینگرابرت گالاگررادلف السودهآرون واینرجوایز مهمجایزه کلود شانوندرنظریه اطلاعات،آنتروپی(بهانگلیسی: Entropy) یا اِنتروپی،معیاری عددی برای اندازه\\u200c گرفتن اطلاعات، یا تصادفی\\u200c بودن یکمتغیر تصادفیاست.', 'faبه بیان دقیق\\u200cتر، آنتروپی یک متغیر تصادفی،متوسطاطلاعات آن است.', 'faبا داشتن یک متغیر تصادفی گسستهX، که مقادیری از الفبایX}}می\\u200cگیرد و از توزیعp:X→[0,1]}\\\\rightarrow [0,1]}پیروی می\\u200cکند، آنتروپی برای آن به صورت زیر تعریف می\\u200cشود:H(X):=−∑x∈Xp(x)log\\u2061p(x)=E[−log\\u2061p(x)]}}p(x)\\\\log p(x)=\\\\mathbb [-\\\\log p(x)]}هرچه آنتروپی یک متغیر تصادفی بیشتر باشد، ابهام ما درباره آن بیشتر است؛ به این معنی که پس از مشاهده\\u200cی آن، اطلاعات به\\u200cدست\\u200cآمده از آن بیشتر خواهد بود.آنتروپی یک منبع اطلاعات، حد پایین نرخ بهترینفشرده\\u200cسازی بی\\u200cاتلافداده\\u200cهای آن منبع است.اطلاعات حاصل از مشاهده یکرویداد تصادفی، برابر با منفیلگاریتماحتمال رخ دادن آن تعریف می\\u200cشود.', 'faیک تابع برای اندازه\\u200c گرفتن اطلاعات یک روی\\u200cداد تصادفی، ویژگی\\u200cهایی دارد:- این\\u200cکه اندازه\\u200cی اطلاعات، نامنفی باشد.- اطلاعات حاصل از مشاهدهٔ یک رویداد قطعی (یعنی با احتمال برابر با یک) صفر باشد.- و مهم\\u200cتر از همه این\\u200cکه، اطلاعات حاصل از دو مشاهدهٔ مستقل، برابر با جمع اطلاعات حاصل از مشاهدهٔ تک\\u200cتک آن\\u200cها باشد.می\\u200cتوان نشان داد تنها تابعی که این سه ویژگی را برمی\\u200cآورد، منفی لگاریتم احتمال است.', 'faاندازۀ اطلاعات با تابع لگاریتم در پایه\\u200cهای مختلف، با هم تنها در یک ضریب ثابت اختلاف دارد.', 'faمتداول\\u200cترین پایهٔ لگاریتم در محاسبهٔ اطلاعات، ۲ است که اطلاعات را در واحد بیت محاسبه می\\u200cکند.به\\u200cطور کلی در علوم و مهندسی، آنتروپی معیاری برای ابهام یا بی\\u200cنظمی است.کلود شانوندر مقالهٔ انقلابی خود با نام «A Mathematical Theory of Communication» در ۱۹۴۸، آنتروپی شانون را معرفی کرد و پایه\\u200cگذار نظریهٔ اطلاعات شد.', 'fa[۱]آنتروپی در نظریهٔ اطلاعات رابطهٔ تنگاتنگی با مفهومآنتروپیدرترمودینامیک آماریدارد.', 'faاین قیاس برخاسته از این است که مقادیر متغیر\\u200cهای تصادفی،انرژیریزحالت\\u200cهارا تعیین می\\u200cکنند و برای همین فرمول گیبز برای آنتروپی به صورت صوری دقیقاً مانند فرمول شانون است.', 'faآنتروپی در سایر بخش\\u200cهای ریاضی همچونترکیبیاتویادگیری ماشیننیز دارای اهمیت است.آنتروپی نتیجهٔ انداختن دو سکهٔ سالم برابر با ۲ بیت است.', 'faهر کدام از چهار حالت ممکن ۰٫۲۵ احتمال دارد.', 'faاطلاعات حاصل از هر مشاهده برابر با−log2(0.25)=2()=2}و میانگین اطلاعات حالت\\u200cهای ممکن برابر با ۲ بیت است.مقدمه[ویرایش]ایدهٔ\\u200c اصلینظریه اطلاعاتاین است که «ارزش اطلاعاتی» منتقل شده از طریق یک پیام به میزان غافلگیر کننده بودن این پیام بستگی دارد.', 'faاگر یک رویداد بسیار محتمل رخ بدهد، پیام، اطلاعات بسیار کمی را منتقل می\\u200cکند.', 'faدر عین حال اگر یک رویداد بسیار غیر محتمل رخ دهد، پیام،\\u200c اطلاعات آگاه\\u200cکننده\\u200cتری را منتقل می\\u200cکند.', 'faبرای نمونه، دانش اینکه عددی خاص، عدد برندهٔ یکبخت\\u200cآزمایینیست، اطلاع بسیار کمی در اختیار ما قرار می\\u200cدهد چرا که هر عدد خاص انتخابی به احتمال زیاد برنده نخواهد شد.', 'faولی دانش اینکه عددی خاص برندهٔ بخت\\u200cآزمایی خواهد بود، ارزش اطلاعاتی زیادی دارد چراکه پیام آن رخداد یک پیامد بسیاد نامحتمل است.محتوای اطلاعاتی یک رویدادE، تابعی است که با کاهش احتمال آن رویدادp(E)افزایش می\\u200cیابد.', 'faهنگامی کهp(E)به یک نزدیک می\\u200cشود، شگفتی رویداد کم است چرا که انتظار رخداد آن را داریم و هنگامی کهp(E)به صفر نزدیک می\\u200cشود، شگفتی رویداد زیاد است چرا که انتظار رخداد آن رویداد را نداریم.', 'faاین رابطه توسط رابطهٔ زیر مشروح است:log\\u2061(1p(E))}}که در این رابطه\\u200clogیا همانلگاریتمهنگامی که احتمال رویداد برابر با یک است، میزان شگفتی را صفر می\\u200cکند.', 'fa[۲]در اصل،logتنها تابعی است که این مجموعه از توصیف\\u200cها را ارضا می\\u200cکند.', 'faبنابراین می\\u200cتوان میزان اطلاع یا شگفتی رویدادEبرابر است با:I(E)=−log2\\u2061(p(E))(p(E))}که برابر با عبارت زیر است:I(E)=log2\\u2061(1p(E))}}آنتروپی، مقدار مورد انتظار (میانگین) اطلاعات منتقل شده با تشخیص خروجی یکآزمایش تصادفیرا به ما می\\u200cدهد.', 'fa[۳]تعریف[ویرایش]آنتروپی متغیر تصادفی گسستهٔXبا تابع چگالی احتمالP(X)را باH(X) (X)}نمایش می\\u200cدهند که این\\u200cگونه تعریف می\\u200cشود:H(X)=E[I(X)]=E[−logb\\u2061(P(X))].', 'fa(X)=\\\\mathbb [\\\\mathrm (X)]=\\\\mathbb [-\\\\log _(\\\\mathrm (X))].', 'fa}در رابطهٔ بالاE[⋅] [\\\\cdot ]}تابع امید ریاضی وI(⋅) (\\\\cdot )}تابع میزان اطلاعات رویداد است.I(X) (X)}تابعی از یک متغیر تصادفی، و در نتیجه یک متغیر تصادفی است.bپایهٔ لگاریتم است و آنتروپی را با واحدهای متفاوت به دست می\\u200cدهد.', 'faمتداول\\u200cترینb،۲، e، و ۱۰ هستند که به ترتیب آنتروپی را در واحدهای بیت و nat و hartley به دست می\\u200cدهد.می\\u200cتوان آنتروپیXرا به صورت باز هم نوشت:H(X)=∑i=1nP(xi)I(xi)=−∑i=1nP(xi)logb\\u2061P(xi).', 'fa(X)=\\\\sum _^ (x_)\\\\,\\\\mathrm (x_)}=-\\\\sum _^ (x_)\\\\log _\\\\mathrm (x_)}.', 'fa}همچنین،I(0)=0×log(0) (0)=0\\\\times log(0)}را صفر تعریف می\\u200cکنیم که با مشاهدهٔlimp→0+plog\\u2061(p)=0p\\\\log(p)=0}نیز سازگار است.آنتروپی متغیر تصادفیXبه شرطYباتوزیع احتمال مشترکP(X,Y)نیز به صورت زیر تعریف می\\u200cشود:H(X|Y)=−∑i,jP(xi,yj)log\\u2061P(xi,yj)P(yj) (X|Y)=-\\\\sum _P(x_,y_)\\\\log ,y_)})}}}H(X|Y) (X|Y)}میانگین اطلاعات حاصل از مشاهدهٔXبه شرط اطلاع ازYرا نشان می\\u200cدهد.نظریه اندازه[ویرایش]آنتروپی را می\\u200cتوان به صورت صوری در زباننظریهٔ اندازهبه صورت روبه\\u200cرو تعریف کرد:[۴]اگر(X,Σ,μ)یکفضای احتمالاتیباشد وپیشامدA∈Σرا داشته باشیم، مقدار شگفتیAبرابر است با:σμ(A)=−ln\\u2061μ(A)(A)=-\\\\ln \\\\mu (A)}مقدار امید شگفتیAبرابر است با:hμ(A)=μ(A)σμ(A)(A)=\\\\mu (A)\\\\sigma _(A)}یکافرازalmost-μخانواده\\u200cای از مجموعه\\u200cهاP⊆P(X)}(X)}است به گونه\\u200cای کهμ(∪P)=1وμ(A∩B)=0برای هرA,B∈Pمتمایز.', 'fa(این یک سست\\u200cسازی از شروط همیشگی برای افراز است.)', 'faآنتروپیPبرابر است با:Hμ(P)=∑A∈Phμ(A)(P)=\\\\sum _h_(A)}اگرMیکجبر سیگمابر رویXباشد، آنتروپیMبرابر است با:Hμ(M)=supP⊆MHμ(P)(M)=\\\\sup _H_(P)}در نهایت، آنتروپیفضای احتمالاتیبرابر است باHμ(Σ)(\\\\Sigma )}، یعنی آنتروپی نسبت بهμهمهٔ جبر سیگمای همهٔ زیرمجموعه\\u200cهای قابل اندازه\\u200cگیریX.مثال[ویرایش]مقالهٔ اصلی:فرآیند برنولینمودار آنتروپی نتیجهٔ پرتاب یک سکه در واحد بیت بر حسب احتمال شیر آمدن آن.', 'faهر چقدر احتمال شیر آمدن سکه به ۰٫۵ نزدیکتر باشد ابهام در مورد نتیجهٔ آن بیشتر است و اطلاع از نتیجه، به\\u200cطور میانگین اطلاعات بیشتری دربردارد.متغیر تصادفیX، نتیجهٔ پرتاب یک سکه با احتمال شیرpو خط1−pاست.', 'faهرچقدرpبه12نزدیکتر باشد، ابهام در مورد نتیجهٔ پرتاب بیشتر است و به همین ترتیب اطلاع از نتیجهٔ پرتاب به\\u200cطور میانگین، اطلاعات بیشتری دربردارد.', 'faدر واقع بیش\\u200cترین آنتروپی برایp=12}و برابر با ۱ بیت است.H(X)=−∑i=1nP(xi)log2\\u2061P(xi)=−∑i=1212log2\\u2061(12)=1, (X)=-\\\\sum _^ (x_)\\\\log _\\\\mathrm (x_)}=-\\\\sum _^\\\\log _()}=1,}وقتیpصفر یا یک باشد، هیچ ابهامی درباره نتیجهٔ پرتاب نیست و به همین ترتیب اطلاع از نتیجهٔ پرتاب هیچ اطلاعاتی در برندارد.H(X)=−(0log2\\u2061(0)+1log2\\u2061(1))=0.', 'fa(X)=-\\\\left(\\\\log _()}+\\\\log _()}\\\\right)=0.', 'fa}برایp=14}انتظار داریم آنتروپی کمتر از مورد یکنواخت و بیشتر از مورد بی\\u200cابهام باشد.H(X)=−(14log2\\u2061(14)+34log2\\u2061(34))≈0.81 (X)=-\\\\left(\\\\log _()}+\\\\log _()}\\\\right)\\\\approx 0.81}به\\u200cطور کلی، توزیع یکنواخت، بیشترین آنتروپی، و یک رویداد قطعی، کمترین آنتروپی را دارا هستند.توصیف صفات[ویرایش]برای درک مفهوم−∑pilog\\u2061(pi)\\\\log(p_)}، ابتدا یک تابع اطلاعاتIبرای رویدادiام با احتمالpi}تعریف می\\u200cکنیم.', 'faمقدار اطلاعات بدست آمده از مشاهده پدیدهٔiاز ویژگی\\u200cهای بنیادین اطلاعات شانون پیروی می\\u200cکند:[۵]I(p)به صورتیکنوادرpکاهش می\\u200cیابد: افزایش در احتمال یک رویداد، اطلاعات حاصل از مشاهدهٔ آن را کاهش می\\u200cدهد و بلعکس.I(1)=0: رویدادهایی که همیشه رخ می\\u200cدهند، هیچ اطلاعاتی را منتقل نمی\\u200cکنند.I(p1⋅p2)=I(p1)+I(p2)\\\\cdot p_)=I(p_)+I(p_)}: اطلاعات آموخته شده ازرویداد\\u200cهای مستقلبرابر است با جمع اطلاعات بدست آمده از هر رویداد.با فرض داشتن دو رویداد مستقل، اگر رویداد اولnپیامد هم\\u200cشانس و دیگریmپیامد هم\\u200cشانس داشته باشد، در این صورتnmپیامد هم\\u200cشانس برای رویداد توأم آن\\u200cها وجود دارد.', 'faاین بدان معناست که اگر برای رمزگذاری مقدار اولlog2\\u2061(n)(n)}بیت و برای رمزگذاری مقدار دوم بهlog2\\u2061(m)(m)}بیت نیاز داشته باشیم، برای رمزگذاری هر دوی آن\\u200cها بهlog2\\u2061(nm)=log2\\u2061(n)+log2\\u2061(m)(nm)=\\\\log _(n)+\\\\log _(m)}بیت نیاز داریم.شانون کشف کرد که یک انتخاب مناسب برایIبه صورت زیر است:[۶]I(p)=log\\u2061(1p)=−log\\u2061(p)})=-\\\\log(p)}در واقع تنها مقادیر ممکن برایIبه فرمI(u)=klog\\u2061(u)به ازای مقادیر منفی برایkمی\\u200cباشند.', 'faهمچنین گزینش یک مقدار برایk، هم\\u200cارز با گزینش مقدارx>1برایk=−1log\\u2061(x)}}است که در این صورت می\\u200cتوان مقدار پایهٔ لگاریتم را به کمکxتغییر داد.', 'faبنابرین آنتروپی با ویژگی\\u200cهای فوق توصیف می\\u200cشود.فشرده\\u200cسازی داده\\u200cها[ویرایش]مقاله\\u200cهای اصلی:قضیه کدگذاری منبع شانونوفشرده\\u200cسازی داده\\u200cهاآنتروپی یک منبع اطلاعات، حد پایین متوسط بهترین نرخ فشرده\\u200cسازی بدون اتلاف داده\\u200cهای آن منبع است.', 'faبه بیان دقیق\\u200cتر هیچ روش فشرده\\u200cسازی ای وجود ندارد کهبه\\u200cطور میانگینمقدار متغیر تصادفیXرا با کمتر ازH(X) (X)}بیت فشرده کند.', 'faاین حد پایین بسیار قوی است، به\\u200cطوری که برای دنباله\\u200cهای به طولnاز داده\\u200cهای هر منبع تصادفیX، یک روش فشرده\\u200cسازی وجود دارد که به\\u200cطور میانگین، نتیجه هر مشاهده را حداکثر باH(X)+1n (X)+}بیت فشرده می\\u200cکند.آنتروپی به عنوان معیاری از تنوع[ویرایش]مقالهٔ اصلی:تنوع زیستیآنتروپی یکی از راه\\u200cهای متعدد سنجشتنوع زیستیاست و از آن به صورتشاخص شانوناستفاده می\\u200cشود.', 'fa[۷]شاخص تنوع یک معیار کمی آماری برای بررسی انواع گوناگون موجود در یک مجموعهٔ داده است.کاربرد در یادگیری ماشین[ویرایش]روش\\u200cهاییادگیری ماشینبه طور عمده مبتنی برآمارو همچنیننظریه\\u200cٔ اطلاعاتاست.', 'faبه طور کلی، آنتروپی یک معیار برایعدم قطعیتاست و هدف یادگیری ماشین کاهش عدم قطعیت است.الگوریتم\\u200cهاییادگیری درخت تصمیمازآنتروپی نسبیاستفاده می\\u200cکنند تا قوانین تصمیم\\u200cگیری حاکم بر داده\\u200cها در هر گره را پیدا کند.', 'fa[۸]کسب اطلاعات در درخت\\u200cهای تصمیمIG(Y,X)، که برابر است با تفاوت آنتروپیYو آنتروپی شرطیYبه شرطX، اطلاع مورد انتظار را کمیت دهی می\\u200cکند.مدل\\u200cهایاستنباط بیزیاغلب با استفاده ازاصل حداکثر آنتروپی،توزیع احتمال پیشینرا بدست می\\u200cآورند.', 'fa[۹]منطق این روش این است که توزیعی که بهترین بیان از دانش ما از حالت کنونی یک سامانه را دارد، همانی است که بیشترین آنتروپی را دارد بنابراین برای توزیع پیشین بودن مناسب است.طبقه\\u200cبندی در یادگیری ماشینکه توسطرگرسیون لجستیکیاشبکه\\u200cهای عصبی مصنوعیپیاده\\u200cسازی می\\u200cشود، اغلب از از یکتابع زیاناستاندارد، به نام زیانآنتروپی متقاطع، استفاده می\\u200cکند که میانگین آنتروپی متقاطع بین واقعیت و توزیع\\u200cهای پیش\\u200cبینی شده را کمینه می\\u200cکند.', 'fa[۱۰]به طور کلی، آنتروپی متقاطع یک معیار برای محاسبهٔ تفاوت میان ۲ مجموعهٔ داده\\u200cها است، مانندواگرایی کولبک-لیبلریا همانآنتروپی نسبی.جستارهای وابسته[ویرایش]آنتروپی (ترمودینامیک)آنتروپی متقاطعآنتروپی تفاضلیآنتروپی شرطیآنتروپی (پیکان زمان)کدگذاری آنتروپیاطلاع فیشرفاصلهٔ همینگتاریخچهٔ آنتروپیفاصلهٔ لون\\u200cاشتایناطلاعات مقابلسرگشتگیاعداد تصادفیشاخص شانونمنابع[ویرایش]↑Shannon, C. E.', 'fa(1948-10).', 'faA mathematical theory of communication.The Bell System Technical Journal.27(4): 623–656.doi:10.1002/j.1538-7305.1948.tb00917.x.ISSN0005-8580.', 'fa}:Check date values in:|date=(help)↑Entropy (for data science) Clearly Explained!!', 'fa!, retrieved2022-12-19↑«David MacKay: Information Theory, Inference, and Learning Algorithms: The Book».www.inference.org.uk.', 'faدریافت\\u200cشده در۲۰۲۲-۱۲-۱۹.↑EntropyinnLab↑Carter، Tom (مارس ۲۰۱۴).An introduction to information theory and entropy[مقدمه\\u200cای بر نظریهٔ اطلاعات و آنتروپی](PDF).↑Chakrabarti, C. G., and Indranil Chakrabarty.', 'faShannon entropy: axiomatic characterization and application.', 'faInternational Journal of Mathematics and Mathematical Sciences 2005.17 (2005): 2847-2854url↑Spellerberg, Ian F.; Fedor, Peter J.', 'fa(2003-05).', \"faA tribute to Claude Shannon (1916-2001) and a plea for more rigorous use of species richness, species diversity and the 'Shannon-Wiener' Index: On species richness and diversity.Global Ecology and Biogeography(به انگلیسی).12(3): 177–179.doi:10.1046/j.1466-822X.2003.00015.x.\", 'fa}:Check date values in:|date=(help)↑Batra, Mridula; Agrawal, Rashmi (2018).', 'faPanigrahi, Bijaya Ketan; Hoda, M. N.; Sharma, Vinod; Goel, Shivendra (eds.).', 'faComparative Analysis of Decision Tree Algorithms.Nature Inspired Computing(به انگلیسی).', 'faSingapore: Springer: 31–36.doi:10.1007/978-981-10-6747-1_4.ISBN978-981-10-6747-1.↑Jaynes, Edwin T.', 'fa(1968-09).', 'faPrior Probabilities.IEEE Transactions on Systems Science and Cybernetics.4(3): 227–241.doi:10.1109/TSSC.1968.300117.ISSN2168-2887.', 'fa}:Check date values in:|date=(help)↑\"The Cross‐Entropy Method: A Unified Approach to Combinatorial Optimisation, Monte‐Carlo Simulation and Machine Learning\".Kybernetes.34(6): 903–903.', 'fa2005-07-01.doi:10.1108/03684920510595562.ISSN0368-492X.Elements of Information Theory(انگلیسی)داده\\u200cهای کتابخانه\\u200cایکتابخانه\\u200cهای ملیاسپانیافرانسه(داده\\u200cها)آلماناسرائیلایالات متحده آمریکاژاپنجمهوری چکسایرکاربرد چندوجهی اصطلاحات موضوعینبوروش\\u200cهایفشرده\\u200cسازی داده\\u200cهابی\\u200cاتلافنظریهآنتروپی•پیچیدگی کولموگروف•افزونگی (نظریه اطلاعات)•با اتلافکدگذاری آنتروپیرمزگذاری شانون-فانو•Shannon–Fano–Elias•هافمن•کدگذاری هافمن انطباقی•Arithmetic•Range•Golomb•Universal(Gamma•Exp-Golomb•Fibonacci•Levenshtein)واژه\\u200cنامه\\u200eکدبندی طول اجرا•Byte pair encoding•DEFLATE•Lempel–Ziv(LZ77/78•LZSS•LZW•LZWL•LZO•LZMA•LZX•LZRW•LZJB•LZS•LZT•ROLZ)\\u200eغیرهCTW•تبدیل باروز-ویلر•PPM•DMC•DeltaصدانظریهCompanding•کانولوشن•دامنه دینامیک•Latency•نمونه\\u200cبرداری•قضیه نمونه\\u200cبرداری نایکوئیست-شنون•Sound qualityAudio codecpartsLPC(LAR•LSP) •WLPC•CELP•ACELP•A-law•μ-law•ADPCM•DPCM•MDCT•تبدیل فوریه•Psychoacoustic modelغیرهنرخ بیت(CBR•ABR•سرعت بیت متغیر) •Speech compression•Sub-band codingتصویرلغاتفضای رنگ•پیکسل•Chroma subsampling•Compression artifact•وضوح تصویریروش\\u200cهاکدبندی طول اجرا•فشرده\\u200cسازی برخالی•Wavelet•EZW•SPIHT•LP•تبدیل کسینوسی گسسته•Chain code•تئوری کارانن-لوفغیرهTest images•PSNR quality measure•Quantizationویدیولغاتویدئو•فریم (فیلم)•فریم ریت•Interlace•Frame types•Video quality•وضوح صفحه نمایشVideo codec partsMotion compensation•تبدیل کسینوسی گسسته•کمیت (پردازش سیگنال)غیرهVideo codecs•نظریه نرخ-اعوجاج•نرخ بیت(CBR•ABR•سرعت بیت متغیر)Timeline of information theory, data compression, and error-correcting codesهمچنینقالب\\u200cهای فشرده\\u200cسازیبرای قالب\\u200cها ونرم\\u200cافزارهای پیاده\\u200cسازی\\u200cشدهٔ فشرده\\u200cسازیبرای کدک\\u200cها را ببینید.برگرفته از «https://fa.wikipedia.org/w/index.php?title=آنتروپی_اطلاعات&oldid=38481552»رده\\u200cها:آنتروپی اطلاعاتتصادفیتصادفی آمارینظریه آمارینظریه اطلاعاتنظریه سامانه\\u200cهای پیچیدهفشرده\\u200cسازی داده\\u200cهارده\\u200cهای پنهان:خطاهای یادکرد: تاریخCS1: بازه مختصر سالیادکردهای دارای منبع به زبان انگلیسیمقاله\\u200cهای ویکی\\u200cپدیا همراه شناسه\\u200cهای BNEمقاله\\u200cهای ویکی\\u200cپدیا همراه شناسه\\u200cهای BNFمقاله\\u200cهای ویکی\\u200cپدیا همراه شناسه\\u200cهای GNDمقاله\\u200cهای ویکی\\u200cپدیا همراه شناسه\\u200cهای J9Uمقاله\\u200cهای ویکی\\u200cپدیا همراه شناسه\\u200cهای LCCNمقاله\\u200cهای ویکی\\u200cپدیا همراه شناسه\\u200cهای NDLمقاله\\u200cهای ویکی\\u200cپدیا همراه شناسه\\u200cهای NKCمقاله\\u200cهای ویکی\\u200cپدیا همراه شناسه\\u200cهای FASTاین صفحه آخرین\\u200cبار در \\u200f۲۳ دسامبر ۲۰۲۳ ساعت \\u200f۱۰:۰۶ ویرایش شده است.همهٔ نوشته\\u200cها تحتمجوز Creative Commons Attribution/Share-Alikeدر دسترس است؛ برای جزئیات بیشترشرایط استفادهرا بخوانید.ویکی\\u200cپدیا® علامتی تجاری متعلق به سازمان غیرانتفاعیبنیاد ویکی\\u200cمدیااست.سیاست حفظ حریم خصوصیدربارهٔ ویکی\\u200cپدیاتکذیب\\u200cنامه\\u200cهاآیین\\u200cنامه رفتاریتوسعه\\u200cدهندگانآماربیانیهٔ کوکینمای موبایلToggle limited content width', 'huShannon-entrópiafüggvény – WikipédiaUgrás a tartalomhozFőmenüFőmenüáthelyezés az oldalsávbaelrejtésNavigációKezdőlapTartalomKiemelt szócikkekFriss változtatásokLap találomraTudakozóRészvételKezdőknekSegítségKözösségi portálKapcsolatfelvételAdományokKeresésKeresésFiók létrehozásaBejelentkezésSzemélyes eszközökFiók létrehozásaBejelentkezésLapok kijelentkezett szerkesztőknektovábbi információkKözreműködésekVitalap[elrejt]Április 27-én szombatonszemélyes találkozóttartunk Budapesten, ahol téged is várunk!Tartalomjegyzékáthelyezés az oldalsávbaelrejtésBevezető1Definíció2A Shannon-képlet intuitív levezetése3Története4Axiomatikus definíciók5Jegyzetek6Források7További információkTartalomjegyzék kinyitása/becsukásaShannon-entrópiafüggvény45 nyelvEnglishAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEspañolEuskaraفارسیFrançaisGalegoעבריתBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Hivatkozások szerkesztéseSzócikkVitalapmagyarOlvasásSzerkesztésLaptörténetEszközökEszközökáthelyezés az oldalsávbaelrejtésMűveletekOlvasásSzerkesztésLaptörténetÁltalánosMi hivatkozik erre?Kapcsolódó változtatásokSpeciális lapokHivatkozás erre a változatraLapinformációkHogyan hivatkozz erre a lapra?Rövidített URL készítéseQR-kód letöltéseWikidata-adatlapNyomtatás/\\u200bexportálásKönyv készítéseLetöltés PDF-kéntNyomtatható változatTárslapokWikimédia CommonsA Wikipédiából, a szabad enciklopédiábólEz aközzétett változat,ellenőrizve:2023. augusztus 13.PontosságellenőrzöttEz a szócikk vagy szakaszlektorálásra, tartalmi javításokra szorul.A felmerült kifogásokata szócikk vitalapjarészletezi (vagy extrém esetben a szócikk szövegében elhelyezett, kikommentelt szövegrészek).Ha nincs indoklás a vitalapon (vagy szerkesztési módban a szövegközben), bátran távolítsd el a sablont!Csak akkor tedd a lap tetejére ezt a sablont, ha az egész cikk megszövegezése hibás.', 'huHa nem, az adott szakaszba tedd, így segítve a lektorok munkáját!', 'hu(2005 júniusából)AShannon-féle entrópiafüggvényClaude Shannonamerikaimatematikuséshíradástechnikaiszakember munkája.', 'huAz elméletet a negyvenes évek legvégén azinformációnevű fogalomfizikai mennyiséggéésmérhetővétételére született.', 'huMa ez a fogalom és a tanulmányozása azinformációelméletegyik alapja.Definíció[szerkesztés]Afüggvénydefiniáláshoz feltételezzük, hogy egykommunikációsfolyamatban veszünk részt, melynek csatornáján azXhalmaz jeleiből álló véges sorozatok, üzenetek áramlanak.', 'huHa sok üzenet áll rendelkezésre, mérni vagy becsülni tudjuk azt ap(x)valószínűséget, hogy adottx∈Xelem milyen gyakran fordulhat elő egy üzenetben, továbbá, hogy valahogy mérhető vagy meghatározható azxjel információtartalma is, amit azI(x)jelöl.Legyen egy üzenet azx=(x1,x2,...,xj)∈Xj,x_,...,x_)\\\\in X^}jeleksorozata.Ekkor az üzenet információtartalma Shannon definíciója szerintH(x)=H(x1,x2,...,xj)=p(x1)I(x1)+p(x2)I(x2)+...+p(xj)I(xj)=p(x1)log2p(x1)−1+p(x2)log2p(x2)−1+...+p(xj)log2p(xj)−1,x_,...,x_)=p(x_)I(x_)+p(x_)I(x_)+...+p(x_)I(x_)=p(x_)log_p(x_)^+p(x_)log_p(x_)^+...+p(x_)log_p(x_)^}.Tehát azxüzenet információtartalma jeleiI(x):=p(x)log2[p(x)−1][p(x)^]}„egyedi információtartalmának” várható értéke.A Shannon-képlet intuitív levezetése[szerkesztés]Shannon egyvéges sokjelbőlálló (véges ábécé feletti) üzenet információértékét az üzenet jeleinek mint a jelre jellemzővalószínűséggelbekövetkező események információtartalmának „átlagos”, azazvárható értékekénthatározta meg:H(x1,x2,...,xj)=∑i=1jp(xi)I(xi),x_,...,x_\\\\right)=\\\\sum _^p\\\\left(x_\\\\right)I\\\\left(x_\\\\right)}.Ez még nem elégséges a definícióhoz, pontosan azért, mert az I(x) „adott jelhez tartozó információtartalom” mennyiséget nem definiáltuk.', 'huEnnek definiálásához Shannon további két feltételezéssel élt:Kisebb valószínűségű üzenetnek (és így jelnek) az információtartalma nagyobb.', 'huHiszen ha olyan eseményről szerzünk tudomást, amely átlagos, gyakori, azaz nagy valószínűségű; annak nincs nagy jelentősége; ellenben ritkább eseményre kevésbé vagyunk felkészülve, tehát ha ilyen következik be, az fontosabb.1/2 valószínűségű esemény információtartalma 1 legyen (egységválasztás, azaznormálás).Független események együttesének mint eseménynek az információtartalma a két esemény információtartalmának összege legyen (additivitás).Matematikailag bebizonyítható , hogy a következő képlet adja meg, ha a fenti normális(?)', 'hufeltételezésekkel élünk:H(X_)}\\\\right)}=H(x1,x2,...,xn),x_,...,x_\\\\right)}:=∑i=1np(xi)log2\\u20611p(xi)^p(x_)\\\\log _\\\\right)}}}A fenti H függvény néhány jellemzője:p(x)≤p(y)⇒H(x)≥H(y) )\\\\leq p(\\\\mathbf )\\\\Rightarrow H(\\\\mathbf )\\\\geq H(\\\\mathbf )}p(x)p(y)=p(xy)⇒H(xy)=H(x)+H(y) )p(\\\\mathbf )=p(\\\\mathbf )\\\\Rightarrow H(\\\\mathbf )=H(\\\\mathbf )+H(\\\\mathbf )}p(x)=12⇒H(x)=1 )=}\\\\Rightarrow H(\\\\mathbf )=1}Története[szerkesztés]Korábban (1928)R. V. L. Hartleymár bevezetett egy információfogalmat (Hartley-információ).', 'huSzerinte egy n elemű véges X halmaz valamely elemének azonosításához, ha elvileg bármelyik lehet közülük a keresett elem,I(n) = log2|X| = log2(n)mértékű információra van szükség.', 'huE definíció szerint bármely elem (kiválasztása) azonos információtartalmat hordoz.A fenti definíció azért bírálható, mert az információ értéke nem függ attól, hogy az egyes elemeket milyen valószínűséggel kívánjuk kiválasztani.', 'huShannon vezetett be egy konkrét problémából, egy hírközlési csatorna kapacitásának definíciójának szükségességéből kiindulva egy olyan információdefiníciót, amely a fenti problémára tekintettel van („The Mathematical Theory of Communication”c.', 'hucikkeiben, mely aBell System Technical Journalc.', 'hulapban jelent meg1948júliusában és októberében).Neumann Jánosjavaslatára nevezte el saját információmérő függvényétentrópiafüggvénynek, noha az elnevezés nemcsak a fizikai entrópiával kapcsolatos sok hasonlóságra mutat rá, de attól való különbségei miatt rengeteg kritika is érte (az ilyenek egy lehetséges motivációjáról és alternatív definícióról ld.', 'hupéldául T. Stonier:Információ és az Univerzum belső szerkezete).Később – elsősorbanszovjet– kutatók, mint Fagyejev, Hincsin, axiomatikusan is felépítették.Axiomatikus definíciók[szerkesztés]A H függvény tulajdonságai a következők:1.∀i∈:H(a1,...,ai−1,xi,ai+1)∈C[−∞,+∞]:\\\\ H(a_,...,a_,\\\\mathbf _,a_)\\\\in C[-\\\\infty ,+\\\\infty ]}2.∀i,j∈:i≤j⇒:\\\\ i\\\\leq j\\\\Rightarrow }⇒H(x1,...,xi−1,xi,xi+1,...,xj−1,xj,xj+1,...,xn)=H(x1,...,xi−1,xj,xi+1,...,xj−1,xi,xj+1,...,xn),...,x_,x_,x_,...,x_,x_,x_,...,x_)=H(x_,...,x_,x_,x_,...,x_,x_,x_,...,x_)}3.max=H(1n,1n,...,1n)})\\\\ |\\\\ }\\\\in \\\\mathbb ^\\\\right\\\\}=H\\\\left(},},...,}\\\\right)}4.H(x1,...,xn,0)=H(x1,...,xn),...,x_,0)=H(x_,...,x_\\\\right)}5.xn=∑k=1myk⇒H(x1,...,xn−1,y1,...,ym)=H(x1,...,xn−1)+p(xn)H(y1,...,ym)=\\\\sum _^y_\\\\Rightarrow H(x_,...,x_,y_,...,y_)=H(x_,...,x_)+p(x_)H(y_,...,y_)}6.H(12,12)=1},}\\\\right)=1}Fagyejev belátta, hogy 1.-2.-5.-6.-ból következik, hogy H-t a Shannon-képlet adja meg;Hincsin belátta, hogy 1.-3.-4.-6.-ból is következik ugyanez.Jegyzetek[szerkesztés]Források[szerkesztés]Csiszár Imre – Fritz József:Információelmélet.ELTEjegyzet.', 'huNemzeti Tankönyvkiadó, Bp., 1995.Csiszár Villő:Információelmélet.Előadássorozat, ELTE, 2003.Shigeru Furuichi, Flavia-Corina Mitroi-Symeonidis, Eleutherius Symeonidis, On some properties of Tsallis hypoentropies and hypodivergences, Entropy, 16(10) (2014), 5377-5399; DOI:10.3390/e16105377Shigeru Furuichi, Flavia-Corina Mitroi, Mathematical inequalities for some divergences, Physica A 391 (2012), pp.', 'hu388-400, DOI:10.1016/j.physa.2011.07.052;ISSN0378-4371Shigeru Furuichi, Nicuşor Minculete, Flavia-Corina Mitroi, Some inequalities on generalized entropies, J. Inequal.', 'huAppl., 2012, 2012:226.', 'huDOI: 10.1186/1029-242X-2012-226Claude Shannon:„The Mathematical Theory of Communication”;Bell System Technical Journal, 1948. július-októberTovábbi információk[szerkesztés]Alice és Bob - 2. rész: Alice és Bob számítógépezikInformatikai portál• összefoglaló, színes tartalomajánló lapA lap eredeti címe: „https://hu.wikipedia.org/w/index.php?title=Shannon-entrópiafüggvény&oldid=26362636”Kategória:FüggvényekInformatikaRejtett kategóriák:Az összes lektorálandó lapLektorálandó lapok 2005 júniusábólA lap utolsó módosítása: 2023. augusztus 13., 18:38A lap szövegeCreative Commons Nevezd meg!', 'hu– Így add tovább!', 'hu4.0licenc alatt van; egyes esetekben más módon is felhasználható.', 'huRészletekért lásd afelhasználási feltételeket.Adatvédelmi irányelvekA WikipédiárólJogi nyilatkozatCode of ConductFejlesztőkStatisztikákSütinyilatkozatMobil nézetKorlátozott tartalomszélesség ki/be', \"itEntropia (teoria dell'informazione) - WikipediaEntropia (teoria dell'informazione)Da Wikipedia, l'enciclopedia libera.Vai alla navigazioneVai alla ricercaVoce principale:Teoria dell'informazione#Entropia.Questa voce o sezione sull'argomento informatica è priva o carente dinoteeriferimenti bibliografici puntuali.Sebbene vi siano unabibliografiae/o deicollegamenti esterni, manca la contestualizzazione delle fonti connote a piè di paginao altri riferimenti precisi che indichino puntualmente la provenienza delle informazioni.\", 'itPuoimigliorare questa vocecitando le fontipiù precisamente.', \"itSegui i suggerimenti delprogetto di riferimento.Nellateoria dell'informazionel'entropiaè una misura della quantità diinformazionecontenuta in unmessaggiotrasferito attraverso uncanale di comunicazione.\", \"it[1]L'unità di misuratipica di questagrandezzaè ilBit.\", \"it[2]Indice1Storia2Definizione2.1Informazione intrinseca2.2Entropia di una sorgente di informazione2.3Entropia congiunta2.4Entropia condizionale2.5Informazione mutua2.6Entropia relativa3Legame con l'entropia termodinamica4Grandezze associate4.1Efficienza di un alfabeto5Esempi6Note7Bibliografia8Voci correlate9Altri progetti10Collegamenti esterniStoria[modifica|modifica wikitesto]Lo stesso argomento in dettaglio:Cronologia della teoria dell'informazione.Si deve aClaude Shannonlo studio dell'entropia nella teoria dell'informazione.\", \"itIl suo primo lavoro sull'argomento si trova nell'articoloUna teoria matematica della comunicazionedel 1948.\", \"itNel primo teorema di Shannon, o teorema di Shannon sulla codifica di sorgente, egli dimostrò che una sorgente casuale d'informazione non può essere rappresentata con un numero dibitinferiore alla sua entropia, cioè alla suaautoinformazione media.\", \"it[3]Tale risultato era implicito nella definizione dell'entropia di John Von Neumann, anche se lo stesso Von Neumann, interrogato al riguardo da Shannon nel forse unico scambio di opinioni tra loro, non ritenne la cosa degna di attenzione.\", 'itCome ricordò Shannon più tardi a proposito del risultato da lui trovato:«La mia più grande preoccupazione era come chiamarla.', 'itPensavo di chiamarla informazione, ma la parola era fin troppo usata, così decisi di chiamarla incertezza.', \"itQuando discussi della cosa con John Von Neumann, lui ebbe un'idea migliore.\", 'itMi disse che avrei dovuto chiamarla entropia, per due motivi: \"Innanzitutto, la tua funzione d\\'incertezza è già nota nella meccanica statistica con quel nome.', \"itIn secondo luogo, e più significativamente, nessuno sa cosa sia con certezza l'entropia, così in una discussione sarai sempre in vantaggio»Definizione[modifica|modifica wikitesto]Informazione intrinseca[modifica|modifica wikitesto]L'informazione intrinseca di unevento, detta ancheautoinformazione, è la quantità d'incertezza associata allo stesso.\", \"itPiù concretamente è l'informazione che si ottiene affermando che tale evento si sia realizzato o meno, rimuovendo quindi l'incertezza associata.\", \"itL'autoinformazione è la forma più semplice di entropia definita da Shannon, e costituisce il punto di partenza nella definizione di altri concetti dellateoria dell'informazione.L'ambiguità esistente tra incertezza ed informazione non deve stupire.\", \"itEsse si presentano infatti come due facce della stessa medaglia: senza incertezza non c'è informazione, e quanta più incertezza c'è nel segnale aleatorio, tanto più informativo è rivelare qual è la determinazione del segnale.Formalmente, siaXunasorgentedi eventix, l'entropiaIassociata ad un singolo evento è definita dalla seguente scrittura:I(x)=−logb\\u2061p(x)p(x)}dovep(x)è la probabilità che l'eventoxaccada.Il logaritmo nasce dal fatto che attraverso lanotazione posizionaleè possibile distinguereNeventi equiprobabili con l'utilizzo di solelogb\\u2061NN}cifre, dovebè labase di numerazione.\", \"itSignifica quindi che l'informazione di un evento può essere vista come la quantità di cifre in basebda utilizzare per distinguere l'evento accaduto da tutti gli altri eventi possibili.\", \"itIl logaritmo diventa indispensabile se considerando due eventi indipendenti la cui probabilità è il prodotto delle singole probabilità si vuole che l'entropia totale sia la somma delle entropie dei singoli eventi.\", 'it[4]Entropia di una sorgente di informazione[modifica|modifica wikitesto]Entropia di una variabile diBernoulliNel caso delle sorgenti di informazione, per entropia si intende una grandezza utile a stimare a priori ilrateo della quantità di informazione emessa.', \"itAssunto che non sia possibile conoscere a priori qualedatoverrà emesso in un certo istante, ma solo la sua probabilità, si definisce l'entropia della sorgente come la media pesata dell'autoinformazionedeisimboliemissibili rispetto alla loro probabilità di emissione, a meno di una costante positiva di proporzionalità:[5]H=−K∑ip(xi)log\\u2061p(xi))\\\\log )}}}Nel caso l'alfabetodella sorgente sia costituito di simboli indipendenti, ovvero equiprobabili, l'espressione dell'entropia si riduce a:H=−K∑ilog\\u2061p(xi))}}}Nel caso particolare in cui la sorgente sia del tipo continuo invece che discreto è necessario descrivere l'entropia utilizzando l'espressioneintegraleomologa:H=−K∫p(x)log\\u2061p(x)dx}dx}Entropia congiunta[modifica|modifica wikitesto]Questa pagina sull'argomento Matematica sembra trattare argomenti unificabili alla paginaEntropia congiunta, che potrebbe confluire qui.Puoi contribuireunendoi contenuti in una pagina unica.\", 'itCommenta la procedura di unione usandoquesta pagina di discussione.', \"itSegui i suggerimenti delprogetto di riferimento.EntropieindividualiH(X),H(Y),congiunteH(X,Y), econdizionaliper una coppia di sottosistemi correlatiX,Yconinformazione mutuaI(X;Y).L'entropia congiunta di due variabili aleatorie discreteXeYè semplicemente l'entropia della coppia:(X,Y).\", \"itQuesto implica che, seXeYsonoindipendenti, allora la loro entropia congiunta è la somma delle loro entropie individuali.Per esempio, se(X,Y)rappresenta la posizione di un pezzo discacchi(Xla riga edYla colonna), allora l'entropia congiunta della riga e della colonna su cui è posto il pezzo sarà l'entropia della posizione del pezzo.H(X,Y)=EX,Y[−log\\u2061p(x,y)]=−∑x,yp(x,y)log\\u2061p(x,y) _[-\\\\log p(x,y)]=-\\\\sum _p(x,y)\\\\log p(x,y)}Nonostante la notazione simile, l'entropia congiunta non deve essere confusa con l'entropia incrociata.Entropia condizionale[modifica|modifica wikitesto]Questa pagina sull'argomento Matematica sembra trattare argomenti unificabili alla paginaEntropia condizionale, che potrebbe confluire qui.Puoi contribuireunendoi contenuti in una pagina unica.\", 'itCommenta la procedura di unione usandoquesta pagina di discussione.', \"itSegui i suggerimenti delprogetto di riferimento.L'entropia condizionale è la quantità di informazione necessaria per descrivere il valore di una variabile aleatoriaX }noto il valore di un'altra variabile aleatoriaY.\", 'itÈ anche nota come \"equivoco diXconY\".Nel contesto deicanali di telecomunicazionerappresenta l\\'incertezza rimanente su undatoin corso di trasmissione, rispetto all\\'informazione già trasmessa.Formalmente l\\'entropia condizionaleHdi una variabile aleatoriaX, data la variabile aleatoriaYè definita dalla seguente scrittura:H(X|Y)=EY[H(X|y)]=−∑y∈Yp(y)∑x∈Xp(x|y)log\\u2061p(x|y)=∑x,yp(x,y)log\\u2061p(y)p(x,y) _[H(X|y)]=-\\\\sum _p(y)\\\\sum _p(x|y)\\\\log p(x|y)=\\\\sum _p(x,y)\\\\log }}Un\\'importante corollario di questa definizione è che l\\'entropia condizionale si può esprimere come differenza tra l\\'entropia congiuntaH(X,Y)e l\\'entropia intrinsecadiY.H(X|Y)=H(X,Y)−H(Y)Informazione mutua[modifica|modifica wikitesto]Lo stesso argomento in dettaglio:Informazione mutua.L\\'informazione mutua è la quantità di informazione su una variabile aleatoria che può essere ricavata osservandone un\\'altra.', 'itIn un sistema di comunicazione è importante che sia massimizzata la quantità di informazione condivisa dai segnali inviati e ricevuti.', \"itL'informazione mutua diX, relativamente aYè:I(X;Y)=∑x,yp(x,y)log\\u2061p(x,y)p(x)p(y)p(x,y)\\\\log }}Un'importante proprietà dell'informazione mutua è cheI(X;Y)=H(X)−H(X|Y).Ossia, conoscendoY, possiamo risparmiare in mediaI(X;Y)bit nella codifica diX, rispetto al caso in cuiYè ignota.L'informazione mutua è simmetrica;I(X;Y)=I(Y;X)=H(X)+H(Y)−H(X,Y).L'informazione mutua può essere espressa come media dellaDivergenza di Kullback–Leiblerdellaprobabilità a posterioridiX, dato il valore diY, rispetto alla probabilità a priori diX:I(X;Y)=Ep(y)[DKL(p(X|Y=y)‖p(X))].\", 'it_[D_ }(p(X|Y=y)\\\\|p(X))].', 'it}In altre parole, essa misura quanto, in media, la probabilità della distribuzioneXcambia se conosciamo il valore diY.', 'itQuesto è spesso calcolato come divergenza dal prodotto delle distribuzioni marginali rispetto alla veradistribuzione congiunta:I(X;Y)=DKL(p(X,Y)‖p(X)p(Y)).', 'it}(p(X,Y)\\\\|p(X)p(Y)).', 'it}L\\'informazione mutua può essere considerata una statistica per stabilire l\\'indipendenza tra una coppia di variabili ed ha una distribuzione asintotica ben specificata.Entropia relativa[modifica|modifica wikitesto]Lo stesso argomento in dettaglio:Divergenza di Kullback-Leibler.L\\'entropia relativa, anche nota come \"divergenza di Kullback-Leible\", è un modo per confrontare due distribuzioni: una \"vera\"distribuzione di probabilitàp(X)ed una distribuzione arbitrariaq(X).', 'itSe comprimiamo dei dati in un qualche modo, per cuiq(x)è la distribuzione seguita dai dati compressi, quando in realtà la distribuzione dei dati èp(x), la divergenza di Kullback–Leibler è il numero di bit addizionali medi per dato necessari alla compressione.', \"itÈ quindi definita comeDKL(p(X)‖q(X))=∑x∈Xp(x)log\\u2061p(x)q(x) }(p(X)\\\\|q(X))=\\\\sum _p(x)\\\\log }}Legame con l'entropia termodinamica[modifica|modifica wikitesto]Lo stesso argomento in dettaglio:Entropia (termodinamica).Dalla definizione statistica dell'entropia termodinamica si intuisce che l'informazionee questa grandezza termodinamica siano in qualche modo correlati.\", \"itGli studi approfonditi in questo campo sono legati al lavoro pionieristico diClaude Shannonnel campo dellateoria dell'informazione.Nel 1948 Claude Shannon infatti enuncia il teorema di unicità dell'entropia: dato un insieme di caratteri alfanumericiA=}e dettap(i)la probabilità di osservare il simboloA(i), si definisce una funzione di entropiaH(p(0),p(1),…,p(n)) (p(0),p(1),\\\\ldots ,p(n))}, che deve rispettare le tre condizioni seguenti:seA(k)ha probabilitàp(k)=0di verificarsi, alloraH(p(0),p(1),…,p(k−1),0)=H(p(0),p(1),…,p(k−1)) (p(0),p(1),\\\\ldots ,p(k-1),0)=\\\\mathbb (p(0),p(1),\\\\ldots ,p(k-1))};dati i sistemi indipendentiAeB, si ha la seguente condizione di subadditività:H(A,B)<H(A)+H(B) (A,B)<\\\\mathbb (A)+\\\\mathbb (B)};l'entropiaH }è massima quandop(i)=1/r(doverè il numero totale di stati).Allora si dimostra che tale definizione di entropiaH }è ben posta ed è l'unica possibile.L'informazione viene matematicamente espressa dalla relazioneI=−log2\\u2061PP}che, utilizzando illogaritmoin base 2 dellaprobabilitàPche si verifichi un dato evento, permette di ottenere un valore misurato inbit.\", \"it1 bit equivale ad esempio all'informazione ottenibile dal lancio di una moneta (P=0,5).Dall'entropia espressa dalla relazione di Boltzmann è facile ricavare l'uguaglianzaS=log2\\u2061PP}che permette di esprimere l'entropia nella medesima unità di misura dell'informazione, ossia il bit.\", 'itNotare comePsi identifichi conΓ.', 'itIn conclusione si dimostra che vale la relazioneI=−Sche si può enunciare come \"a un aumento di entropia corrisponde una perdita di informazione su un dato sistema, e viceversa\".Grandezze associate[modifica|modifica wikitesto]Efficienza di un alfabeto[modifica|modifica wikitesto]Dato un alfabeto diNsimboli, la sua entropialogb\\u2061(N)(N)}nel trasmettere informazioni è massima se tutti i simboli vengono utilizzati con la stessa frequenza e si può definire l\\'efficienza dell\\'alfabeto come il rapporto tra la sua entropia e quella massima possibile per un alfabeto diNsimboli:η[X]=−∑i=1NP(xi)⋅logb\\u2061P(xi)logb\\u2061N.^ (x_)\\\\cdot \\\\log _ (x_)}}}}}.', 'it}Per comprimere file senza perdere informazione è necessario appunto utilizzare un alfabeto più efficiente.', 'itSe si osserva un file compresso con un editor di testo o esadecimale si può notare la grande casualità dei byte in esso contenuti.', \"itAlgoritmi che permettono di migliorare una codifica poco efficiente sono ad esempio lacodifica di Huffmane lacodifica aritmetica, entrambe le codifiche devono stimare la probabilità con cui si presentavano i simboli della codifica precedente per poterla migliorare.Esempi[modifica|modifica wikitesto]Fig.1 - Entropia di una sorgente binariaL'entropia di unasorgente binariaXche ha probabilitàpdi produrre1, probabilitàqdi produrre0e di conseguenzap+q=1è (vedi Fig.\", 'it1):H[X]=−(plog2\\u2061p+qlog2\\u2061q)=−[plog2\\u2061p+(1−p)log2\\u2061(1−p)].', 'it[X]=-\\\\left(p\\\\log _+q\\\\log _\\\\right)=-\\\\left[p\\\\log _+\\\\left(1-p\\\\right)\\\\log _\\\\left(1-p\\\\right)\\\\right].', 'it}Vale quindi 1 bit in caso di equiprobabilità dei risultati, e 0 bit nel caso in cui la sorgente sia completamente prevedibile (e cioè emetta sempre 0 o sempre 1).', \"itTale risultato è ragionevole in quanto nel primo caso si afferma che è necessario un bit d'informazione per ogni messaggio emesso dalla sorgente, mentre nel secondo caso non è necessario alcun bit in quanto si conosce a priori il valore di tutti i messaggi e quindi la sorgente è del tutto inutile.Per far capire la stretta correlazione tra entropia dell'informazione ed entropia della termodinamica possiamo fare il seguente esempio:Consideriamo un sistema fisico in date condizioni di temperatura, pressione e volume, e stabiliamone il valore dell'entropia; in connessione è possibile stabilire il grado di ordine e quindi l'ammontare delle nostre informazioni (in senso microscopico).\", \"itSupponiamo ora di abbassare la temperatura lasciando invariati gli altri parametri: osserviamo che la sua entropia diminuisce poiché il suo grado di ordine aumenta (ordine statico che corrisponde alla mancanza di movimento, lavoro) e con esso il nostro livello d'informazione.\", 'itAl limite, alla temperatura prossima allo zero assoluto, tutte le molecole sono \"quasi\" ferme, l\\'entropia tende al minimo e l\\'ordine (cristallizzato, non quello dell\\'organizzazione neghentropica che necessita di un sistema aperto) è il massimo possibile e con esso si ha la massima certezza d\\'informazione; infatti non esiste più alcuna alternativa fra cui scegliere.Note[modifica|modifica wikitesto]^Entropia, inTreccani.it –Vocabolario Treccanion line, Roma, Istituto dell\\'Enciclopedia Italiana.', \"it;Entropia, inTreccani.it – Enciclopedie on line, Roma, Istituto dell'Enciclopedia Italiana.\", \"it;Informazione, inTreccani.it – Enciclopedie on line, Roma, Istituto dell'Enciclopedia Italiana.^Bit, inTreccani.it – Enciclopedie on line, Roma, Istituto dell'Enciclopedia Italiana.^Shannon 2001.^Shannon 2001, p. 1.^Shannon 2001, pp.\", 'it9-14,27-28.Bibliografia[modifica|modifica wikitesto](EN)Claude Elwood Shannon,A mathematical theory of communication, inACM SIGMOBILE Mobile Computing and Communications Review, vol.', 'it5, n. 1, New York (NY, USA),Association for Computing Machinery, 1º gennaio 2001[prima pubblicazione 1948],DOI:10.1145/584091.584093,ISSN1559-1662(WC·ACNP).', 'it(EN)Robert Mario Fano,Transmission of information; a statistical theory of communications, MIT Press, 1961.R.', \"itBonazzi, R. Catena, S. Collina, L. Formica, A. Munna e D. Tesini,Telecomunicazioni per l'ingegneria gestionale.\", 'itCodifica di sorgente.', 'itMezzi di trasmissione.', 'itCollegamenti, Pitagora Editrice, 2004,ISBN88-371-1561-X.', 'it(EN) Xin Chen, Brent Francia, Ming Li, Brian McKinnon e Amit Seker,A theory of uncheatable program plagiarism detection and its practical implementation(PDF), 5 maggio 2002.URL consultato il 15 dicembre 2008.Olivier Costa de Beauregard,Irreversibilità, entropia, informazione: il secondo principio della scienza del tempo, Di Renzo Editore, 1994.', 'it(EN) Thomas M. Cover e Joy A. Thomas,Elements of Information Theory, 2ª ed., Hoboken (NJ, USA), Wiley, 2006,ISBN978-0-471-24195-9.', 'it(EN)Michael Wise,Improved Detection Of Similarities In Computer Program And Other Texts(PDF), 1996.', 'it(EN) M. Tribus e E.C.', 'itMcIrvine,Energy and information, inScientific American, n. 224, Nature Publishing Group, 1971, pp.', 'it178-184.', 'it(EN) Shigeru Furuichi, Flavia-Corina Mitroi-Symeonidis e Eleutherius Symeonidis,On some properties of Tsallis hypoentropies and hypodivergences, inEntropy, n. 16, MDPI, 15 ottobre 2014,DOI:10.3390/e16105377,ISSN5377-5399(WC·ACNP).', 'it(EN) Shigeru Furuichi e Flavia-Corina Mitroi-Symeonidis,Mathematical inequalities for some divergences, inPhysica A: Statistical Mechanics and its Applications, n. 391, Science Direct, 2012, pp.', 'it388-400,DOI:10.1016/j.physa.2011.07.052,ISSN0378-4371(WC·ACNP).', 'it(EN) Shigeru Furuichi, Nicuşor Minculete e Flavia-Corina Mitroi-Symeonidis,Some inequalities on generalized entropies, inJournal of Inequalities and Applications, 2012:226, Springer, 2012,DOI:10.1186/1029-242X-2012-226.', \"it(EN) Dènes Petz,Entropy, von Neumann and the von Neumann entropy(PDF), inJohn von Neumann and the Foundations of Quantum Physics, Dordrecht, Kluwer Academic Publishers, 2001,DOI:10.1016/S1355-2198(03)00070-4,ISBN0792368126,ISSN1355-2198(WC·ACNP).URL consultato il 22 marzo 2005(archiviato dall'url originaleil 9 maggio 2005).Voci correlate[modifica|modifica wikitesto]EntropiaNegentropia, l'entropia differenzialeCompressione dei datiClaude ShannonJohn von NeumannAltri progetti[modifica|modifica wikitesto]Altri progettiWikizionarioWikimedia CommonsWikizionariocontiene il lemma di dizionario «entropia»Wikimedia Commonscontiene immagini o altri file sull'entropiaCollegamenti esterni[modifica|modifica wikitesto]entropia, suTreccani.it – Enciclopedie on line,Istituto dell'Enciclopedia Italiana.Lucio Bianco e Maurizio Talamo,ENTROPIA, inEnciclopedia Italiana, V Appendice,Istituto dell'Enciclopedia Italiana, 1992.entropia, inDizionario delle scienze fisiche,Istituto dell'Enciclopedia Italiana, 1996.Antonio Di Meo,entropia, inEnciclopedia della scienza e della tecnica,Istituto dell'Enciclopedia Italiana, 2007-2008.entropìa, suVocabolario Treccani,Istituto dell'Enciclopedia Italiana.entropìa, susapere.it,De Agostini.entropia, inEnciclopedia della Matematica,Istituto dell'Enciclopedia Italiana, 2013.\", 'it(EN)entropy/Shannon’s entropy, suEnciclopedia Britannica, Encyclopædia Britannica, Inc.(EN) Eric W. Weisstein,Entropia, suMathWorld, Wolfram Research.', 'it(EN) Lukasz Kozlowski,Shannon entropy calculator, sushannonentropy.netmark.pl.V·D·MMetodi dicompressione dei datiLosslessTeoriaEntropia·Complessità·Ridondanza·LossyCodificazione entropicaShannon-Fano·Shannon-Fano-Elias·Huffman·Adattiva di Huffman·Aritmetica·A catena·Golomb·Universale(Gamma·Exp-Golomb·Fibonacci·Levenshtein)DizionarioRLE·Byte pair encoding·DEFLATE·Lempel–Ziv(LZ77/78·LZSS·LZW·LZWL·LZO·LZMA·LZX·LZRW·LZJB·LZS·LZT·ROLZ)AltroCTW·BWT·PPM·DMC·DeltaAudioTeoriaCompansione·Convoluzione·Intervallo dinamico·Latenza·Campionamento·Teorema di Nyquist–Shannon·Qualità del suonoParti dei codec audioLPC(LAR·LSP)·WLPC·CELP·ACELP·Legge A·Legge μ·ADPCM·DPCM·MDCT·Trasformata di Fourier·Modello psicoacusticoAltroVelocità di trasmissione(CBR·ABR·VBR)·Compressione vocale·Codifica di sottobandaImmagineTerminiSpazio dei colori·Pixel·Sottocampionamento della crominanza·Artefatto di compressione·RisoluzioneMetodiRLE·Frattale·Wavelet·EZW·SPIHT·LP·DCT·Codice a catena·KLTAltroImmagini di prova·PSNR·QuantizzazioneVideoTerminiCaratteristiche video·Fotogramma·Frequenza dei fotogrammi·Interlacciamento·Tipi di fotogramma·Qualità video·Risoluzione videoParti dei codec videoCompensazione di moto·DCT·QuantizzazioneAltroParti dei codec video·Teoria della distorsione della velocità·Velocità di trasmissione(CBR·ABR·VBR)Cronologia della teoria dell\\'informazione, della compressione dei dati e dei codici di correzione degli erroriVediFormati di compressioneper i formati eSoftware di compressioneper i codecControllo di autoritàLCCN(EN)sh85044152·GND(DE)4743861-7·BNE(ES)XX535116(data)·BNF(FR)cb11985913j(data)·J9U(EN,HE)987007550784405171·NDL(EN,JA)01191172Portale InformaticaPortale MatematicaEstratto da \"https://it.wikipedia.org/w/index.php?title=Entropia_(teoria_dell%27informazione)&oldid=138783715\"Categoria:Teoria dell\\'informazioneCategorie nascoste:Contestualizzare fonti - informaticaContestualizzare fonti - marzo 2015Unire - matematicaUnire - ottobre 2023P3365 letta da WikidataP4223 letta da WikidataP10017 letta da WikidataP10037 letta da WikidataP5844 letta da WikidataP6706 letta da WikidataP9621 letta da WikidataP1417 letta da WikidataP2812 letta da WikidataVoci con codice LCCNVoci con codice GNDVoci con codice BNEVoci con codice BNFVoci con codice J9UVoci con codice NDLVoci non biografiche con codici di controllo di autoritàMenu di navigazioneStrumenti personaliAccesso non effettuatodiscussionicontributiregistratientraNamespaceVoceDiscussioneitalianoVisiteLeggiModificaModifica wikitestoCronologiaAltroRicercaNavigazionePagina principaleUltime modificheUna voce a casoNelle vicinanzeVetrinaAiutoSportello informazioniComunitàPortale ComunitàBarIl WikipedianoFai una donazioneContattiStrumentiPuntano quiModifiche correlatePagine specialiLink permanenteInformazioni paginaCita questa voceOttieni URL breveDownload QR codeElemento WikidataStampa/esportaCrea un libroScarica come PDFVersione stampabileIn altri progettiWikimedia CommonsIn altre lingueAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa Indonesia日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Modifica collegamentiQuesta pagina è stata modificata per l\\'ultima volta il 12 apr 2024 alle 11:32.Il testo è disponibile secondo lalicenza Creative Commons Attribuzione-Condividi allo stesso modo; possono applicarsi condizioni ulteriori.', \"itVedi lecondizioni d'usoper i dettagli.Informativa sulla privacyInformazioni su WikipediaAvvertenzeCodice di condottaSviluppatoriStatisticheDichiarazione sui cookieVersione mobile\", 'mhrШеннонын формулжо — ВикипедийСодержанийышке куснашТӱҥ менюТӱҥ менюӧрдыж панельыш кусарашшылташНавигацийТӱҥ лаштыкТӱшка порталПытартыш тӧрлатымаш-влакЧокым лаштыкПолышМодмо верНадырлашКычалмашКычалашАккаунтым ышташПурашЛӱмал ӱзгар-влакАккаунтым ышташПурашӦрдыж редактор-влаклан странице-влакутларак пален налашНадырКаҥашымашШеннонын формулжо45 йылмеAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Кылвер-влакым тӧрлашЛаштыкКаҥашымашолык марийЛудашТӧрлаташКодым тӧрлаташЭртымгорным ончалашӰзгар-влакӰзгар-влакӧрдыж панельыш кусарашшылташЫштымашЛудашТӧрлаташКодым тӧрлаташЭртымгорным ончалашЧумырТышке кондышо кылвер-влакВаш кылдалтше тӧрлатымаш-влакФайлым вераҥдашСпецвелыж-влакТутыш кылверВелыж нерген информацийЛаштыкым цитироватлашКӱчык URL-адресым налашСкачать QR-кодВикиданный-влак иктыкСавыктымаш/экспортКнигам ышташPDF семын тулен налашСавыкташлан версийМоло проектлаштеВикиклатВикипедий — эрыкан энциклопедий гыч материалШеннонын формулжо(рушлаформула Шеннона) — тӱрлӧ лийынкертшашлык годым I информацийын чотшым шотлымоформул: I= -i=1Npilog2pi, тушто pi - i событийын лийынкертшашлыкше, N – лийшаш событийын чотшо.', 'mhrhttps://mhr.wikipedia.org/w/index.php?title=Шеннонын_формулжо&oldid=122578 гыч налмеКатегорий:ИнформатикеТиде велыжым пытартыш гана 29 кылме 2013 кечын 18:51 жаплан тӧрлымӧ.ТекстымCreative Commons Attribution-ShareAlikeлицензий почеш кучылташ лиеш, тыгак посна келшык-влак лийын кертыт.', 'mhrРашракКучылтмо келшыкыштеончо.Шолыплык политикеВикипедий нергенМут кучымаш деч кораҥмашКодекс поведенияЫштызе-влакИктешлымашКуки нерген келшыкМобильный версийКонтент лӱмерын шыгыремдыме лопкытшым Ончыкташ/Шылташ', 'deEntropie (Informationstheorie) – WikipediaEntropie (Informationstheorie)aus Wikipedia, der freien EnzyklopädieZur Navigation springenZur Suche springenEntropie(nach dem Kunstwort ἐντροπία)[1]ist in derInformationstheorie:einfach gesagt: die durchschnittliche Anzahl von Entscheidungen (bits), die benötigt werden, um ein Zeichen aus einer Zeichenmenge zu identifizieren oder zu isolieren,anders gesagt: ein Maß, welches für eine Nachrichtenquelle den mittlerenInformationsgehaltausgegebener Nachrichten angibt.Der Begriff ist eng verwandt mit derEntropiein derThermodynamikundstatistischen Mechanik.Das informationstheoretische Verständnis des BegriffesEntropiegeht aufClaude E. Shannonzurück und existiert seit etwa 1948.', 'deIn diesem Jahr veröffentlichte Shannon seine fundamentale ArbeitA Mathematical Theory of Communication[2]und prägte damit die moderne Informationstheorie.Die Entropie wird üblicherweise mit einem großenEta(H }) bezeichnet.', 'de[1]Inhaltsverzeichnis1Definition2Interpretation3Maximaler Entropiewert und Normierung4Beispiele4.1Alphabet4.2Münzwurf4.3Idealer Würfel5Entropietests6Datenkompression und Entropie7Alternative Möglichkeiten der Informationsquantifizierung8Ähnlichkeit zur Entropie in der Physik9Siehe auch10Literatur11Weblinks12EinzelnachweiseDefinition[Bearbeiten|Quelltext bearbeiten]Claude Elwood Shannondefinierte die EntropieH }einer diskreten, gedächtnislosen Quelle (diskreten Zufallsvariable)Xüber einem endlichen, aus Zeichen bestehendenAlphabetZ=,z_,\\\\dots ,z_\\\\}}wie folgt: Zunächst ordnet man jederWahrscheinlichkeitpeines Ereignisses seinenInformationsgehaltI(z)=−log2\\u2061pzp_}zu.', 'deDann ist die Entropie eines Zeichens definiert als derErwartungswertdes InformationsgehaltsH1=E[I]=∑z∈ZpzI(z)=−∑z∈Zpzlog2\\u2061pz _=E[I]=\\\\sum _p_I(z)=-\\\\sum _p_\\\\log _p_}.Seiz∈Z, dann istpz=P(X=z)=P(X=z)}die Wahrscheinlichkeit, mit der das Zeichenzdes Alphabets auftritt, oder gleichwertig(1)H1=−∑i=1mpilog2\\u2061pi _=-\\\\sum _^p_\\\\log _p_},mitpi=pzi=p_}}.', 'deDabei wird0⋅log2\\u20610=00=0}gesetzt (entsprechend demGrenzwertlimx→0xlog2\\u2061xx\\\\log _x}).', 'deSummanden mit verschwindender Wahrscheinlichkeit tragen daher aufgrund der Definition nicht zur Summe bei.Die EntropieHn _}für Wörterwder Längenergibt sich durchHn=−∑w∈Znpwlog2\\u2061pw _=-\\\\sum _}p_\\\\log _p_},wobeipw=P(X=w)=P(X=w)}die Wahrscheinlichkeit ist, mit der das Wortwauftritt.', 'deDie EntropieH }ist dann derLimesn→∞davon:(2)H=limn→∞Hnn =\\\\lim _ _}}}.Wenn die einzelnen Zeichen voneinanderstochastisch unabhängigsind, dann giltHn=nH1 _=n\\\\mathrm _}für allen, alsoH=H1 =\\\\mathrm _}(vgl.Blockentropie).Interpretation[Bearbeiten|Quelltext bearbeiten]Entropie ist ein Maß für den mittleren Informationsgehalt pro Zeichen einer Quelle, die ein System oder eine Informationsfolge darstellt.', 'deIn der Informationstheorie spricht man bei Information ebenso von einem Maß fürbeseitigte Unsicherheit.', 'deJe mehr Zeichen im Allgemeinen von einer Quelle empfangen werden, desto mehr Information erhält man und gleichzeitig sinkt die Unsicherheit über das, was hätte gesendet werden können.Je kleiner die Auftrittswahrscheinlichkeit eines Zeichens ist, desto höher ist seine Information.', 'deAndersherum ist die Information eines Zeichens gering, wenn es oft vorkommt.Anschaulich lässt sich die Definition des Informationsgehalts wie folgt begründen: Wenn ein Ereignis, das mit Wahrscheinlichkeitpi}eintreten kann, tatsächlich eintritt, dann wird dadurch ein konkretes Ereignis aus einer hypothetischen Menge von1pi}}}gleich wahrscheinlichenstochastisch unabhängigen Ereignissenausgewählt.', 'deUm diese Anzahl von Ereignissen unterscheiden zu können, benötigt manlog2\\u2061(1pi)=−log2\\u2061(pi)\\\\left(}}\\\\right)=-\\\\log _(p_)}Binärbits.', 'deDieser Wert gibt also den Informationsgehalt eines speziellen Ereignisses in Bits an.', 'deGewichtet man den tatsächlichen Informationsgehalt der möglichen Ereignisse mit der jeweiligen Eintrittswahrscheinlichkeit, so erhält man den mittleren oder erwarteten Informationsgehalt eines Zeichens.Die Einheit 1Shannonist definiert als der Informationsgehalt eines Ereignisses mit der Wahrscheinlichkeitp=0,55}.', 'deEin Beispiel für ein solches Ereignis ist das ErgebnisKopfeinesMünzwurfs.Die Basis 2 für den Logarithmus ist willkürlich.', 'deEs stellt sich nur heraus, dass sich Bits (Binärziffern) technisch besonders einfach handhaben lassen.', 'deWürde eine andere Basis gewählt werden, zum Beispiel 3, so erhielte manternäreZiffern (Trits).', 'deDer Informationsgehalt lässt sich leicht durch Multiplikation mit demModuluslog3\\u206122}von Bits auf Trits umrechnen.Die mindestens notwendige Anzahl von Bits, die zur Darstellung der Information (des Textes) notwendig ist, ergibt sich aus dem Produkt des durchschnittlichen Informationsgehalts eines ZeichensI(X)=log2\\u2061|Z|=log2\\u2061N|Z|=\\\\log _N}und der Anzahlnder Zeichenzim Informationstext:I(X)|z|=I(X)n=nlog2\\u2061NN}.Shannons ursprüngliche Absicht, die Entropie als das Maß der benötigten Bandbreite einesÜbertragungskanalszu nutzen, wurde schnell verallgemeinert.', 'deDie Entropie wurde generell als ein Maß für den Informationsgehalt betrachtet.', 'deBei einer kleinen Entropie enthält der InformationstextRedundanzenoder statistische Regelmäßigkeiten.Die Entropie ist im Allgemeinen nicht durch (1) gegeben.', 'deBeispielsweise ist die Wahrscheinlichkeit, eine0oder1in der Zeichenkette1010101010…zu finden, genauso groß wie in einer Zeichenkette, die durch statistisch unabhängigeEreignisse(etwa wiederholten Münzwurf) entstanden ist.', 'deDaher ist die EntropieH1 _}einzelner Zeichen für beide Zeichenketten gleich, obwohl die erste Kette weniger zufällig ist.', 'deBereitsH2 _}zeigt einen Unterschied: Die erste Zeichenkette liefertH2=1 _=1}, die zweite liefertH2=2 _=2}.', 'deMan kann das auch so deuten: Die Wahrscheinlichkeit eines Zeichens hängt vom vorangegangenen Zeichen ab.', 'deStochastische Unabhängigkeit ist also nicht gegeben.Für aufeinander folgende Ereignisse, die nicht stochastisch unabhängig sind, reduziert sich die Entropie aufeinander folgender abhängiger Ereignisse fortlaufend.', 'deIn einem solchen Fall kann man auch mit derbedingten Entropieund derQuell-entropiearbeiten, die beide auf Verbundwahrscheinlichkeiten aufbauen.In engem Zusammenhang mit bedingter Entropie steht auch dieTransinformation, welche die Stärke des statistischen Zusammenhangs zweier Zufallsgrößen angibt.Noch einfacher formuliert, ist die Entropie die durchschnittliche Anzahl von Entscheidungen (bits), die benötigt werden, um ein Zeichen aus einer Zeichenmenge zu identifizieren oder zu isolieren.Es ist sinnvoll, dass ein Alphabet aus mindestens zwei verschiedenen Zeichen vorliegt.', 'deEine Alphabetsgröße voneinsbedeutet, dass man weder über neu ankommende Zeichen aus der Senderquelle neue Information erhält, noch die Unsicherheit über das vorangegangene Zeichen verringern kann.Maximaler Entropiewert und Normierung[Bearbeiten|Quelltext bearbeiten]Möchte man ein normiertes Maß für die Entropie einer beliebigendiskreten Verteilunghaben, ist es von Vorteil, die maximal mögliche Entropie, die bei Gleichverteilung derpi}erreicht wird, zur Normierung heranzuziehen.', 'deSeiN=|Z|die Anzahl der Zeichen inXüber dem AlphabetZ, dann ist diemaximale EntropieHmax _ }}gegeben wennpi=1|Z|=1N∀pi=}=}\\\\ \\\\forall \\\\;p_},dann istHmax=−∑i=1N1Nlog2\\u20611N=log2\\u2061N _ }=-\\\\sum _^}\\\\log _}}=\\\\log _}.Daraus folgt beispielsweiseHmax=1 _ }=1}für eine Binärverteilung (Z=}), also benötigt man ein Bit pro Zeichen und|I|Zeichen für die komplette InformationI.', 'deDieser Wert wird erreicht, wenn Nullen und Einsen gleich häufig vorkommen.', 'deNormiert man nun die Entropie einer beliebigen Verteilung mitN=|Z|verschiedenen Zeichen mitHmax _ }}erhält man:HHmax=−∑i=1|Z|pi⋅log2\\u2061pilog2\\u2061N=−∑i=1|Z|pi⋅logN\\u2061pi≤1 } _ }}}=-\\\\sum _^\\\\cdot }}}}}=-\\\\sum _^\\\\cdot \\\\log _}}\\\\leq 1}Die so definiertenormierteEntropie kann maximal den Wert1annehmen.Um die Entropien von Nachrichten unterschiedlicher Länge vergleichen zu können, hat man dieEntropierateeingeführt, die die Entropie auf das einzelne Zeichen bezieht (siehe dort).Beispiele[Bearbeiten|Quelltext bearbeiten]Alphabet[Bearbeiten|Quelltext bearbeiten]Bei gleichmäßiger Verteilung kann bei einem Alphabet auf kein Zeichen verzichtet werden.', 'deDagegen ist dieBuchstabenhäufigkeitin der deutschen Sprache ungleichmäßig, siehe auch:Entropie (Kryptologie).', 'deBeispielsweise ist der Buchstabe E im Deutschen siebenmal so häufig wieModerO, was zu Redundanz im Alphabet führt.', 'deNun möchte man ermitteln, wie groß diese Redundanz ist.SeiN=26die Größe des Alphabets.', 'deDie Redundanz R berechnet sich mitR=Hmax−H _ }-\\\\mathrm }.', 'deFür das deutsche Alphabet errechnet man anhand der Buchstabenhäufigkeit eine EntropieH }von 4,0629 bit/Zeichen.', 'deDie maximale Entropie beträgtHmax=log2\\u2061|26|=4,7004 _ }=\\\\log _|26|=47004}bit/Zeichen.', 'deDamit folgt eine Redundanz vonR=4,7004−4,0629=0,63757004-40629=06375}bit/Zeichen.', 'deBerechnet man weiter die gesamte Redundanz, die sich aus der Summe der Redundanzen eines jeden Zeichens ergibt, so erhält manR⋅N=16,575575}Bits.', 'deNun wäre interessant zu wissen, wie vielen Zeichen dies aus unserem Alphabet entspricht.', 'deDividiert man die redundanten Bits durch den durchschnittlichen Informationsgehalt eines gleichverteilten Zeichens, so erhält man(R⋅N)/Hmax=3,53 _ }=353}Zeichen → 3 Zeichen.Ohne Informationsverlust könnte das Alphabet also um drei Buchstaben reduziert werden.', 'deDiese Überlegung berücksichtigt nur die statistische Verteilung der Buchstaben.', 'deHäufige Buchstabenkombinationen wieSCHoderSTbleiben genauso unberücksichtigt (bedingte Entropie) wie gleich klingende Buchstaben (Q,K).Münzwurf[Bearbeiten|Quelltext bearbeiten]Maximale Entropie bei p=0.5Bei einem Münzwurf sind idealerweiseKopfoderZahlgleich wahrscheinlich.', 'deWenn man die Entropie als Maß für dieUngewissheitauffasst, wird sie hier einen maximalen Wert aufweisen.', 'deEs ist völlig ungewiss, ob beim nächsten WurfKopfoder aberZahlgeworfen wird.SeiXeine diskrete Zufallsvariable und derErwartungswertE(X)=∑P(X=xi)⋅xi)\\\\cdot x_}mitP(X=x0)=p0=p=12)=p_=p=}}(Kopf) undP(X=x1)=p1=q=12)=p_=q=}}(Zahl),so ergibt sich aus obiger Definition EntropieH=1 =1}bit.Anders bei einer gezinkten Münze, etwa einer Münze, die im Mittel in 60 % der FälleKopfund nur in 40 % der FälleZahlanzeigt.', 'deDie Ungewissheit ist hier geringer als bei der normalen Münze, da man eine gewisse Präferenz fürKopfhat.', 'deGemessen als Entropie liegt die Ungewissheit bei nur noch etwa 0,971.Die Summe der Wahrscheinlichkeiten ist immer 1.p+q=1Die Entropie lässt sich aus der Summe der Teilentropien berechnen:H=Hp+Hq=−p⋅log2\\u2061p−q⋅log2\\u2061q =\\\\mathrm _+\\\\mathrm _=-p\\\\cdot \\\\log _p-q\\\\cdot \\\\log _q}Ersetzt manqdurch den Ausdruck1−p, so erhält man die FormelH=−p⋅log2\\u2061p−(1−p)⋅log2\\u2061(1−p) =-p\\\\cdot \\\\log _p-(1-p)\\\\cdot \\\\log _(1-p)}Dies kann man grafisch folgendermaßen darstellen:Für jedespkann man daraus die Entropie direkt ablesen.', 'deDie Funktion ist symmetrisch zur Geradenp=0,55}.', 'deSie fällt beip=0steil zu einem Entropie-Wert von 0 ab.', 'deAuch bei Werten, die sich dem sicheren Ereignis vonp=1nähern, fällt die Entropie auf 0 ab.Dieser Zusammenhang gilt jeweils für ein Zufallsereignis.', 'deBei mehreren Zufallsereignissen muss man die einzelnen Entropien zusammenzählen und man kann so leicht Entropiewerte über 1 erreichen.', 'deDie Wahrscheinlichkeitpdagegen bleibt auch bei Wiederholungen definitionsgemäß immer zwischen 0 und 1.Entropie in Abhängigkeit von der Zahl der MünzwürfeWiederholt man den Münzwurf zweimal, wächst die Zahl der Möglichkeiten auf vier.', 'deDie Wahrscheinlichkeit jeder einzelnen Möglichkeit liegt bei 0,25.', 'deDie Entropie des zweimaligen Münzwurfes ist dann 2 Sh.', 'deWenn man einen idealen Münzwurf mehrfach wiederholt, dann addiert sich die Entropie einfach.', 'deDie Entropie einer Reihe von 20 idealen Münzwürfen berechnet sich einfach:H=20⋅1bit=20bit =20\\\\cdot 1\\\\;\\\\mathrm =20\\\\;\\\\mathrm }.', 'deDies ist im Bild dargestellt.Man kann nicht einfach auseinemWert der Wahrscheinlichkeit die Entropie ausrechnen.', 'deDie Entropie betrifft den gesamten Zufallsprozess.', 'deJede Teilwahrscheinlichkeit eines möglichen Ergebnisses geht in die Berechnung der Entropie des Gesamtprozesses ein.', 'deDie Angabe einer Teilentropie für jedes mögliche Ergebnis ist dabei wenig sinnvoll.', 'deIn der Shannonschen Entropieformel sollte also die Summe der Wahrscheinlichkeiten 1 ergeben, sonst kann das Ergebnis missverständlich sein.Speichert man eine Folge von Münzwürfen als Bitfolge, dann bietet es sich an,Kopfstets durch 0 undZahlstets durch 1 zu repräsentieren (oder umgekehrt).', 'deBei der gezinkten Münze sind kompaktereKodierungenmöglich, zum Beispiel dieHuffman-Kodierung.Idealer Würfel[Bearbeiten|Quelltext bearbeiten]Bei einem Wurf eines idealen Würfels mit sechs Möglichkeiten ist die Entropie größer als 1.', 'deIm Allgemeinen ist die Entropie größer als 1 für ein Zufallsereignis mit stochastisch unabhängigen Zufallsvariablen aus einem Zufallsexperiment mit mehr als zwei gleichberechtigten Möglichkeiten imErgebnisraum.', 'deIhr Wert wird bei gleich wahrscheinlichen Möglichkeiten im Ergebnisraum folgendermaßen berechnet:Seindie Anzahl der Möglichkeiten, dann sind die Wahrscheinlichkeitenpi=1n=}}und die EntropieH=log2(1pi)=log2\\u2061(n) =\\\\log _\\\\!\\\\left(}}\\\\right)=\\\\log _(n)}.Beim idealen Würfel sind sechs Möglichkeiten im Ergebnisraum.', 'deDaraus folgt die Entropie für einmaliges Werfen:H=log2\\u20616=log2\\u2061(2⋅3)=log2\\u20612+log2\\u20613=1+log2\\u20613≈1+1,585=2,585Sh =\\\\log _6=\\\\log _(2\\\\cdot 3)=\\\\log _2+\\\\log _3=1+\\\\log _3\\\\approx 1+1585=2585\\\\;\\\\mathrm }Entropie vs. Zahl der MöglichkeitenEinfach zu berechnen ist die Entropie eines Wurfes eines idealen Achterwürfels: Er hat acht gleichberechtigte Möglichkeiten.H=log2\\u20618=3Sh =\\\\log _8=3\\\\;\\\\mathrm }Die Entropie eines Wurfes mit dem idealen Achterwürfel entspricht der Entropie von drei Würfen mit der idealen Münze.Die Abbildung stellt den Zusammenhang zwischen der Entropie und der Zahl der gleichberechtigten Möglichkeiten eines Zufallsexperimentes dar.Entropietests[Bearbeiten|Quelltext bearbeiten]Um zu testen, wie gut Daten komprimierbar sind, oder um Zufallszahlen zu testen, werden Entropietests verwendet.', 'deAls Zufallszahltest wird die Entropie einer bestimmten Anzahl von Zufallszahlen bestimmt und ab einem Mindestwert, beispielsweise 7 Bit je Byte, gilt er als bestanden.', 'deAllerdings gibt es viele solcher Tests, da die Entropie nicht eindeutig ist; sie kann beispielsweise bitbasiert oder bytebasiert definiert sein.Ein einfaches Beispiel:Eine Quelle, etwa ein Spielwürfel oder eine Münze, gebe nur die Werte 0xAA (dezimal 170) und 0x55 (dezimal 85) aus, beide mit gleicher Wahrscheinlichkeit.', 'deBitweise ist die Ausgabe zu 50 % 0 oder 1, byteweise ist sie zu 50 % 0xAA oder 0x55.', 'deDie bitweise Entropie ist (mitlog=log2})HHmax=−1/log\\u2061(2)⋅(1/2⋅log\\u2061(1/2)+1/2⋅log\\u2061(1/2))=1 } _ }}}=-1/\\\\log(2)\\\\cdot (1/2\\\\cdot \\\\log(1/2)+1/2\\\\cdot \\\\log(1/2))=1}während die byteweise Entropie mitHHmax=−1/log\\u2061(256)⋅(1/2⋅log\\u2061(1/2)+1/2⋅log\\u2061(1/2))=1/8 } _ }}}=-1/\\\\log(256)\\\\cdot (1/2\\\\cdot \\\\log(1/2)+1/2\\\\cdot \\\\log(1/2))=1/8}deutlich kleiner ist.Der Hersteller dieses Zufallszahlengenerators wird natürlich als Entropie des Geräts die bitweise Entropie, also 1, angeben.', 'deAnalog wird ein Programmierer eines Kompressionsprogramms möglichst diejenige Basis wählen, bei der die Entropie minimal ist (hier Bytes), sich also die Daten am besten komprimieren lassen.Dieses Beispiel ist wenig realistisch, da nur zwei von 256 möglichen Werten verwendet werden, aber wenn auch die anderen Bytes mit einer kleinen Wahrscheinlichkeit von beispielsweise 1/123456789 ausgegeben werden, so ändert dies an der bitweisen Entropie nichts und die byteweise wird kaum größer; sie bleibt unter 1/2.', 'deErst mit Annäherung der Byte-Wahrscheinlichkeiten an 1/256 erreicht die byteweise Entropie den Wert 1, aber dann kann es noch Korrelationen der Bytes geben, also etwa die Folge0xaaaaviel häufiger sein als die Folge0x5555.', 'deDies ist der Hauptgrund, weshalb es viele verschiedene Zufallszahlentests gibt.Diese Mehrdeutigkeit ist nicht möglich beim Entropiebelag, da dort nicht nur über Wahrscheinlichkeiten summiert wird, sondern über ergodische Wahrscheinlichkeiten von Zuständen, Zustandsübergangswahrscheinlichkeiten und bedingte Wahrscheinlichkeiten.', 'deBerechnet wird er mit der Theorie derMarkow-Kette.', 'deAllerdings ist der Rechenaufwand dafür bei realen Zufallszahlengeneratoren hoch.Es ist wichtig zu erklären, dass Entropietests nur Gleichwahrscheinlichkeit messen, und keine echte Unvorhersehbarkeit.', 'deDer Unterschied zwischen echten Zufallszahlengeneratoren und Pseudozufallszahlengeneratoren ist unmessbar.Datenkompression und Entropie[Bearbeiten|Quelltext bearbeiten]DieEntropiekodierungist einKompressionsalgorithmus, um Daten verlustfrei zu komprimieren.', 'deIn diesem Zusammenhang spielen dieKreuzentropiesowie dieKullback-Leibler-Divergenzals Maß für die durch eine schlechte Kodierung ausgelösten Verschwendungen von Bits eine Rolle.Beispiel:Gegeben sei die ZeichenketteABBCAADA(siehe auchEntropiekodierung).Die Buchstaben-Wahrscheinlichkeit:pA=4/8=0,5=4/8=05};pB=0,25=025};pC=pD=0,125=p_=0125}H=−(0,5⋅log2\\u20610,5+0,25⋅log2\\u20610,25+2⋅(0,125⋅log2\\u20610,125))=1,75 =-(05\\\\cdot \\\\log _5}+025\\\\cdot \\\\log _25}+2\\\\cdot (0125\\\\cdot \\\\log _125}))=175}Maximalentropie (pA=pB=pC=pD=0,25=p_=p_=p_=025}):Hmax=−4⋅(0,25⋅log2\\u20610,25)=−log2\\u20614−1=log2\\u20614=2 _ }=-4\\\\cdot (025\\\\cdot \\\\log _25})=-\\\\log _}=\\\\log _=2}Die Maximalentropie lässt sich ebenso mit der Formel der maximalen Entropie berechnen:Hmax=log2\\u2061||=log2\\u20611pA=log2\\u20614=2 _ }=\\\\log _|}=\\\\log _}}=\\\\log _=2}Alternative Möglichkeiten der Informationsquantifizierung[Bearbeiten|Quelltext bearbeiten]Ein anderer Zugang, den Informationsgehalt einer Nachricht zu messen, ist durch dieKolmogorow-Komplexitätgegeben, worin der kürzestmöglicheAlgorithmuszur Darstellung einer gegebenen Zeichenkette die Komplexität der Nachricht angibt.', 'deÄhnlich ist dieLogische Tiefedefiniert, die sich aber auf dieZeitkomplexitäteines Algorithmus zur Erzeugung der Daten bezieht.Gregory Chaitinist ebenfalls über die Shannonsche Definition der Entropie einer Information hinausgegangen (sieheAlgorithmische Informationstheorie).', 'deDiedifferentielle Entropiehingegen kann zum Vergleich des Informationsgehalts zwischen kontinuierlichenZufallsvariablenverwendet werden.Ähnlichkeit zur Entropie in der Physik[Bearbeiten|Quelltext bearbeiten]In der Physik (sieheThermodynamik, speziellEntropie) spielt eine gleich benannte Größe eine wesentliche Rolle[3][4].', \"deDie physikalische Entropie unterscheidet sich von der Shannon'schen Informationsentropie durch einen zusätzlichen NormierungsfaktorkB }}, dieBoltzmannsche Konstante, und durch die Ersetzung der im Logarithmus benutzten Basis (derduale Logarithmuswird durch dennatürlichen Logarithmusersetzt).\", 'deSomit unterscheiden sich physikalische Entropie und mathematische Entropie durch den positiven UmrechnungsfaktorkBln\\u20612 }\\\\ln 2}, versehen mit seiner physikalischen Einheit.', 'deDer Zusammenhang wurde durch einMaxwellscher Dämongenanntes Gedankenexperiment hergestellt.Siehe auch[Bearbeiten|Quelltext bearbeiten]Auffälligkeit (Informationstheorie)BlockentropieDifferentielle Entropiefür kontinuierliche VerteilungenEntropieschätzungQuellentropieRényi-EntropieTransinformationundKullback-Leibler-DivergenzLiteratur[Bearbeiten|Quelltext bearbeiten]C. E. Shannon:A Mathematical Theory of Communication.', 'deIn:Bell System Technical Journal.Band27,Nr.3, 1948,S.379–423,doi:10.1002/j.1538-7305.1948.tb01338.x(PDF).Claude Elwood ShannonundWarren Weaver:The Mathematical Theory of Communication, University of Illinois Press 1963,ISBN 0-252-72548-4(Softcover) undISBN 0-252-72546-8(Hardcover)Rolf Johannesson:Informationstheorie, Addison-Wesley 1992,ISBN 3-89319-465-7Norbert Bischof:Struktur und Bedeutung, 1998,ISBN 3-456-83080-7(Das Buch ist für Sozialwissenschaftler geschrieben und erklärt mathematische Zusammenhänge Nicht-Mathematikern in sehr verständlicher Weise.', 'deDas Kapitel 2 widmet sich der Informationstheorie.', 'de)Sven P. Thoms:Ursprung des Lebens.', 'deFischer, Frankfurt a. M. 2005,ISBN 3-596-16128-2(Das Buch ist aus biologischer und chemischer Perspektive geschrieben.', 'deEin Kapitel behandelt den Zusammenhang von Leben und Entropie.', 'de)Thomas Cover:Elements of Information Theory, Wiley-Interscience 2006,ISBN 0-471-24195-4(Das Buch ist nur auf Englisch erhältlich.', 'deEs behandelt ausführlich die Informationstheorie und ist mathematisch gehalten.', 'de)Martin Werner:Information und Codierung, Vieweg 2002,ISBN 3-528-03951-5Weblinks[Bearbeiten|Quelltext bearbeiten]Wikibooks: Zusammenhang zwischen Entropie und Information– Lern- und LehrmaterialienEinführung der Entropie als Gesamtzufallsmenge mit vielen Beispielen und Erklärungen zur Formel von ShannonEntropie und InformationOwen Maroney:Information Processing and Thermodynamic Entropy.In: Edward N. Zalta (Hrsg.', 'de):Stanford Encyclopedia of Philosophy.Shannon entropie Rechner (Text)(englisch)Shannon entropie Rechner (Binary)(englisch)Einzelnachweise[Bearbeiten|Quelltext bearbeiten]↑abKulturgeschichte der Physik, Károly Simonyi, Urania-Verlag, Leipzig 1990,ISBN 3-332-00254-6, S. 372.↑C.', 'deE. Shannon:A Mathematical Theory of Communication.', \"deIn:Bell System Technical Journal.Band27,Nr.3, 1948,S.379–423,doi:10.1002/j.1538-7305.1948.tb01338.x(PDF).↑Konkrete Ähnlichkeiten zwischen der Shannon'schenInformationsentropieund der thermodynamischen Entropie werden u. a. behandelt in: U. Krey, A. Owen,Basic Theoretical Physics – A Concise Overview, Berlin, Springer 2007,ISBN 978-3-540-36804-5und in: Arieh Ben-Naim:Statistical Thermodynamics Based on Information: A Farewell to Entropy.\", 'de2008,ISBN 978-981-270-707-9↑W.', 'deA. Kreiner:Thermodynamik und Informationstheorie – Deutungen und Bedeutungsunterschiede im Entropiebegriff.doi:10.18725/OPARU-4097Eine vergleichende GegenüberstellungAbgerufen von „https://de.wikipedia.org/w/index.php?title=Entropie_(Informationstheorie)&oldid=237345848“Kategorien:InformationKybernetikInformationstheorieNavigationsmenüMeine WerkzeugeNicht angemeldetDiskussionsseiteBeiträgeBenutzerkonto erstellenAnmeldenNamensräumeArtikelDiskussionDeutschAnsichtenLesenBearbeitenQuelltext bearbeitenVersionsgeschichteWeitereSucheNavigationHauptseiteThemenportaleZufälliger ArtikelMitmachenArtikel verbessernNeuen Artikel anlegenAutorenportalHilfeLetzte ÄnderungenKontaktSpendenWerkzeugeLinks auf diese SeiteÄnderungen an verlinkten SeitenSpezialseitenPermanenter LinkSeiten\\xad\\xadinformationenArtikel zitierenKurzlinkQR-Code herunterladenWikidata-DatenobjektDrucken/\\u200bexportierenBuch erstellenAls PDF herunterladenDruckversionIn anderen ProjektenCommonsIn anderen SprachenAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Links bearbeitenDiese Seite wurde zuletzt am 15.', 'deSeptember 2023 um 10:04 Uhr bearbeitet.Abrufstatistik·AutorenDer Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden.', 'deMöglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen.', 'deDurch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden.Wikipedia® ist eine eingetragene Marke der Wikimedia Foundation Inc.DatenschutzÜber WikipediaImpressumVerhaltenskodexEntwicklerStatistikenStellungnahme zu CookiesMobile Ansicht', \"frEntropie de Shannon — WikipédiaAller au contenuMenu principalMenu principaldéplacer vers la barre latéralemasquerNavigationAccueilPortails thématiquesArticle au hasardContactContribuerDébuter sur WikipédiaAideCommunautéModifications récentesFaire un donRechercherRechercherCréer un compteSe connecterOutils personnelsCréer un compteSe connecterPages pour les contributeurs déconnectésen savoir plusContributionsDiscussionSommairedéplacer vers la barre latéralemasquerDébut1Historique2Définition formelle3Justification de la formule4Cas de deux variables indépendantes5Exemples simplesAfficher / masquer la sous-section Exemples simples5.1Processus de Bernoulli5.2Tirage aléatoire dans une urne5.3Entropie d'un texte6Propriétés7Utilité pratique8Notes et références9Voir aussiAfficher / masquer la sous-section Voir aussi9.1Bibliographie9.2Articles connexesBasculer la table des matièresEntropie de Shannon45 languesAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Modifier les liensArticleDiscussionfrançaisLireModifierModifier le codeVoir l’historiqueOutilsOutilsdéplacer vers la barre latéralemasquerActionsLireModifierModifier le codeVoir l’historiqueGénéralPages liéesSuivi des pages liéesTéléverser un fichierPages spécialesLien permanentInformations sur la pageCiter cette pageObtenir l'URL raccourcieTélécharger le code QRÉlément WikidataImprimer / exporterCréer un livreTélécharger comme PDFVersion imprimableDans d’autres projetsWikimedia CommonsUn article de Wikipédia, l'encyclopédie libre.Pour les articles homonymes, voirEntropie.Enthéorie de l'information, l'entropie de Shannon, ou plus simplemententropie, est une fonction mathématique qui, intuitivement, correspond à la quantité d'informationcontenue ou fournie par une source d'information.\", \"frCette source peut être un texte écrit dans une langue donnée, unsignal électriqueou encore unfichier informatiquequelconque (suite d'octets).\", \"frElle a été introduite parClaude Shannon.Du point de vue d'un récepteur, plus la source émet d'informations différentes, plus l'entropie(ouincertitudesur ce que la source émet) est grande.\", \"frAinsi, si une source envoie toujours le même symbole, par exemple la lettre 'a', alors son entropie estnulle, c'est-à-dire minimale.\", \"frEn effet, un récepteur qui connaît seulement les statistiques de transmission de la source est assuré que le prochain symbole sera un 'a'.\", \"frPar contre, si la source envoie un 'a' la moitié du temps et un 'b' l'autre moitié, le récepteur est incertain de la prochaine lettre à recevoir.\", \"frL'entropie de la source dans ce cas est doncnon nulle(positive) et représente quantitativement l'incertitude qui règne sur l'information émanant de la source.\", \"frL'entropie indique alors la quantité d'information nécessaire pour que le récepteur puisse déterminer sans ambiguïté ce que la source a transmis.\", \"frPlus le récepteur reçoit d'information sur le message transmis, plus l'entropie (incertitude) vis-à-vis de ce message croît.\", \"frEn particulier, plus la source est redondante, moins elle contient d'information.\", \"frEn l'absence de contraintes particulières, l'entropie est maximale pour une source dont tous les symboles sont équiprobables.Historique[modifier|modifier le code]Au début des années 1940, les télécommunications étaient dominées par le modeanalogique.\", \"frLes sons et les images étaient transformés en signaux électriques dont l'amplitude et/ou la fréquence sont des fonctions continues du signal d'entrée.\", 'frUn bruit ajouté pendant la transmission résultait en une dégradation du signal reçu.', \"frL'archétype de ce type de bruit prend la forme de grésillement pour la radio et de neige pour la télévision.\", \"frAujourd'hui, les signaux sont également codés sous formenumérique.\", \"frUn bruit ajouté pendant la transmission se traduira par une erreur sur les données numériques transmises, se manifestant par exemple par l'apparition de pixels aberrants sur une image de télévision.\", \"frDans les deux cas, on souhaite d'une part transmettre le maximum de données en un minimum de temps sur un canal de transmission donné, d'autre part, on souhaite pouvoir corriger les altérations dues au bruit dans une limite donnée.En 1948, Claude Shannon, ingénieur en génie électrique auxLaboratoires Bell, formalisa mathématiquement la nature statistique de « l'information perdue » dans les signaux des lignes téléphoniques.\", \"frPour ce faire, il développa le concept général d'entropie de l'information, fondamental dans la théorie de l'information[1], ce qui lui permit d'évaluer la quantité d'information maximale qu'on pouvait transmettre dans un canal donné.\", \"frIl a également montré qu'en utilisant une stratégie de codage numérique adéquat, il était possible de transmettre les informations de façon que le récepteur soit en mesure de restaurer le message original bruité sans perte d'information, sous réserve de réduire la vitesse de transmission des informations.Initialement, il ne semble pas que Shannon ait été au courant de la relation étroite entre sa nouvelle mesure et les travaux précédents enthermodynamique.\", \"frLe termeentropiea été suggéré par le mathématicienJohn von Neumannpour la raison que cette notion ressemblait à celle déjà connue sous le nom d'entropie enphysique statistique.\", \"frIl aurait ajouté que ce terme était de plus assez mal compris pour pouvoir triompher dans tout débat[2].En1957,Edwin Thompson Jaynesdémontrera le lien formel existant entre l'entropiemacroscopique introduite parClausiusen 1847, l'entropie microscopique introduite parGibbs, et l'entropie mathématique de Shannon.\", \"frCette découverte fut qualifiée parMyron Tribusde « révolution passée inaperçue »[3].Le calcul de l'entropie d'une source de messages donne une mesure de l'information minimale que l'on doit conserver afin de représenter ces données sans perte.\", \"frEn termes communs, dans le cas particulier de la compression de fichiers en informatique, l'entropie indique le nombre minimal debitsque peut atteindre un fichier compressé.\", \"frEn pratique, l'entropie de l'image ou du son se voit davantage abaissée en retirant des détails imperceptibles pour les humains, comme lors de la compression des sons par le format MP3, des images par JPEG ou des vidéos par MPEG.Définition formelle[modifier|modifier le code]Pour une source, qui est unevariable aléatoirediscrèteXcomportantnsymboles, chaque symbolexiayant une probabilitéPid'apparaître, l'entropieHde la sourceXest définie comme :Hb(X)=−E[logb\\u2061P(X)]=∑i=1nPilogb\\u2061(1Pi)=−∑i=1nPilogb\\u2061Pi.\", 'fr(X)=-\\\\mathbb [\\\\log _]=\\\\sum _^P_\\\\log _\\\\left(}}\\\\right)=-\\\\sum _^P_\\\\log _P_.\\\\,\\\\!', \"fr}oùE }désigne l'espérance mathématique, etlogb}lelogarithmeen baseb.\", \"frOn utilise en général un logarithme à base 2 car l'entropie possède alors les unités debit/symbole.\", 'frLes symboles représentent les réalisations possibles de la variable aléatoireX.', \"frDans ce cas, on peut interpréterH(X)comme le nombre de questions à réponse oui/non que doit poser en moyenne le récepteur à la source, ou la quantité d'information en bits que la source doit fournir au récepteur pour que ce dernier puisse déterminer sans ambiguïté la valeur deX.H(X)=H2(X)=−∑i=1nPilog2\\u2061Pi.\", 'fr(X)=-\\\\sum _^P_\\\\log _P_.\\\\,\\\\!', \"fr}Si on dispose de deux variables aléatoiresXetY, on définit d'une façon analogue la quantitéH(X,Y), appelée l'entropie conjointe, des variablesXetY:H(X,Y)=−∑i,jP(X=xi,Y=yj)log2\\u2061P(X=xi,Y=yj)P(X=x_,Y=y_)\\\\log _P(X=x_,Y=y_)}ainsi que l'entropie conditionnelle[4]deYrelativement àX:H(Y|X)=−∑i,jP(X=xi,Y=yj)log2\\u2061P(Y=yj|X=xi)=∑iP(X=xi)(−∑jP(Y=yj|X=xi)log2\\u2061P(Y=yj|X=xi))P(X=x_,Y=y_)\\\\log _P(Y=y_\\\\,|\\\\,X=x_)=\\\\sum _P(X=x_)\\\\left(-\\\\sum _P(Y=y_\\\\,|\\\\,X=x_)\\\\log _P(Y=y_\\\\,|\\\\,X=x_)\\\\right)}Justification de la formule[modifier|modifier le code]Dans le cas où l'on dispose deNsymboles équiprobables avecN=2n},nentier, il suffit denquestions, en procédant pardichotomie, pour déterminer le symbole envoyé par la source.\", \"frDans ce cas, la quantité d'information contenue par le symbole est exactementn=log2\\u2061(N)(N)}(on doit divisernfois par 2 l'ensemble des possibilités pour qu'il n'en reste plus qu'une).\", \"frIl est naturel de conserver cette formule dans le cas oùNn'est pas unepuissance de 2.\", \"frPar exemple, si les symboles sont les lettres de l'alphabet ainsi que le symbole espace (soit 27 symboles), l'information contenue par un symbole estlog2\\u2061(27)≈4,75(27)\\\\approx 4,75}, valeur intermédiaire entre 4 bits (permettant de coder 16 symboles) et 5 bits (qui permet d'en coder 32).\", \"frCette définition de l'entropie dans le cas équiprobable est comparable à celle donnée en thermodynamique parBoltzmann.Supposons maintenant que lesNsymboles soient répartis ennsous-catégories, lai-ème catégorie étant constituée deNisymboles (avec doncN=N1+...+Nn+...+N_}).\", 'frPar exemple, les 27 caractères considérés précédemment peuvent être répartis en trois catégories, les voyelles, les consonnes et le caractère espace.', 'frSoit X la variable aléatoire donnant la catégorie du symbole considéré.', 'frPosonsPi=Ni/N=N_/N}la probabilité que le symbole considéré appartienne à lai-ème catégorie.', \"frLa détermination du symbole peut être effectuée en deux temps, d'abord celui de sa catégorie X, exigeant une quantité d'information H(X), puis, au sein de sa catégorie, la détermination du symbole.\", \"frSi la catégorie à laquelle appartient le symbole est lai-ème, cette dernière étape demande une quantité d'information égale àlog2\\u2061(Ni)(N_)}.\", \"frCette éventualité se produisant avec une probabilitéPi, la quantité moyenne d'information pour déterminer le symbole connaissant sa catégorie est∑i=1nPilog2\\u2061(Ni)^P_\\\\log _(N_)}.\", \"frLa quantité d'information totalelog2\\u2061(N)(N)}pour déterminer le symbole est donc la somme de la quantité H(X) pour déterminer sa catégorie, et de la quantité moyenne∑iPilog2\\u2061(Ni)P_\\\\log _(N_)}pour déterminer le symbole au sein de sa catégorie.\", \"frOn a donc :log2\\u2061(N)=H(X)+∑iPilog2\\u2061(Ni)(N)=H(X)+\\\\sum _P_\\\\log _(N_)}donc :H(X)=log2\\u2061(N)−∑iPilog2\\u2061(Ni)=−∑iPilog2\\u2061(Ni/N)=−∑iPilog2\\u2061(Pi)(N)-\\\\sum _P_\\\\log _(N_)=-\\\\sum _P_\\\\log _(N_/N)=-\\\\sum _P_\\\\log _(P_)}Par exemple, la quantité d'information de 4,75 bits pour déterminer un caractère parmi 27 se scinde en H(X) = 0,98 bits pour déterminer sa catégorie (voyelle,consonne, espace) auxquels s'ajoutent 3,77 bits en moyenne pour déterminer le caractère au sein de sa catégorie.Cas de deux variables indépendantes[modifier|modifier le code]On peut vérifier a posteriori la cohérence de cette définition avec la propriété d'additivité de l'entropie.\", 'frSoient deux variables aléatoires indépendantesXetY.', \"frOn s'attend à ce queH(X,Y)=H(X)+H(Y).\", \"frPar exemple, si (X,Y) représente la position d'un objet dans un tableau (X étant le numéro de ligne et Y le numéro de colonne), H(X,Y) est la quantité d'information nécessaire pour déterminer cette position.\", \"frC'est la somme de la quantité d'information H(X) pour déterminer son numéro de ligne et de la quantité d'information H(Y) pour déterminer son numéro de colonne.\", 'frOr, la probabilité duproduit cartésiende ces variables aléatoires est donnée par :P(X=x,Y=y)=P(X=x)P(Y=y)qui sera abrégé par la suite enP(x,y)=P(x)P(y).', \"frOn a alors :H(X,Y)=−∑x∈X∑y∈YP(x,y)log\\u2061P(x,y)=−∑x∈X∑y∈YP(x)P(y)log\\u2061[P(x)P(y)]=−∑x∈X∑y∈YP(x)P(y)[log\\u2061P(x)+log\\u2061P(y)]=−∑x∈X∑y∈YP(x)P(y)log\\u2061P(x)−∑y∈Y∑x∈XP(x)P(y)log\\u2061P(y)=−∑x∈XP(x)log\\u2061P(x)∑y∈YP(y)−∑y∈YP(y)log\\u2061P(y)∑x∈XP(x)=−∑x∈XP(x)log\\u2061P(x)−∑y∈YP(y)log\\u2061P(y)=H(X)+H(Y)H(X,Y)&=-\\\\sum _\\\\sum _P(x,y)\\\\log P(x,y)\\\\\\\\&=-\\\\sum _\\\\sum _P(x)P(y)\\\\log \\\\left[P(x)P(y)\\\\right]\\\\\\\\&=-\\\\sum _\\\\sum _P(x)P(y)\\\\left[\\\\log P(x)+\\\\log P(y)\\\\right]\\\\\\\\&=-\\\\sum _\\\\sum _P(x)P(y)\\\\log P(x)-\\\\sum _\\\\sum _P(x)P(y)\\\\log P(y)\\\\\\\\&=-\\\\sum _P(x)\\\\log P(x)\\\\sum _P(y)-\\\\sum _P(y)\\\\log P(y)\\\\sum _P(x)\\\\\\\\&=-\\\\sum _P(x)\\\\log P(x)-\\\\sum _P(y)\\\\log P(y)\\\\\\\\&=H(X)+H(Y)\\\\end}}comme attendu.Exemples simples[modifier|modifier le code]Processus de Bernoulli[modifier|modifier le code]EntropieΗ(X)d'un tirage de pile ou face, mesurée en bits, tracé par rapport au biais de la pièceP(X= 1), avecX= 1représentant un résultat face[5]:14–15Ici, l'entropie vaut au plus 1 bit, et communiquer le résultat d'un lancer de pièce (2 valeurs possibles) demandera une moyenne d'au plus 1 bit (valeur atteinte pour un tirage équilibré).\", \"frPour un tirage de dé (6 issues possibles), on aurait une entropie de log2(6) bits.Articles détaillés :Processus de BernoullietFonction entropie binaire.On lance une pièce à pile ou face en connaissant les probabilités des deux résultats ; le modèle classique est celui d'unprocessus de Bernoulli.L'entropie du résultat du lancer à venir est maximale pour une pièce équilibrée.\", \"frIl s'agit bien du cas d'incertitude maximale car c'est là qu'il est le plus difficile de prédire l'issue du lancer ; le résultat de chaque lancer de la pièce donne un bit complet d'information.\", \"frEn effetH(X)=−∑i=1np(xi)logb\\u2061p(xi)=−∑i=1212log2\\u206112=−∑i=1212⋅(−1)=1H(X)&=-\\\\sum _^)\\\\log _p(x_)}\\\\\\\\&=-\\\\sum _^}\\\\log _}}\\\\\\\\&=-\\\\sum _^}\\\\cdot (-1)}=1\\\\end}}Cependant, si on sait par avance que la pièce n'est pas équilibrée, mais que pile et face ont des probabilitéspetq, avecp≠q, alors l'incertitude est moindre.\", \"frÀ chaque tirage, un côté est plus susceptible de sortir que l'autre.\", \"frL'incertitude réduite est quantifiée dans une entropie moindre : en moyenne, chaque lancer délivre moins d'un bit d'information.\", \"frPar exemple, sip= 0,7, alorsH(X)=−plog2\\u2061(p)−qlog2\\u2061(q)=−0,7log2\\u2061(0,7)−0,3log2\\u2061(0,3)≈−0,7⋅(−0,515)−0,3⋅(−1,737)=0,8816<1H(X)&=-p\\\\log _(p)-q\\\\log _(q)\\\\\\\\&=-0,7\\\\log _(0,7)-0,3\\\\log _(0,3)\\\\\\\\&\\\\approx -0,7\\\\cdot (-0,515)-0,3\\\\cdot (-1,737)\\\\\\\\&=0,8816<1\\\\end}}L'équiprobabilité mène à une incertitude maximale et donc une entropie maximale.\", \"frL'entropie ne peut donc que décroitre en s'éloignant la valeur correspondant à l'équiprobabilité.\", \"frLes cas extrêmes correspondant à des pièces double-face ou double-pile, où l'incertitude, et donc l'entropie, est nulle, et chaque tirage donne 0 bit d'information puisque le résultat est connu et certain[5]:14–15.Tirage aléatoire dans une urne[modifier|modifier le code]Considérons une urne contenant une boule rouge, une boule bleue, une boule jaune et une boule verte.\", 'frOn tire une boule au hasard.', \"frIl s'agit de communiquer la couleur tirée.\", \"frAucun tirage n'étant privilégié, l'entropie est maximale, égale ici àlog2(4) = 2.\", \"frSi on convient que les couleurs sont codées respectivement 00, 01, 10, 11, l'information contenue dans le tirage correspond effectivement à 2 bits.Mais si une certaine couleur est plus représentée que les autres, alors l'entropie est légèrement réduite.\", \"frSupposons par exemple que l'urne contienne 4 boules rouges, 2 bleues, 1 jaune et 1 verte.\", \"frL'entropie est alors de 7/4.\", \"frEn effet,H(x)=−48log2\\u2061(48)−28log2\\u2061(28)−18log2\\u2061(18)−18log2\\u2061(18)=−log2\\u2061(1/2)2−log2\\u2061(1/4)4−log2\\u2061(1/8)8−log2\\u2061(1/8)8=log2\\u2061(2)2+log2\\u2061(4)4+log2\\u2061(8)8+log2\\u2061(8)8=12+24+38+38=74H(x)&=-}\\\\log _\\\\left(}\\\\right)-}\\\\log _\\\\left(}\\\\right)-}\\\\log _\\\\left(}\\\\right)-}\\\\log _\\\\left(}\\\\right)\\\\\\\\[3pt]&=-(1/2)}}-(1/4)}}-(1/8)}}-(1/8)}}\\\\\\\\[3pt]&=(2)}}+(4)}}+(8)}}+(8)}}\\\\\\\\[3pt]&=}+}+}+}=}\\\\end}}Si les couleurs sont codées respectivement 0 pour le rouge, 10 pour le bleu, 110 pour le jaune et 111 pour le vert[6], alors l'information sur la couleur tirée occupe 1 bit une fois sur deux, 2 bits une fois sur quatre et 3 bits une fois sur quatre, soit en moyenne 7/4 bits, correspondant à l'entropie calculée.Entropie d'un texte[modifier|modifier le code]Considérons un texte constitué d'une chaîne de lettres et d'espaces, soit 27 caractères.Si ces caractères sont équiprobables, l'entropie associée à chaque caractère estlog2\\u2061(27)=4,75…(27)=475\\\\ldots }, ce qui signifie qu'il faut entre 4 et 5 bits pour transmettre un caractère.\", \"frMais si le texte est exprimé dans unlangage natureltel que le français, comme la fréquence de certains caractères n'est pas très importante (ex : 'w'), tandis que d'autres sont très communs (ex : 'e'), l'entropie de chaque caractère n'est pas si élevée.\", \"frCompte tenu de la fréquence de chaque caractère, une estimation effectuée sur la langue anglaise par Shannon donne comme valeur de l'entropie environ 4,03[7].L'entropie est en fait encore plus faible, car il existe des corrélations entre un caractère et celui, voire ceux qui le précèdent.\", \"frDes expériences ont été menées afin d'estimer empiriquement cette entropie.\", 'frPar exemple, A dispose du texte et demande à B de le deviner lettre par lettre (espaces comprises).', \"frSi B devine correctement la lettre, on compte 1 et si B se trompe, on compte 4,75 (correspondant à l'entropie d'un caractère équiprobable, donnée plus haut).\", 'frOn obtient ainsi expérimentalement une entropie de 1,93 bits par lettre[8],[9].Enfin, laloi de Zipf(empirique)[10]amène à des considérations du même ordre, cette fois-ci pour les mots.', \"frD'après l'ouvrage de 1955Connaissance de l'électronique[11]une lettre dans une langue donnée représente dans la pratique 1,1bit-symbole(terminologie employée par cet ouvrage).\", 'frCetteredondanceexplique la facilité avec laquelle on peut briser plusieurs chiffrements de complexité moyenne si on dispose de leur algorithme, même sans connaître laclé de chiffrement.', \"frC'est elle aussi qui permet de retrouver le contenu d'un texte parlé ou écrit dont une grande partie est altérée pour une raison ou une autre.Propriétés[modifier|modifier le code]Voici quelques propriétés importantes de l'entropie de Shannon :H(X)≥0avec égalité si et seulement s'il existeitel queP(X=xi)=1)=1}H(X)=−∑ipilog\\u2061pi≤−∑ipilog\\u2061qip_\\\\log p_\\\\leq -\\\\sum _p_\\\\log q_}oùqi}est unedistribution de probabilitéquelconque sur la variable X (Inégalité de Gibbs).DémonstrationLa fonctionlogarithme naturel, étant concave, est bornée supérieurement par n'importe quelle droite tangente à son graphe.\", 'frEn particulier :ln\\u2061(z)≤z−1On a ainsi :∑ipiln\\u2061qipi≤∑ipi(qipi−1)=∑i(qi−pi)=∑iqi−∑ipi=1−1=0p_\\\\ln }}}\\\\leq \\\\sum _p_\\\\left(}}}-1\\\\right)=\\\\sum _(q_-p_)=\\\\sum _q_-\\\\sum _p_=1-1=0}donc ;∑ipiln\\u2061qipi≤0p_\\\\ln }}}\\\\leq 0}∑ipiln\\u2061qi≤∑ipiln\\u2061pip_\\\\ln q_\\\\leq \\\\sum _p_\\\\ln p_}La fonctionlogétant proportionnelle au logarithme naturel, avec un coefficient de proportionnalité positif, on a également :∑ipilog\\u2061qi≤∑ipilog\\u2061pi=−H(X)p_\\\\log q_\\\\leq \\\\sum _p_\\\\log p_=-H(X)}⇒H(X)≤−∑ipilog\\u2061qip_\\\\log q_}H(X)≤log2\\u2061(n)(n)}.', \"frLa quantitélog2\\u2061(n)(n)}est l'entropie maximale, correspondant à une distribution uniforme, c’est-à-dire quand tous les états ont la même probabilité.\", \"frL'entropie maximale augmente avec le nombre d'états possibles (ce qui traduit l'intuition que plus il y a de choix possibles, plus l'incertitude peut être grande).\", \"frCependant, cette augmentation de l'entropie n'est qu'une possibilité : l'entropie où beaucoup d'états sont possibles mais avec une très faible probabilité pour la plupart d'entre eux peut tout à fait être inférieure à l'entropie du pile ou face (le bit).\", \"frPar exemple, s'il y a 100 états, dont l'un probable à 99 % et les autres également improbables, l'entropie est de seulement 0,14 bit.DémonstrationOn applique l'inégalité de Gibbs avec la probabilité uniformeqi=1/n=1/n}, ce qui donne,compte tenu que∑ipi=1p_=1}:H(X)=−∑ipilog\\u2061pi≤−∑ipilog\\u2061qi=∑ipilog\\u2061(n)=log\\u2061(n)p_\\\\log p_\\\\leq -\\\\sum _p_\\\\log q_=\\\\sum _p_\\\\log(n)=\\\\log(n)}Elle estsymétrique:H(X,Y)=H(Y,X)Elle estcontinueH(X,Y)=H(X)+H(Y|X)H(X,Y)≤H(X)+H(Y)avec égalité si et seulement si les variables sont indépendantes.H(Y|X)≤H(Y)H(Z|X,Y)≤H(Z|X)H(X1,…,Xn)=H(X1)+H(X2|X1)+…+H(Xn|X1,…,Xn−1),\\\\ldots ,X_)=H(X_)+H(X_\\\\,|\\\\,X_)+\\\\ldots +H(X_\\\\,|\\\\,X_,\\\\ldots ,X_)}H(X1,…,Xn)≤∑i=1nH(Xi),\\\\ldots ,X_)\\\\leq \\\\sum _^H(X_)}Utilité pratique[modifier|modifier le code]L'entropie de Shannon est utilisée pour numériser une source en utilisant le minimum possible debitssans perte d'information.\", \"frSi le canal de transmission de l'information a une capacité de C bits par seconde et si les symboles qu'envoie la source ont une entropie H, alors la vitesse maximale de transmission des symboles est de C/H symboles par seconde, cette vitesse pouvant être approchée d'aussi près que l'on veut au moyen d'un système de codage adéquat des symboles.De plus, si du bruit brouille la transmission, la capacité C du canal de transmission diminue.\", 'frEn effet, des informations supplémentaires doivent être envoyées par la source afin que le récepteur puisse reconstituer le message sans erreur.', \"frCes informations occupent une place supplémentaire qui diminuent la capacité C. Soitpla probabilité qu'un bit 0 soit modifié en 1 et inversement.\", \"frLes informations supplémentaires envoyées par la source doivent permettre au récepteur de savoir si le bit envoyé est erroné (avec une probabilitép) ou s'il est correct (avec une probabilité 1 -p).\", \"frLa quantité d'information correspondante par bit est−plog2\\u2061(p)−(1−p)log2\\u2061(1−p)(p)-(1-p)\\\\log _(1-p)}.\", 'frLa capacité de transmission devient alorsC(1+plog2\\u2061(p)+(1−p)log2\\u2061(1−p))(p)+(1-p)\\\\log _(1-p))}.', \"frElle est nulle sip= 1/2, cas correspondant à un message totalement brouillé.L'entropie de Shannon permet aussi de quantifier le nombre minimum de bits sur lesquels on peut coder un fichier, mesurant ainsi les limites que peuvent espérer atteindre les algorithmes decompression sans pertecomme lecodage de Huffman, puis ultérieurement l'algorithmeLZH.\", \"frElle est également utilisée dans d'autres domaines, par exemple la sélection du meilleur point de vue d'un objet entrois dimensions[12].L'entropie de Shannon est utilisée également en imagerie (médicale ou spatiale) à la base de la théorie de l'information Mutuelle (Mutual Information (MI)).\", \"frElle permet notamment de recaler deux images différentes l'une sur l'autre en minimisant l'entropie des deux images.\", \"frEn pratique cela permet de comparer les scanners d'un patient A quelconque avec un patient de référence B. Enfin, en génétique, l'entropie de Shannon permet de repérer sur unchromosomeles portions d'ADN contenant le plus d'information.Notes et références[modifier|modifier le code](en)Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé«Entropy (information theory)»(voir la liste des auteurs).↑(en)Claude E.Shannon, «A mathematical theory of communication»,Bell System Technical Journal,vol.vol.\", 'fr27,\\u200ejuillet et octobre 1948,p.379-423 et 623-656(lire en ligne)↑M.', 'frTribus, E.C.', 'frMcIrvine, “Energy and information”, Scientific American, 224 (September 1971).↑La référence est dans(en)Myron Tribus, «Some Observations on Systems, Probability, Entropy and Management»↑Didier Dacunha-Castelle, Marie Duflo,Probabilités et statistiques, t.I, Masson (1982), p.19↑aetb(en)Thomas M. Cover et Joy A. Thomas,Elements of Information Theory, Hoboken, New Jersey,Wiley,1991, 748p.', \"fr(ISBN978-0-471-24195-9,lire en ligne)↑Les codes sont tels qu'aucun d'eux n'est préfixe d'un autre, de sorte qu'une suite de tirages avec remise puisse être communiquée sans ambiguïté pour le récepteur.↑Léon Brillouin,La science et la théorie de l'information, Masson (1959), réédité par Gabay (1988),p.23↑Léon Brillouin,La science et la théorie de l'information, Masson (1959), réédité par Gabay (1988),p.24↑La mesure dépend de la culture du récepteur.\", 'frUne phrase comme \"nous obtenons la série suivante rapidement convergente\" fournira un plus grand taux de réussite chez des mathématiciens que chez des non-mathématiciens.', \"frDe même, explique Brillouin, si on utilise d'autres vocabulaires très spécialisés comme médical, financier, politique, etc.↑ou de sa généralisation mathématique, laloi de Mandelbrot↑Editions du Tambourinaire, 1955↑(en)P.-P. Vàzquez, M. Feixas, M. Sbert, W. Heidrich,Viewpoint selection using viewpoint entropy, Proceedings of the Vision Modeling and Visualization Conference, 273-280, 2001.Voir aussi[modifier|modifier le code]Bibliographie[modifier|modifier le code]Elisabeth Gassiat,Entropie, compression et statistique: conférence Shannon 100 (26/10/2016)Léon Brillouin,La science et la théorie de l'information, Masson (1959), réédité par Gabay (1988)(en)Claude E. Shannon,[PDF]A mathematical theory of communication,Bell System Technical Journalet 623-656, juillet-octobre, 1948Articles connexes[modifier|modifier le code]Complexité de KolmogorovEntropie de RényiEntropie métriqueEntropie différentielleLoi de probabilité d'entropie maximaleInformation mutuelleThéorème du codage de sourceThéorie de l'informationPortail des mathématiquesPortail des télécommunicationsPortail de l'informatique théoriqueCe document provient de «https://fr.wikipedia.org/w/index.php?title=Entropie_de_Shannon&oldid=213960519».Catégories:Théorie de l'informationÉchelle logarithmiqueClaude ShannonCatégories cachées :Portail:Mathématiques/Articles liésPortail:Sciences/Articles liésProjet:Mathématiques/ArticlesPortail:Télécommunications/Articles liésPortail:Technologies/Articles liésPortail:Informatique théorique/Articles liésPortail:Informatique/Articles liésLa dernière modification de cette page a été faite le 4 avril 2024 à 16:39.Droit d'auteur: les textes sont disponibles souslicence Creative Commons attribution, partage dans les mêmes conditions; d’autres conditions peuvent s’appliquer.\", 'frVoyez lesconditions d’utilisationpour plus de détails, ainsi que lescrédits graphiques.', 'frEn cas de réutilisation des textes de cette page, voyezcomment citer les auteurs et mentionner la licence.Wikipedia® est une marque déposée de laWikimedia Foundation, Inc., organisation de bienfaisance régie par le paragraphe501(c)(3)du code fiscal des États-Unis.Politique de confidentialitéÀ propos de WikipédiaAvertissementsContactCode de conduiteDéveloppeursStatistiquesDéclaration sur les témoins (cookies)Version mobileActiver ou désactiver la limitation de largeur du contenu', 'heאנטרופיה (סטטיסטיקה) – ויקיפדיהלדלג לתוכןתפריט ראשיתפריט ראשיהעברה לסרגל הצדהסתרהניווטעמוד ראשיברוכים הבאיםשינויים אחרוניםערכים מומלציםפורטליםערך אקראיתרומה לוויקיפדיהקהילהשער הקהילהעזרהייעוץמזנוןכיכר העירחדשותלוח מודעותיצירת קשרספר אורחיםחיפושחיפושיצירת חשבוןכניסה לחשבוןכלים אישייםיצירת חשבוןכניסה לחשבוןדפים לעורכים שלא נכנסו לחשבוןמידע נוסףתרומותשיחהתוכן ענייניםהעברה לסרגל הצדהסתרההתחלה1הגדרה ואקסיומטיקה2דוגמה3שימושים4ראו גם5קישורים חיצוניים6הערות שולייםמצב תוכן הענייניםאנטרופיה (סטטיסטיקה)45 שפותEnglishAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEspañolEuskaraفارسیFrançaisGalegoMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語עריכת הקישוריםערךשיחהעבריתקריאהעריכת קוד מקורעריכהגרסאות קודמותכליםכליםהעברה לסרגל הצדהסתרהפעולותקריאהעריכת קוד מקורעריכהגרסאות קודמותכללידפים המקושרים לכאןשינויים בדפים המקושריםדפים מיוחדיםקישור קבועמידע על הדףציטוט הדף הזהקבלת כתובת מקוצרתהורדת קוד QRפריט ויקינתוניםהדפסה/יצואיצירת ספרהורדה כ־PDFגרסה להדפסהבמיזמים אחריםויקישיתוףמתוך ויקיפדיה, האנציקלופדיה החופשיתבסטטיסטיקהובתחומים נוספים, ובעיקר בתורת האינפורמציה,אנטרופיה(באנגלית:Entropy) היא מדד לגודלו האפקטיבי שלמרחב הסתברות.', 'heהאנטרופיה שלהתפלגות אחידה בדידהעל n מצבים היאlog2\\u2061nn}.', 'heאת מושג האנטרופיה המתמטית פיתח אבי תורת האינפורמציהקלוד שאנוןב־1948[1].לדוגמה,הטלת מטבעמחזירה אחת מבין שתי אפשרויות, והטלת קובייהמחזירה אחת מבין שש אפשרויות.', 'heברור שאת התוצאה של הטלת הקוביה קשה יותר לחזות מאשר את זו של המטבע.', 'heחיבור התוצאות של שתי קוביות מחזיר אחת מבין 11 אפשרויות, שבהן 7 היא השכיחה ביותר, ואילו 2 או 12 נדירות ביחס.', 'heכאן לא די לומר שגודל מרחב ההסתברות הוא 11 - ההסתברויות אינן אחידות, ולכן לא ניתן במבט ראשון לקבוע האם תוצאת החיבור קשה יותר לחיזוי מאשר, נאמר, בחירה של ספרה אקראית בין 1 ל־9 (בהתפלגות אחידה בדידה).', 'heהצורך להשוות באופן מדויק בין מרחבי התפלגות שונים קיים בכל תחומי המדע, ומדידת האנטרופיה באופן שיוצג להלן שכיחה בפיזיקה, בתורת האינפורמציהבביולוגיה(שם היא נקראתמדד שאנון-ויבר) ובתחומים נוספים.הגדרה ואקסיומטיקה[עריכת קוד מקור|עריכה]אם X הואמרחב הסתברותסופי, עםהסתברויותהבאותp1,…,pn,\\\\dots ,p_}המייצגות את המאורעות השונים במרחב, אזי האנטרופיה שלו מוגדרת לפי הנוסחהH(X)=−∑ipilog2\\u2061(pi)p_\\\\log _(p_)}זהו ערך חיובי, המקייםH(X)≤log2\\u2061(n)(n)}, עם שוויון רק כאשר ההסתברויות שוות כולן זו לזו.', 'heבמובן זה, האנטרופיה מייצגת את הלוגריתםשל גודל המרחב, ולא את הגודל עצמו.', 'heעל־פי אותה נוסחה בדיוק אפשר לחשב את האנטרופיה שלמשתנה מקרי(המקבל מספר סופי של ערכים).', 'heבשני המקרים, האנטרופיה אינה מתחשבת בטיבם של המאורעות השונים במרחב, אלא בהסתברות שהם יתרחשו.את \"מספר האפשרויות\" שמייצג X אפשר למדוד בדרכים נוספות, כגון ספירת מצבים נאיבית (n, במקרה שלנו),ממוצע הרמונישל ההסתברויות ((∑pi−1/n)−1^/n)^}), ועוד.ניתן להסתכל על מזווית קצת שונה.', 'heאם אנו מסתכלים על מספר מאורעות בלתי תלויים וזרים מתקיים השוויוןp(A1∪A2∪⋯∪An)=∑1np(n)\\\\cup A_\\\\cup \\\\dots \\\\cup A_)=\\\\sum _^p(n)}וכאשר ממשקלים את ההסתברות בערך המאורעות עצמם מתקבלת התוחלת של המשתנה המקריE\\u2061(X)=∑ixiP(X=xi) (X)=\\\\sum _x_P(X=x_)}מנקודת מבט זו ניתן לראות את התוחלת על משתנה מקרי i.i.d כמדד כללי שמייחס להסתברות שאחד מהמאורעות יקרה.A1∪A2∪⋯∪An\\\\cup A_\\\\cup \\\\dots \\\\cup A_}לעומת זאת אנטרופיה היא מדד להסתברות שכל המאורעות יקרו ביחדA1∩A2∩⋯∩An\\\\cap A_\\\\cap \\\\dots \\\\cap A_}וזאת בשל העובדה שפונקציית הלוגריתם הופכת כפל לסכום.H(X)=E(x)=∑(−p⋅ln\\u2061(p))=ln\\u2061(∏pi−pi)}]}})}שזהו למעשה ממוצע הנדסי משוקלל על ההסתברות.הסיבה לכך שמדד האנטרופיה נחשב למדד המתאים בהקשרים רבים כל־כך קשורה לכמה תכונות יסודיות שהוא מקיים.כדי שניתן יהיה להסביר תכונות אלה, עלינו להזכיר מושג יסודי אחר בסטטיסטיקה: התפלגות מותנית.', 'heאם X ו־Y שני משתנים מקריים, אז עבור כל ערך אפשרי y של Y, אפשר לבנות משתנה מקרי חדשX∣Y=y, \"המשתנה המותנה\", המייצג את הערכים שיכול לקבל X אם ידוע ש־Y קיבל את הערך y.', 'heכאשר הערך של Y אינו ידוע, מסמנים את המשתנה המותנה בסימוןX∣Y; זהו, אם כך, משתנה מקרי, שהתפלגותו המדויקת תלויה בערך שיקבל Y.פונקציית האנטרופיה H מקיימת את ארבע התכונות הבאות:אדיטיביות: אם X ו־Y שני משתנים מקרייםבלתי תלויים, אזH(X,Y)=H(X)+H(Y).', 'heבמילים אחרות, האנטרופיה שלמכפלה ישרהשל מרחבי התפלגות שווה לסכום האנטרופיות של שני המרחבים.פיצול: אם X משתנה מקרי ו־Y פונקציה של X, אזH(X)=H(X∣Y)+H(Y), כאשרH(X∣Y)מייצג את התוחלתשלH(X∣Y=y)במעבר על כל הערכים האפשריים של Y.רציפות: האנטרופיה שלהתפלגות ברנוליb(p)היאפונקציה רציפהשל p.נורמליות: האנטרופיה של ההתפלגות האחידה על שני מצבים, היא 1.משפט: פונקציית האנטרופיה H היא הפונקציה היחידה המקיימת את ארבע התכונות לעיל.הוכחה: נניח ש-H היא פונקציה המוגדרת על משתנים מקריים, ומקיימת את תכונות האדיטיביות, הפיצול, הרציפות והנורמליות.', 'heראשית נחשב אתH(p), שהוא הערך של H במשתנה ברנוליb(p).', 'heיהיוX,Y∼b(p)משתני ברנולי בלתי תלויים.', 'heנסמן ב-Z את המשתנה המוגדר להיות 0 אם X=Y ו-1 אחרת.', 'heZ הוא פונקציה של הזוג הסדור (X,Y), ולפיאקסיומותהפיצול והאדיטיביות,2H(p)=H(X)+H(Y)=H(X,Y)=H(X,Y|Z)+H(Z).', 'heאבל Z עצמו מתפלג ברנולי, עם הסתברות 2pq להיות 1 (כאשר q=1-p).', 'heלפי ההגדרה,H(X,Y|Z)=(p2+q2)H(p2p2+q2)+2pqH(12).+q^)H\\\\left(}+q^}}\\\\right)+2pqH\\\\left(}\\\\right).', 'he}, ולכן2H(p)=(p2+q2)H(p2p2+q2)+2pqH(12)+H(2pq)+q^)H\\\\left(}+q^}}\\\\right)+2pqH\\\\left(}\\\\right)+H\\\\left(2pq\\\\right)}.', 'heזוהימשוואה פונקציונלית, שהפתרון היחיד שלה הואH(p)=−plog2\\u2061p−qlog2\\u2061qp-q\\\\log _q}.', 'heעבור משתנה המקבל n ערכים, אפשר לחשב את H באינדוקציה, על ידי התניה בקבלת הערך האחרון:H(p1,…,pn)=(1−pn)H(p11−pn,…,pn−11−pn)+H(pn),\\\\dots ,p_)=(1-p_)H(}}},\\\\dots ,}}})+H(p_)}.דוגמה[עריכת קוד מקור|עריכה]נפתור את הדוגמה שבפתיח (מה יותר קשה לחיזוי - סכום הטלת שתי קוביות או התפלגות אחידה בין 9 תוצאות): למרחב אחיד בגודל 9 יש 9 תוצאות שלכל אחת מהן הסתברות19}, ולכן האנטרופיה היא9⋅(−19log2\\u2061(19))=log2\\u2061(9)≈3.17\\\\log _\\\\left(\\\\right))=\\\\log _(9)\\\\approx 3.17}, ואילו האנטרופיה של מרחב התוצאות האפשריות של סכום שתי קוביות היא−∑pilog2\\u2061(pi)=−[2⋅(136log2\\u2061(136))+2⋅(118log2\\u2061(118))+2⋅(112log2\\u2061(112))+2⋅(19log2\\u2061(19))+2⋅(536log2\\u2061(536))+(16log2\\u2061(16))]≈3.27.\\\\ -\\\\sum p_\\\\log _(p_)=-\\\\left[2\\\\cdot \\\\left(\\\\log _\\\\left(\\\\right)\\\\right)+2\\\\cdot \\\\left(\\\\log _\\\\left(\\\\right)\\\\right)+2\\\\cdot \\\\left(\\\\log _\\\\left(\\\\right)\\\\right)+2\\\\cdot \\\\left(\\\\log _\\\\left(\\\\right)\\\\right)\\\\right.+\\\\\\\\\\\\left.2\\\\cdot \\\\left(\\\\log _\\\\left(\\\\right)\\\\right)+\\\\left(\\\\log _\\\\left(\\\\right)\\\\right)\\\\right]\\\\approx 3.27.\\\\end}}כלומר, מעט קשה יותר לחזות את התוצאה של סכום שתי קוביות מאשר את התוצאה של בחירה אקראית מתוך 9 אפשרויות.שימושים[עריכת קוד מקור|עריכה]לאנטרופיה של שאנון קשר הדוק ליכולתלדחוס אינפורמציהוליכולת ללמוד מהאינפורמציה באמצעותאלגוריתמיםשללמידת מכונה.', 'heמושגים נוספים הקשורים לאנטרופיה קשר הדוק הםאינפורמציה הדדיתואנטרופיה מותנית.', \"heלאנטרופיה יש גם קשר עמוק למושגסיבוכיות קולמוגורוב.ראו גם[עריכת קוד מקור|עריכה]אנטרופיהמרחק לוינשטייןמכניקה סטטיסטיתתאוריהעקרון גידול האנטרופיה•ergodic theoryתרמודינמיקה סטטיסטיתצברים•פונקציית חלוקה•משוואות מצב•פוטנציאלים תרמודינמיים: (U•H•F•\\u200fG) •קשרי מקסוולמודל סטטיסטיFerromagnetism models(איזינג•פוטס•הייזנברג•חלחולEN) • חלקיקים בעלישדה כוחות(כוחות דלדולEN•פוטנציאל לנארד-ג'ונס)גישות מתמטיותמשוואת בולצמן•משפט־H•משוואת ולסוב•מדרג BBGKY•תהליך סטוכסטי•תורת שדה ממוצעותורת השדות הקונפורמיתתופעות קריטיותמעבר פאזה•אקספוננט קריטי(מרחק קורלציה•size scaling)אנטרופיהבולצמן•שאנון•צאליס•רניי•פון נוימןיישומיםתורת השדות הסטטיסטית(חלקיקים יסודיים•נוזלי־על) •פיזיקה של חומר מעובה•מערכות מורכבות(כאוס•תורת האינפורמציה•אנטרופיה בתרמודינמיקה ובתורת האינפורמציה•מכונת בולצמן)קישורים חיצוניים[עריכת קוד מקור|עריכה]מדיה וקבצים בנושאאנטרופיהבוויקישיתוףאנטרופיה (סטטיסטיקה), באתראנציקלופדיה למתמטיקה(באנגלית)אנטרופיה, באתרMathWorld(באנגלית)אנטרופיה (תורת האינפורמציה), דף שער בספרייה הלאומיתהערות שוליים[עריכת קוד מקור|עריכה]^C.E.\", 'heShannon, \"A Mathematical Theory of Communication\",Bell System Technical Journal, vol.', 'he27, pp.', 'he379–423, 623-656, July, October, 1948בקרת זהויותNLI:987007550784405171BNE:XX535116BnF:cb11985913j(data)GND:4743861-7LCCN:sh85044152NDL:01191172NKC:ph425914אוחזר מתוך \"https://he.wikipedia.org/w/index.php?title=אנטרופיה_(סטטיסטיקה)&oldid=37721859\"קטגוריות:סטטיסטיקהתורת ההסתברותתורת האינפורמציהקטגוריות מוסתרות:ויקיפדיה: ערכים עם מזהה J9Uויקיפדיה: ערכים עם מזהה BNEויקיפדיה: ערכים עם מזהה BNFויקיפדיה: ערכים עם מזהה GNDויקיפדיה: ערכים עם מזהה LCCNויקיפדיה: ערכים עם מזהה NDLויקיפדיה: ערכים עם מזהה NKCערכים שבהם תבנית בריטניקה אינה מתאימהדף זה נערך לאחרונה ב־24 בדצמבר 2023, בשעה 20:42.הטקסט מוגש בכפוף לרישיוןCreative Commons ייחוס-שיתוף זהה 4.0; ייתכן שישנם תנאים נוספים.', 'heר׳ אתתנאי השימושלפרטים.מדיניות פרטיותאודות ויקיפדיההבהרות משפטיותקוד התנהגותמפתחיםסטטיסטיקותהצהרה על עוגיותתצוגת מכשירים ניידיםהחלפת מצב רוחב תוכן מוגבל', 'slEntropija (informatika) - Wikipedija, prosta enciklopedijaPojdi na vsebinoGlavni meniGlavni meniprestavi v stransko letvicoskrijNavigacijaGlavna stranNaučite se urejatiIzbrani člankiNaključna stranZadnje spremembeSkupnostPomočPod lipoPortal skupnostiStik z namiDenarni prispevkiIskanjeIščiUstvari računPrijavaOsebna orodjaUstvari računPrijavaStrani za neprijavljene urejevalceveč o temPrispevkiPogovorna stran[opusti]Do 31. maja lahko sodelujete v natečajuWikimedia CEE Pomlad 2024.Vsebinaprestavi v stransko letvicoskrijUvod1Primer meta kovanca2Zunanje povezave3Glej tudiVklopi kazalo vsebineEntropija (informatika)45 jezikovAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Uredi povezaveStranPogovorslovenščinaPreberiUredi stranUredi kodoZgodovinaOrodjaOrodjaprestavi v stransko letvicoskrijDejanjaPreberiUredi stranUredi kodoZgodovinaSplošnoKaj se povezuje semPovezane spremembePosebne straniTrajna povezavaPodatki o straniNavedba člankaPridobi skrajšani URLPrenesi kodo QRPredmet v WikipodatkihRazširi vseUredi medjezikovne povezaveTiskanje/izvozUstvari e-knjigoPrenesi kot PDFRazličica za tiskV drugih projektihWikimedijina zbirkaIz Wikipedije, proste enciklopedijeZa druge pomene glejEntropija.EntropijaaliShannonova entropijaje vinformatikikoličina, ki merinegotovostizida poskusa povezanega sslučajno spremenljivko.', 'slTa vrsta entropije določa zapričakovano vrednostkoličinoinformacije, ki jo pridobimo takrat, ko izvedemo poskus, in dobimo njeno vrednost.', 'slTa entropija je torej merilo za količino informacije, ki jo dobimo s poznavanjem vrednosti slučajne spremenljivke.Označimo jo sH }.', 'slMerimo pa jo vbitih.Primer: met kovanca ima entropijo enega bita.', 'slSeveda v primeru, da kovanec ni pravilen, je negotovost manjša in s tem tudi entropija nižja.Uvedel jo jeameriškielektrotehnikinmatematikClaude Elwood Shannon(1916 – 2001) v letu1948.Če je X slučajna spremenljivka, ki lahko zavzame diskretne vrednosti x1, x2, x3,….', 'slxn, potem je Shannonova entropija zanjo enakaH(X)=∑i=1np(xi)I(xi)=−∑i=1np(xi)logb\\u2061p(xi),^)\\\\,I(x_)}=-\\\\sum _^)\\\\log _p(x_)},}kjer je b osnovalogaritma(kadar je enota za entropijobit, je b enak 2).Entropija pri metu kovanca v odvisnosti od pravilnosti kovanca (Pr(X=1).', 'slFunkcija kaže maksimum.', 'slNajvečja je entropija pri verjetnosti 0,5.', 'slPopolnoma nepravilni kovanec (na obeh stranehglavaalištevilka) ima entropijo enako 0 (znan je izid naslednjega meta).Povezava med entropijo in številom meta kovanca.Primer meta kovanca[uredi|uredi kodo]Pri metu kovanca sta enako verjetna dva izidaglavainštevilka.', 'slČe je entropija merilo za negotovost, potem je očitno, da mora graf entropije imeti tudi maksimum.Za posamezne poskuse velja P(X=x0) = ½ (glava) oziroma p0in P(X=x1) = ½ (številka) oziroma p1Iz tega dobimo po definiciji entropije zanjo vrednost H = 1 bit.', 'slTo pomeni, da nam vsak izid meta kovanca da 1 bit informacije.Primer je popolnoma drugačen, če kovanec ni pravilen (vsak izid poskusa ni enako verjeten).', 'slRecimo, da je verjetnost 60% (p0) da padeglavain 40% (p1) da padeštevilka.', 'slVelja p0+ p1= 1.', 'slEntropija se izračuna po zgornjem obrazcu na naslednji načinH=−(p⋅log2\\u2061p+q⋅log2\\u2061q)p+q\\\\cdot \\\\log _q)}kjer je q enak 1-p. V tem primeru imamo večjo nesigurnost v izid poskusa.', 'slS tem pa tudi zmanjšano entropijo oziroma vsak met kovanca nam da manj kot 1 bit informacije.', 'slČe pa bi imel kovanec na obeh stranehglavo(popolnoma nepravilni kovanec), bi bila entropija nič.', 'slizid meta kovanca nam ne bi dal nobene informacije, ker je izid poskusa vedno znan oziroma enak.Za vsako verjetnost lahko izračunamo entropijo iz zgornjega izraza.', 'slProblem nastopi pri vrednosti p = 0, kjer dobimo za entropijo vrednost nič.', 'slV ostalem delu pa je graf funkcije entropija/verjetnost simetrična.Če vržemo kovanec dvakrat, imamo štiri možne izida poskusa.', 'slVerjetnost vsakega izida pa je 0,25.', 'slPri 20 zaporednih metih je verjetnost 20 bitov.Zunanje povezave[uredi|uredi kodo]Entropija in informacija(rusko)Opis entropije v informatikiArhivirano2011-05-15 naWayback Machine.', 'sl(angleško)Informatika in entropijaArhivirano2009-05-21 naWayback Machine.', 'sl(angleško)Glej tudi[uredi|uredi kodo]klasična entropijastatistična entropijaStrniNormativna kontrolaNarodne knjižniceŠpanijaFrancija(data)NemčijaIzraelZDAJaponskaČeška republikaDrugoFaceted Application of Subject TerminologyPridobljeno iz »https://sl.wikipedia.org/w/index.php?title=Entropija_(informatika)&oldid=5799974«Kategorije:StatistikaStatistične teorijeEntropijaTeorija informacijSkrite kategorije:Predloga Webarchive z wayback linkiWikipedijini članki z identifikatorji BNEWikipedijini članki z identifikatorji BNFWikipedijini članki z identifikatorji GNDWikipedijini članki z identifikatorji J9UWikipedijini članki z identifikatorji LCCNWikipedijini članki z identifikatorji NDLWikipedijini članki z identifikatorji NKCWikipedijini članki z identifikatorji FASTČas zadnje spremembe strani: 15:29, 10. oktober 2022.Besedilo se sme prosto uporabljati v skladu z dovoljenjemCreative Commons Priznanje avtorstva-Deljenje pod enakimi pogoji 4.0; uveljavljajo se lahko dodatni pogoji.', 'slZa podrobnosti glejPogoje uporabe.Wikipedia® je tržna znamka neprofitne organizacijeWikimedia Foundation Inc.Pravilnik o zasebnostiO WikipedijiZavrnitve odgovornostiKodeks ravnanjaRazvijalciStatistikaO piškotkihMobilni prikazUredi nastavitve predogledaPreklop omejene širine vsebine', 'svEntropi (informationsteori) – WikipediaHoppa till innehålletHuvudmenyHuvudmenyflytta till sidofältetdöljNavigeringHuvudsidaIntroduktionDeltagarportalenBybrunnenSenaste ändringarnaSlumpartikelLadda upp filerStöd WikipediaKontakta WikipediaHjälpPå andra projektCommonsSkriv ut/exporteraSkapa en bokLadda ned som PDFUtskriftsvänlig versionSökSökSkapa kontoLogga inPersonliga verktygSkapa kontoLogga inSidor för utloggade redigerareläs merBidragDiskussionInnehållflytta till sidofältetdöljInledning1Se ävenVäxla innehållsförteckningenEntropi (informationsteori)45 språkAfrikaansالعربيةБългарскиBoarischBosanskiCatalàČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalego한국어Bahasa IndonesiaItalianoעבריתLietuviųMagyarNederlands日本語Олык марийਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийShqipSimple EnglishSlovenčinaSlovenščinaکوردیСрпски / srpskiSundaไทยTürkçeУкраїнськаاردوTiếng Việt粵語中文Redigera länkarArtikelDiskussionsvenskaLäsRedigeraRedigera wikitextVisa historikVerktygVerktygflytta till sidofältetdöljÅtgärderLäsRedigeraRedigera wikitextVisa historikAllmäntSidor som länkar hitRelaterade ändringarSpecialsidorPermanent länkSidinformationAnvänd som referensHämta förkortad urlLadda ner QR-kodWikidata-objektFrån WikipediaDen här artikelnbehöverkällhänvisningarför att kunnaverifieras.', 'sv(2020-11)Åtgärda genom att lägga till pålitliga källor (gärna som fotnoter).', 'svUppgifter utan källhänvisning kanifrågasättasoch tas bort utan att det behöver diskuteras pådiskussionssidan.Entropiär ett begrepp inominformationsteorin, definierat avClaude Shannon1948, för en informationskälla som genererarsymboler.', 'svBaserat påsannolikhetenför varje symbol definieras entropin över helasannolikhetsfördelningensom:H=−∑ipilog\\u2061pip_\\\\log p_\\\\,}Begreppet definierades utifrån behovet att beräkna kapaciteten hos kommunikationskanaler, och grundar sig påstokastiska sannolikheter.Definitionen är skapad som en analogi till den mikroskopiska definitionen av dentermodynamiskastorhetenentropi.Se även[redigera|redigera wikitext]informationinformationsteorisignalteoriHämtad från ”https://sv.wikipedia.org/w/index.php?title=Entropi_(informationsteori)&oldid=48406219”Kategorier:InformationsteoriSignalbehandlingDolda kategorier:Artiklar som behöver källor 2020-11Alla artiklar märkta med mallen källorAlla artiklar som behöver källorSidan redigerades senast den 1 november 2020 kl.', 'sv01.17.Wikipedias text är tillgänglig under licensenCreative Commons Erkännande-dela-lika 4.0 Unported.', 'svFör bilder, se respektive bildsida (klicka på bilden).', 'svSe vidareWikipedia:Upphovsrättochanvändarvillkor.Wikimedias integritetspolicyOm WikipediaFörbehållUppförandekodUtvecklareStatistikInformation om kakorMobilvyAktivera/inaktivera begränsad bredd på innehåll', 'ukІнформаційна ентропія — ВікіпедіяІнформаційна ентропіяМатеріал з Вікіпедії — вільної енциклопедії.Перейти до навігаціїПерейти до пошукуТеорія інформаціїЕнтропіяДиференціальна ентропіяУмовна ентропіяСпільна ентропіяВзаємна інформаціяУмовна взаємна інформація[en]Відносна ентропіяЕнтропійна швидкістьВластивість асимптотичної рівнорозподіленості[en]Швидкість–спотворення[en]Теорема Шеннона про кодування джерела[en]Пропускна здатність каналуТеорема про кодування для зашумленого каналу[en]Теорема Шеннона — ГартліАлгоритмічна теорія інформаціїпор\\ufeffУ Вікіпедії є статті про інші значення цього терміна:Ентропія (значення).Ця статтямає кілька недоліків.', 'ukБудь ласка, допоможітьудосконалити їїабо обговоріть ці проблеми насторінці обговорення.Цю статтюнаписано занадто професійнимстилемзі специфічною термінологією, що може бути незрозумілим для більшості читачів.Ви можете допомогтивдосконалитицю статтю, зробивши її зрозумілою для неспеціалістів без втрат змісту.', 'ukМожливо,сторінка обговореннямістить зауваження щодо потрібних змін.', 'uk(грудень 2018)Ця статтяпотребує додатковихпосилань на джереладля поліпшення їїперевірності.Будь ласка, допоможітьудосконалити цю статтю, додавши посилання нанадійні (авторитетні) джерела.', 'ukЗверніться насторінку обговоренняза поясненнями та допоможіть виправити недоліки.Матеріал без джерел може бутипіддано сумнівута вилучено.', 'uk(травень 2016)Два біти ентропії: у випадку двох справедливих підкидань монети, ентропія інформації в бітах є логарифмом за основою 2 з числа можливих результатів; за двох монет існує чотири можливі результати, та два біти ентропії.', \"ukВзагалі, інформаційна ентропія є усередненою кількістю інформації, що несе подія, за розгляду всіх можливих результатів.Інформаці́йна ентропі́я(англ.information entropy) — цеусередненашвидкість, із якою продукуєінформаціюстохастичнеджерело даних.Мірою інформаційної ентропії, пов'язаною з кожним можливим значенням даних, є від'ємнийлогарифмфункції маси ймовірностідля цього значення:S=−∑iPiln\\u2061PiP_\\\\ln }}[1].\", 'ukТаким чином, коли джерело даних має менш імовірне значення (наприклад, коли стається низькоймовірна подія), то ця подія несе більше «інформації» («неочікуваності»), ніж коли джерело даних має більш імовірне значення.', 'ukВизначена таким чином кількість інформації, що передається кожною подією, стаєвипадковою змінною, чиє математичне сподівання є інформаційною ентропією.', 'ukВзагалі,ентропіюпозначають безлад або невизначеність, і визначення ентропії, що застосовують в теорії інформації, є прямим аналогомвизначення, що застосовуютьустатистичній термодинаміці.', \"ukПоняття інформаційної ентропії було введеноКлодом Шенноному його праці 1948 року«Математична теорія зв'язку».\", \"uk[2]Базова модель системипередавання данихскладається з трьох елементів: джерела даних,каналу зв'язкута приймача, і, за виразом Шеннона, «фундаментальною задачею зв'язку» є те, щоби отримувач був здатен встановлювати, які дані було породжено джерелом, на основі сигналу, що він отримує каналом.\", \"uk[3]:379–423 та 623–656Ентропія забезпечує абсолютну межу найкоротшої можливої усередненої довжинибезвтратногостиснювальногокодування даних, продукованих джерелом, і якщо ентропія джерела є меншою запропускну спроможністьканалу зв'язку, то дані, породжувані цим джерелом, можливо надійно передавати приймачеві (принаймні теоретично, можливо, нехтуючи деякими практичними міркуваннями, такими як складність системи, потрібної для передавання даних, або кількості часу, що це передавання цих даних може забирати).Інформаційну ентропію зазвичай вимірюють вбітах(що також називають«шеннонами»,англ.bit, shannon), або іноді в «натуральних одиницях» (натах,англ.nat), або в десяткових цифрах (що називають «дитами», «банами» або«гартлі»).\", 'ukОдиниця вимірювання залежить від основи логарифма, що використовують для визначення ентропії.Логарифм розподілу ймовірності є корисним як міра ентропії тому, що для незалежних джерел він є адитивним.', 'ukНаприклад, ентропія підкидання справедливої монети складає 1 біт, а ентропієюmпідкидань єmбіт.', 'ukУ простому поданні, для представлення величини, що може набувати одного зnзначень, потрібноlog2(n)бітів, якщоnє степенем числа 2.', 'ukЯкщо ці значення є однаково ймовірними, то ентропія (в бітах) дорівнює цьому числу.', 'ukЯкщо ж трапляння одного з цих значень є ймовірнішим за інші, то спостереження трапляння цього значення є менш інформативним, ніж якби трапився не такий звичайний результат.', 'ukІ навпаки, рідкісніші події при їхньому спостереженні надають більше інформації.', 'ukОскільки спостереження менш імовірних подій трапляється рідше, в підсумку виходить, що ентропія (при розгляданні її як усередненої інформації), отримувана від нерівномірно розподілених даних, є завжди меншою або рівною доlog2(n).', 'ukЯкщо вихід є незмінним, то ентропія дорівнює нулеві.', 'ukКоли розподіл імовірності джерела даних є відомим, ентропія виражає ці міркування кількісно.Змістспостережуваних подій (змістповідомлень) у визначенні ентропії значення не має.', 'ukЕнтропія враховує лише ймовірність спостереження певної події, тому інформація, яку вона включає, є інформацією про розподіл ймовірності, що лежить в основі, а не про зміст самих подій.Зміст1Введення2Означення3Приклад4Обґрунтування5Аспекти5.1Відношення до термодинамічної ентропії5.2Ентропія як кількість інфромації5.3Ентропія як міра різноманітності5.4Стиснення даних5.5Світовий технологічний потенціал для зберігання та передавання інформації5.6Обмеження ентропії як кількості інформації5.7Обмеження ентропії в криптографії5.8Дані як марковський процес5.9b-арна ентропія6Ефективність7Характеризування7.1Неперервність7.2Симетричність7.3Максимум7.4Адитивність8Додаткові властивості9Розширення дискретної ентропії до неперервного випадку9.1Диференціальна ентропія9.2Взяття границі щільності дискретних точок9.3Відносна ентропія10Застосування в комбінаториці10.1Нерівність Люміса — Уїтні10.2Наближення до біноміального коефіцієнту11Див.', 'ukтакож12Примітки13Література13.1Підручники з теорії інформації14ПосиланняВведення[ред.|ред.', 'ukкод]Основною ідеєю теорії інформації є те, що чим більше хтось знає про предмет, то менше нової інформації про нього вони здатні отримати.', 'ukЯкщо подія є дуже ймовірною, то коли вона трапляється, в цьому немає нічого неочікуваного, і відтак вона привносить мало нової інформації.', 'ukІ навпаки, якщо подія була малоймовірною, то її трапляння є набагато інформативнішим.', 'ukТаким чином,інформаційний вмістєвисхідною функцієюоберненої ймовірності події (1/p, деpє ймовірністю події).', 'ukТепер, якщо може трапитися більше подій, ентропія вимірює усереднений інформаційний вміст, який ви можете очікувати отримати, якщо дійсно трапляється одна з цих подій.', 'ukЗ цього випливає, що викидання грального кубика має більше ентропії, ніж підкидання монети, оскільки кожен з результатів грального кубика має меншу ймовірність, ніж кожен з результатів монети.Отже, ентропія — це міранепередбачуваності(англ.unpredictability) стану, або, рівнозначно, йогоусередненого інформаційного вмісту(англ.average information content).', 'ukЩоби отримати інтуїтивне розуміння цих термінів, розгляньмо приклад політичного опитування.', 'ukЯк правило, такі опитування відбуваються тому, що їхні результати не є заздалегідь відомими.', \"ukІншими словами, результати опитування є відноснонепередбачуваними, і фактичне здійснення опитування та з'ясовування результатів дає деяку новуінформацію; це просто різні способи сказати, що апріорна ентропія результатів опитування є високою.\", 'ukТепер розгляньмо випадок, коли таке ж опитування проводиться вдруге незабаром після першого опитування.', 'ukОскільки результати першого опитування вже є відомими, підсумки другого опитування можна передбачити добре, і ці результати не повинні містити багато нової інформації; в цьому випадку апріорна ентропія результатів другого опитування є низькою по відношенню до апріорної ентропії першого.Тепер розгляньмо приклад з підкиданням монети.', 'ukЯкщо ймовірність випадання аверсу така ж, як і ймовірність випадання реверсу, то ентропія підкидання монети є настільки високою, наскільки це можливо.', 'ukПричиною цього є те, що немає жодного способу передбачити результат підкидання монети заздалегідь — якщо нам треба було би обирати, то найкраще, що ми могли би зробити, це передбачити, що монета випаде аверсом догори, і цей прогноз був би правильним з імовірністю 1/2.', \"ukТаке підкидання монети має один біт ентропії, оскільки є два можливі результати, які трапляються з однаковою ймовірністю, і з'ясування фактичного результату містить один біт інформації.\", 'ukНа противагу до цього, підкидання монети, яка має два аверси й жодного реверсу, має нульову ентропію, оскільки монета завжди випадатиме аверсом, і результат може бути передбачено цілком.', 'ukАналогічно, бінарна подія з рівноймовірними результатами має ентропію Шеннонаlog2\\u20612=12=1}біт.', 'ukПодібно до цього, одинтритіз рівноймовірними значеннями міститьlog2\\u206133}(близько 1.58496) бітів інформації, оскільки він може мати одне з трьох значень.Англомовний текст, якщо розглядати його як стрічку символів, має досить низький рівень ентропії, тобто, він є доволі передбачуваним.', 'ukНавіть якщо ми не знаємо точно, що буде далі, ми можемо бути доволі впевненими, що, наприклад, «e» там буде набагато поширенішою за «z», що поєднання «qu» зустрічатиметься набагато частіше за будь-які інші поєднання, які містять «q», і що поєднання «th» буде набагато поширенішим за «z», «q» або «qu».', 'ukПісля перших кількох літер часто можна вгадати решту слова.', 'ukАнглійський текст має між 0.6 та 1.3 біта ентропії на символ повідомлення.', 'uk[4]:234Якщо схемастисненняє вільною від втрат, — тобто, завжди можна відтворити повне первинне повідомлення шляхом розтискання, — то стиснене повідомлення має такий самий обсяг інформації, як і первинне, але переданий меншою кількістю символів.', 'ukТобто, воно має більше інформації, або вищу ентропію, на один символ.', 'ukЦе означає, що стиснене повідомлення має меншу надмірність.', 'ukГрубо кажучи,теорема Шеннона про кодування джерела[en]стверджує, що схема стиснення без втрат в середньому не може стиснути повідомлення так, щоби воно малобільшеодного біту інформації на біт повідомлення, але що будь-яке значенняменшеодного біту інформації на біт повідомлення може бути досягнуто шляхом застосування відповідної схеми кодування.', 'ukЕнтропія повідомлення на біт, помножена на довжину цього повідомлення, є мірою того, наскільки багато інформації в цілому містить повідомлення.Для наочності уявіть, що ми хочемо передати послідовність, до складу якої входять 4 символи, «А», «Б», «В» і «Г».', 'ukТаким чином, повідомленням для передавання може бути «АБАГГВАБ».', 'ukТеорія інформації пропонує спосіб обчислення найменшої можливої кількості інформації, яка передасть це.', 'ukЯкщо всі 4 літери є рівноймовірними (25 %), ми не можемо зробити нічого кращого (над двійковим каналом), аніж кодувати кожну літеру 2 бітами (у двійковому вигляді): «А» може бути кодовано як «00», «Б» — як «01», «В» — як «10», а «Г» — як «11».', 'ukТепер припустімо, що «А» трапляється з імовірністю 70 %, «Б» — 26 %, а «В» та «Г» — по 2 % кожна.', 'ukМи могли би призначити коди змінної довжини, так, що отримання «1» казало би нам дивитися ще один біт, якщо тільки ми не отримали вже перед цим 2 послідовні біти одиниць.', 'ukВ цьому випадку «А» було би кодовано як «0» (один біт), «Б» — як «10», а «В» і «Г» як «110» і «111».', 'ukЛегко побачити, що протягом 70 % часу потрібно надсилати лише один біт, 26 % часу — два біти, і лише протягом 4 % від часу — 3 біти.', 'ukТоді в середньому необхідно менше за 2 біти, оскільки ентропія є нижчою (через високу поширеність «А» з наступною за нею «Б» — разом 96 % символів).', 'ukОбчислення суми зважених за ймовірностями логарифмічних імовірностей вимірює та схоплює цей ефект.Теорема Шеннона також означає, що жодна схема стиснення без втрат не може скорочувативсіповідомлення.', 'ukЯкщо якісь повідомлення виходять коротшими, хоча б одне повинне вийти довшим в силупринципу Діріхле.', 'ukВ практичному застосуванні це зазвичай не є проблемою, оскільки нас, як правило, цікавить стиснення лише певних типів повідомлень, наприклад, документів англійською мовою на противагу до тексту тарабарщиною, або цифрових фотографій, а не шуму, і не важливо, якщо алгоритм стиснення робить довшими малоймовірні або нецікаві послідовності.Означення[ред.|ред.', 'ukкод]Назвавши її на честьΗ-теореми Больцмана[en], Шеннон означив ентропіюΗ(грецька літераета)дискретної випадкової величиниXз можливими значеннями,\\\\ldots ,x_\\\\right\\\\}}тафункцією маси ймовірностіP(X) (X)}якH(X)=E[I\\u2061(X)]=E[−log\\u2061(P(X))].', 'uk(X)=\\\\mathbb [\\\\operatorname (X)]=\\\\mathbb [-\\\\log(\\\\mathrm (X))].', 'uk}ТутE }єоператором математичного сподівання(англ.expectation), аIєкількістю інформаціїX.', 'uk[5]:11[6]:19–20I(X)і сама є випадковою величиною.Ентропію може бути записано в явному вигляді якH(X)=∑i=1nP(xi)I(xi)=−∑i=1nP(xi)logb\\u2061P(xi), (X)=\\\\sum _^ (x_)\\\\,\\\\mathrm (x_)}=-\\\\sum _^ (x_)\\\\log _\\\\mathrm (x_)},}деbєосновою[en]логарифма, який застосовується.', 'ukЗвичними значеннямиbє 2,число Ейлерае, та 10, а відповідними одиницями ентропії єбітидляb= 2,натидляb=e, табанидляb= 10.', 'uk[7]У випадкуP(xi) = 0для деякогоiзначення відповідного доданку0 logb(0)приймають рівним0, що відповідаєграниціlimp→0+plog\\u2061(p)=0.p\\\\log(p)=0.', 'uk}Можна також визначитиумовну ентропіюдвох подійXтаY, які набувають значеньxiтаyjвідповідно, якH(X|Y)=−∑i,jp(xi,yj)log\\u2061p(xi,yj)p(yj) (X|Y)=-\\\\sum _p(x_,y_)\\\\log ,y_)})}}}деp(xi,yj)є ймовірністю того, щоX=xiтаY=yj.', 'ukЦю величину слід розуміти як ступінь довільності випадкової величиниXпри заданій подіїY.Приклад[ред.|ред.', 'ukкод]ЕнтропіяΗ(X)(тобтоочікувананеочікуваність) підкидання монети, вимірювана в бітах, представлена графічно відносно упередження монетиPr(X= 1), деX= 1є випадінням аверса.Тут ентропія складає щонайбільше 1 біт, і для повідомлення результатів підкидання монети (2 можливі значення) потрібно в середньому не більше 1 біту (в точності 1 біт для справедливої монети).', \"ukРезультат справедливого грального кубика (6 можливих значень) потребуватиме в середньому log26 бітів.Детальніші відомості з цієї теми ви можете знайти в статтіФункція двійкової ентропії[en]таПроцес Бернуллі[en].Розгляньмо підкидання монети з відомими, але не обов'язково справедливими ймовірностями випадання аверсу чи реверсу; це можна змоделювати якпроцес Бернуллі[en].Ентропія невідомого результату наступного підкидання монети є найбільшою, якщо монета є справедливою (тобто, якщо як аверси, так і реверси мають однакову ймовірність 1/2).\", 'ukЦе є ситуацією максимальної невизначеності, оскільки в ній найскладніше передбачити результат наступного підкидання; результат кожного підкидання монети подає один повнийбітінформації.', 'ukЦе є тому, щоH(X)=−∑i=1nP(xi)logb\\u2061P(xi)=−∑i=1212log2\\u206112=−∑i=1212⋅(−1)=1\\\\mathrm (X)&=-\\\\sum _^ (x_)\\\\log _\\\\mathrm (x_)}\\\\\\\\&=-\\\\sum _^}\\\\log _}}\\\\\\\\&=-\\\\sum _^}\\\\cdot (-1)}=1\\\\end}}Проте, якщо ми знаємо, що монета не є справедливою, а випадає аверсом чи реверсом з імовірностямиpтаq, деp≠q, то тоді невизначеності є менше.', 'ukКожного разу, як її підкидають, випадання однієї зі сторін є ймовірнішим, ніж випадання іншої.', 'ukЗниження невизначеності отримує кількісну оцінку в вигляді меншої ентропії: в середньому кожне підкидання монети подає менше ніж один повнийбітінформації.', 'ukНаприклад, якщоp=0.7, тоH(X)=−plog2\\u2061(p)−qlog2\\u2061(q)=−0.7log2\\u2061(0.7)−0.3log2\\u2061(0.3)≈−0.7⋅(−0.515)−0.3⋅(−1.737)=0.8816<1\\\\mathrm (X)&=-p\\\\log _(p)-q\\\\log _(q)\\\\\\\\&=-0.7\\\\log _(0.7)-0.3\\\\log _(0.3)\\\\\\\\&\\\\approx -0.7\\\\cdot (-0.515)-0.3\\\\cdot (-1.737)\\\\\\\\&=0.8816<1\\\\end}}Крайнім випадком є двоаверсова монета, яка ніколи не випадає реверсом, або двореверсова монета, яка ніколи не дає в результаті аверс.', 'ukТут немає ніякої невизначеності.', 'ukЕнтропія дорівнює нулеві: кожне підкидання монети не подає жодної нової інформації, оскільки результати кожного підкидання монети завжди визначені.Ентропію може бути нормалізовано шляхом ділення її на довжину інформації.', 'ukЦе співвідношення називаєтьсяметричною ентропією[en], воно є мірою хаотичності інформації.Обґрунтування[ред.|ред.', 'ukкод]Щоби зрозуміти сенс∑pilog(1/pi), спершу спробуймо визначити функцію інформаціїIз точки зору подіїiз імовірністюpi.', \"ukКількість інформації, отримувана через спостереження подіїi, випливає з шеннонового розв'язку фундаментальнихвластивостейінформації:[8]I(p)ємонотонно спадноювp— збільшення ймовірності події зменшує інформацію зі спостереження події, і навпаки.I(p) ≥ 0— інформація єневід'ємноювеличиною.I(1) = 0— події, які відбуваються завжди, не несуть інформації.I(p1p2) = I(p1) + I(p2)— інформація віднезалежних подійє адитивною.Остання властивість є ключовою.\", 'ukВона стверджує, що спільна ймовірність незалежних джерел інформації несе стільки ж інформації, скільки й ці дві окремі події по одній.', 'ukЗокрема, якщо перша подія може видавати один зnрівноймовірних[en]результатів, а інша має один ізmрівноймовірних результатів, то існуєmnможливих результатів спільної події.', 'ukЦе означає, що якщо для кодування першого значення потрібноlog2(n)біт, а для кодування другого —log2(m), то для кодування обох потрібноlog2(mn) = log2(m) + log2(n).', \"ukШеннон виявив, що правильним вибором функції для кількісної оцінки інформації, яка зберігала би цю адитивність, є логарифмічна, тобтоI(p)=log\\u2061(1p)=−log\\u2061(p): (p)=\\\\log \\\\left(}\\\\right)=-\\\\log(p):}нехайIє інформаційною функцією, що є двічі неперервно диференційовною, тодіI(p1p2)=I(p1)+I(p2)p2I′(p1p2)=I′(p1)I′(p1p2)+p1p2I″(p1p2)=0I′(u)+uI″(u)=0(u↦uI′(u))′=0I(p_p_)&=&I(p_)+I(p_)\\\\\\\\p_I'(p_p_)&=&I'(p_)\\\\\\\\I'(p_p_)+p_p_I''(p_p_)&=&0\\\\\\\\I'(u)+uI''(u)&=&0\\\\\\\\(u\\\\mapsto uI'(u))'&=&0\\\\end}}Цедиференціальне рівнянняведе до розв'язкуI(u)=klog\\u2061uдля будь-якогоk∈R }.\", 'ukУмова 2 веде доk<0, а надто,kможе обиратися з виглядуk=−1/log\\u2061x, деx>1, що рівнозначне обиранню конкретноїоснови логарифма.', 'ukРізніодиниці інформації(бітидлядвійкового логарифмаlog2,натидлянатурального логарифмаln,банидлядесяткового логарифмаlog10і так далі) є лишесталим домноженнямодна одної.', 'ukНаприклад, у випадку підкидання справедливої монети аверс даєlog2(2) = 1біт інформації, що становить приблизно 0.693 нат, або 0.301 десяткових цифр.', 'ukВ силу адитивності,nпідкидань даютьnбіт інформації, що становить приблизно0.693nнат, або0.301nдесяткових цифр.Якщо існує розподіл, в якому подіяiможе траплятися з імовірністюpi, з нього було взятоNпроб, і результатiтрапивсяni=Npiразів, то загальною кількістю інформації, яку ми отримали, є∑iniI(pi)=−∑iNpilog\\u2061pi\\\\mathrm (p_)}=-\\\\sum _\\\\log }}}.Отже,усередненоюкількістю інформації, яку ми отримуємо на подію, є−∑ipilog\\u2061pi.\\\\log }}.}Аспекти[ред.|ред.', 'ukкод]Відношення до термодинамічної ентропії[ред.|ред.', 'ukкод]Детальніші відомості з цієї теми ви можете знайти в статтіЕнтропія в термодинаміці та теорії інформації[en].Натхнення прийняти словоентропіяв теорії інформації виникло від близької подібності між формулою Шеннона, і дуже схожими відомими формулами зістатистичної механіки.Устатистичній термодинаміцінайзагальнішою формулою для термодинамічноїентропіїSтермодинамічної системиєентропія Гіббса,S=−kB∑piln\\u2061pi}\\\\sum p_\\\\ln p_\\\\,}деkBєсталою Больцмана, аpiє ймовірністюмікростану[en].', 'ukЕнтропію Гіббса було визначеноВіллардом Гіббсом1878 року після ранішої праціБольцмана(1872 року).', \"uk[9]Ентропія Гіббса переноситься майже беззмінною до світуквантової фізики, даючиентропію фон Неймана, введенуДжоном фон Нейманом1927 року,S=−kBTr(ρln\\u2061ρ)}\\\\,}(\\\\rho \\\\ln \\\\rho )\\\\,}деρєматрицею густиниквантової механічної системи, аTrєслідом.На побутовому практичному рівні зв'язок між інформаційною та термодинамічною ентропією очевидним не є. Фізики та хіміки схильні більше цікавитисязмінамиентропії при спонтанному розвитку системи від її первісних умов, у відповідності здругим законом термодинаміки, ніж незмінним розподілом імовірності.\", 'ukІ, як вказує малістьсталої БольцманаkB, зміни вS/kBнавіть для невеликих кількостей речовин у хімічних та фізичних процесах являють собою обсяги ентропії, які є надзвичайно великими порівняно з чим завгодно встисненні данихтаобробці сигналів.', \"ukКрім того, в класичній термодинаміці ентропія визначається в термінах макроскопічних вимірювань, і не робить жодних посилань на будь-який розподіл імовірності, що є головним у визначенні ентропії інформаційної.Зв'язок між термодинамікою і тим, що зараз відомо як теорія інформації, було вперше зробленоЛюдвігом Больцманом, і виражено йогознаменитим рівнянням[en]:S=kBln\\u2061(W)}\\\\ln(W)}деSє термодинамічною ентропією окремого макростану (визначену термодинамічними параметрами, такими як температура, об'єм, енергія тощо),Wє числом мікростанів (різних комбінацій частинок у різних енергетичних станах), які можуть дати даний макростан, аkBєсталою Больцмана.\", 'ukПередбачається, що кожен мікростан є однаково правдоподібним, так що ймовірністю заданого мікростану єpi= 1/W.', 'ukПри підставленні цих імовірностей до наведеного вище виразу для ентропії Гіббса (або, рівнозначно, ентропії Шеннона, помноженої наkB) в результаті виходить рівняння Больцмана.', 'ukВ термінах теорії інформації інформаційна ентропія системи є кількістю інформації, якої «бракує» для визначення мікростану при відомому макростані.На думкуДжейнса[en](1957 року), термодинамічну ентропію, як пояснюєтьсястатистичною механікою, слід розглядати якзастосуваннятеорії інформації Шеннона: термодинамічна ентропія інтерпретується як така, що є пропорційною до кількості додаткової інформації Шеннона, необхідної для визначення детального мікроскопічного стану системи, яка залишається не повідомленою описом виключно в термінах макроскопічних величин класичної термодинаміки, з коефіцієнтом пропорційності, який є простосталою Больцмана.', 'ukНаприклад, додавання теплоти до системи підвищує її термодинамічну ентропію, оскільки це збільшує число можливих мікроскопічних станів системи, які узгоджуються з вимірюваними значеннями його макроскопічних величин, роблячи таким чином будь-який повний опис стану довшим.', 'uk(Див.', \"ukстаттютермодинаміка максимальної ентропії[en]).Демон Максвелламоже (гіпотетично) знижувати термодинамічну ентропію системи з використанням інформації про стан окремих молекул, але, як показалиЛандауер[en](з 1961 року) з колегами, щоби діяти, цей демон мусить сам підвищувати термодинамічну ентропію в цьому процесі, принаймні на кількість інформації Шеннона, яку він пропонує спочатку отримати й зберегти; і, таким чином, загальна термодинамічна ентропія не знижується (що розв'язує парадокс).Принцип Ландауеравстановлює нижню межу кількості тепла, яке комп'ютер повинен генерувати для обробки заданого обсягу інформації, хоча сучасні комп'ютери є набагато менш ефективними.Ентропія як кількість інфромації[ред.|ред.\", 'ukкод]Детальніші відомості з цієї теми ви можете знайти в статтіТеорема Шеннона про кодування джерела[en].Ентропія визначається в контексті ймовірнісної моделі.', 'ukНезалежні підкидання справедливої монети мають ентропію по 1 біту на підкидання.', 'ukДжерело, яке завжди породжує довгий рядок «Б», має ентропію 0, оскільки наступним символом завжди буде «Б».Ентропійна швидкістьджерела даних означає усереднене числобітівна символ, потрібне для його кодування.', 'ukЕксперименти Шеннона з передбаченням людьми показують швидкість інформації на символ англійською мовою між 0.6 і 1.3 біта;[10]Алгоритм стисненняPPMможе досягати рівня стиснення в 1.5 біти на символ англомовного тексту.Зверніть увагу на наступні моменти з попереднього прикладу:Кількість ентропії не завжди є цілим числом бітів.Багато бітів даних можуть не нести інформації.', 'ukНаприклад, структури даних часто зберігають інформацію надмірно, або мають ідентичні розділи, незалежно від інформації в структурі даних.Шеннонівське визначення ентропії, при застосуванні до джерела інформації, може визначати мінімальну пропускну здатність каналу, необхідну для надійного передавання джерела у вигляді закодованих двійкових цифр (див.', 'ukзастереження нижче курсивом).', 'ukЦю формулу може бути виведено шляхом обчислення математичного сподівання кількості інформації, яка міститься в одній цифрі з джерела інформації.Див.', 'ukтакожтеорему Шеннона — Гартлі.Ентропія Шеннона вимірює інформацію, яка міститься в повідомленні, на противагу до тієї частини повідомлення, яка є визначеною (або передбачуваною).Приклади останнього включають надмірність у структурі мови та статистичні властивості, які стосуються частот трапляння пар, трійок тощо літер або слів.Див.ланцюги Маркова.Ентропія як міра різноманітності[ред.|ред.', 'ukкод]Докладніше:Міра різноманітностіЕнтропія є одним з декількох способів вимірювання різноманітності.', 'ukЗокрема, ентропія Шеннона є логарифмом1D, індексуістинної різноманітностіз параметром, який дорівнює 1.Стиснення даних[ред.|ред.', 'ukкод]Докладніше:Стиснення данихЕнтропія ефективно обмежує продуктивність найсильнішого можливого стиснення без втрат, яке може бути реалізовано теоретично з допомогоютипового набору[en], або практично із застосуванням кодуванняХаффмана,Лемпеля — Зіва, абоарифметичного кодування.', 'ukДив.', 'ukтакожколмогоровську складність.', 'ukНа практиці для захисту від помилок алгоритми стиснення навмисно включають деяку розумну надмірність у виглядіконтрольних сум.Світовий технологічний потенціал для зберігання та передавання інформації[ред.|ред.', 'ukкод]Дослідження 2011 року в журналі«Science»оцінює світовий технологічний потенціал для зберігання та передавання оптимально стисненої інформації, нормалізований за найефективнішими алгоритмами стиснення, доступними в 2007 році, оцінюючи таким чином ентропію технологічно доступних джерел.', 'uk[11]:60–65Всі числа — в ентропійно стисненихексабайтахТип інформації19862007Зберігання2.6295Широкомовлення4321900Телекомунікації0.28165Автори оцінюють технологічний потенціал людства у зберіганні інформації (повністю ентропійно стисненої) 1986 року, і знову 2007 року.', 'ukВони ділять інформацію на три категорії — зберігання інформації на носії, отримування інформації через мережі однобічногоширокомовлення, та обмін інформацією через двобічнітелекомунікаційнімережі.', 'uk[11]Обмеження ентропії як кількості інформації[ред.|ред.', \"ukкод]Існує ряд пов'язаних з ентропією понять, які певним чином здійснюють математичну кількісну оцінку обсягу інформації:власна інформаціяокремого повідомлення або символу, взятого із заданого розподілу ймовірності,ентропіязаданого розподілу ймовірності повідомлень або символів, таентропійна швидкістьстохастичного процесу.\", 'uk(Для конкретної послідовності повідомлень або символів, породженої заданим стохастичним процесом, також може бути визначено «швидкість власної інформації»: у випадкустаціонарного процесувона завжди дорівнюватиме швидкості ентропії.)', \"ukДля порівняння різних джерел інформації, або встановлення зв'язку між ними, використовуються й іншіКількості інформації.Важливо не плутати наведені вище поняття.\", 'ukЯке з них мається на увазі, часто може бути зрозумілим лише з контексту.', 'ukНаприклад, коли хтось говорить, що «ентропія» англійської мови становить близько 1 біта на символ, вони насправді моделюють англійську мову як стохастичний процес, і говорять прошвидкістьїї ентропії.', 'ukШеннон і сам використовував цей термін таким чином.Проте, якщо ми використовуємо дуже великі блоки, то оцінка посимвольної ентропійної швидкості може стати штучно заниженою.', 'ukЦе відбувається тому, що насправді розподіл ймовірності послідовності не є пізнаваним точно; це лише оцінка.', 'ukНаприклад, припустімо, що розглядаються тексти всіх будь-коли опублікованих книг як послідовність, у якій кожен символ є текстом цілої книги.', 'ukЯкщо існуєNопублікованих книг, і кожну книгу опубліковано лише одного разу, то оцінкою ймовірності кожної книги є1/N, а ентропією (в бітах) є−log2(1/N) = log2(N).', 'ukЯк практичний код, це відповідає призначенню кожній книзіунікального ідентифікатораі використання його замість тексту книги будь-коли, коли потрібно послатися на цю книгу.', 'ukЦе надзвичайно корисно для розмов про книги, але не дуже корисно для характеризування кількості інформації окремих книг або мови в цілому: неможливо відтворити книгу з її ідентифікатора, не знаючи розподілу ймовірності, тобто повного тексту всіх книг.', \"ukКлючова ідея полягає в тому, що повинна братися до уваги складність імовірнісної моделі.Колмогоровська складністьявляє собою теоретичне узагальнення цієї ідеї, яке дозволяє розглядати кількість інформації послідовності незалежно від конкретної ймовірнісної моделі; воно розглядає найкоротшупрограмудляуніверсального комп'ютера, яка виводить цю послідовність.\", 'ukТакою програмою є код, який досягає ентропійної швидкості послідовності для заданої моделі, плюс кодовий словник (тобто, ймовірнісна модель), але вона не може бути найкоротшою.Наприклад, послідовністю Фібоначчі є 1, 1, 2, 3, 5, 8, 13, ….', 'ukПри розгляді цієї послідовності як повідомлення, а кожного числа як символу, існує майже стільки ж символів, скільки є символів у повідомленні, даючи ентропію близькоlog2(n).', 'ukТаким чином, перші 128 символів послідовності Фібоначчі мають ентропію приблизно 7 біт/символ.', 'ukПроте цю послідовність може бути виражено за допомогою формули [F(n) = F(n−1) + F(n−2)дляn= 3, 4, 5, …,F(1) =1,F(2) = 1], і ця формула має значно нижчу ентропію, і застосовується до послідовності Фібоначчі будь-якої довжини.Обмеження ентропії в криптографії[ред.|ред.', 'ukкод]Вкриптоаналізіентропію часто грубо використовують як міру непередбачуваності криптографічного ключа, хоча її справжняневизначеністьє незмірною.', 'ukНаприклад, 128-бітовий ключ, який породжено рівномірно випадково, має 128 біт ентропії.', 'ukВін також вимагає (усереднено)2128−1}спроб для зламу грубою силою.', 'ukПроте, ентропія не відображає число потрібних спроб, якщо можливі ключі вибирають не рівномірно.', 'uk[12][13]Натомість, для вимірювання зусиль, необхідних для атаки грубою силою, можна використовувати міру, що називаютьздогадом(англ.guesswork).', 'uk[14]Через нерівномірність розподілів, що застосовують в криптографії, можуть виникати й інші проблеми.', 'ukНаприклад, розгляньмо 1 000 000-цифровий двійковийшифр Вернамаіз застосуванням виключного «або».', 'ukЯкщо цей шифр має 1 000 000 біт ентропії, це чудово.', 'ukЯкщо цей шифр має 999 999 біт ентропії, які розподілено рівномірно (коли кожен окремий біт шифру має 0.999999 біт ентропії), то він може пропонувати добру безпеку.', 'ukАле якщо цей шифр має 999 999 біт ентропії, так, що перший біт є незмінним, а решта 999 999 бітів вибирають випадково, то перший біт шифрованого тексту не буде шифровано взагалі.Дані як марковський процес[ред.|ред.', 'ukкод]Звичний спосіб визначення ентропії для тексту ґрунтується намарковській моделітексту.', 'ukДля джерела нульового порядку (кожен символ обирається незалежно від останніх символів) двійковою ентропією єH(S)=−∑pilog2\\u2061pi, (})=-\\\\sum p_\\\\log _p_,}деpiє ймовірністюi.', 'ukДлямарковського джерела[en]першого порядку (такого, в якому ймовірність вибору символу залежить лише від безпосередньо попереднього символу)ентропійною швидкістюєH(S)=−∑ipi∑jpi(j)log2\\u2061pi(j), (})=-\\\\sum _p_\\\\sum _\\\\ p_(j)\\\\log _p_(j),}[джерело?', 'uk]деiєстаном(деякими попередніми символами), аpi(j)(j)}є ймовірністюjза заданого попереднього символуi.Для марковського джерела другого порядку ентропійною швидкістю єH(S)=−∑ipi∑jpi(j)∑kpi,j(k)log2\\u2061pi,j(k).', 'uk(})=-\\\\sum _p_\\\\sum _p_(j)\\\\sum _p_(k)\\\\ \\\\log _\\\\ p_(k).', 'uk}b-арна ентропія[ред.|ред.', 'ukкод]В загальному випадкуb-арна ентропіяджерелаS}}= (S,P)зпервинною абеткоюS= ідискретним розподілом ймовірностіP= , деpiє імовірністюai(скажімо,pi=p(ai)), визначається якHb(S)=−∑i=1npilogb\\u2061pi, _(})=-\\\\sum _^p_\\\\log _p_,}Зауваження:bу «b-арній ентропії» є числом різних символівідеальної абетки, яка використовується як стандартне мірило для вимірювання первинних абеток.', 'ukВ теорії інформації для абетки для кодування інформації єнеобхідними і достатнімидва символи.', 'ukТому стандартним є братиb= 2(«двійкова ентропія»).', 'ukТаким чином, ентропія первинної абетки, з її заданим емпіричним розподілом імовірності, — це число, яке дорівнює числу (можливо дробовому) символів «ідеальної абетки», з оптимальним розподілом імовірності, необхідних для кодування кожного символу первинної абетки.', 'ukТакож зверніть увагу, що «оптимальний розподіл імовірності» тут означаєрівномірний розподіл: первинна абетка зnсимволів має найвищу можливу ентропію (для абетки зnсимволів), коли розподіл імовірності абетки є рівномірним.', 'ukЦією оптимальною ентропією виявляєтьсяlogb(n).Ефективність[ред.|ред.', 'ukкод]Первинна абетка з нерівномірним розподілом матиме меншу ентропію, ніж якби ці символи мали рівномірний розподіл (тобто, були «оптимальною абеткою»).', \"ukЦю дефективність ентропії може бути виражено відношенням, яке називається ефективністю:[ця цитата потребує посилання]η(X)=−∑i=1np(xi)logb\\u2061(p(xi))logb\\u2061(n)=logn\\u2061(∏i=1np(xi)−p(xi))^)\\\\log _(p(x_))}(n)}}=\\\\log _(\\\\prod _^p(x_)^)})}[прояснити]Ефективність є корисною для кількісної оцінки ефективності використанняканалу зв'язку.\", 'ukЦе формулювання також називають нормалізованою ентропією, оскільки ентропія ділиться на максимальну ентропіюlogb\\u2061(n)(n)}}.', 'ukКрім того, ефективності байдужий вибір (додатної) базиb, про що свідчить нечутливість до неї в кінцевому логарифмі.Характеризування[ред.|ред.', 'ukкод]Ентропія Шеннонахарактеризується[en]невеликим числом критеріїв, перерахованих нижче.', 'ukБудь-яке визначення ентропії, яке задовольняє ці припущення, має вигляд−K∑i=1npilog\\u2061(pi)^p_\\\\log(p_)}деKє сталою, яка відповідає виборові одиниць вимірювання.Даліpi= Pr(X=xi), аΗn(p1, …,pn) = Η(X).Неперервність[ред.|ред.', 'ukкод]Ця міра повинна бутинеперервною, так що зміна значень ймовірностей на дуже незначну величину повинна змінювати ентропію лише на незначну величину.Симетричність[ред.|ред.', 'ukкод]При перевпорядкуванні виходівxiця міра повинна залишатися незмінною.Hn(p1,p2,…)=Hn(p2,p1,…) _\\\\left(p_,p_,\\\\ldots \\\\right)=\\\\mathrm _\\\\left(p_,p_,\\\\ldots \\\\right)}тощоМаксимум[ред.|ред.', 'ukкод]Ця міра повинна бути максимальною тоді, коли всі виходи є однаково правдоподібними (невизначеність є найвищою, коли всі можливі події є рівноймовірними).Hn(p1,…,pn)≤Hn(1n,…,1n)=logb\\u2061(n).', 'uk_(p_,\\\\ldots ,p_)\\\\leq \\\\mathrm _\\\\left(},\\\\ldots ,}\\\\right)=\\\\log _(n).', 'uk}Для рівноймовірних подій ентропія повинна зростати зі збільшенням числа виходів.Hn(1n,…,1n⏟n)=logb\\u2061(n)<logb\\u2061(n+1)=Hn+1(1n+1,…,1n+1⏟n+1).', 'uk_\\\\underbrace },\\\\ldots ,}} _=\\\\log _(n)<\\\\log _(n+1)=\\\\mathrm _\\\\underbrace },\\\\ldots ,}} _.', 'uk}Для безперервних випадкових змінних розподілом із максимальноюдиференціальною ентропієюєбагатовимірний нормальний розподіл.Адитивність[ред.|ред.', 'ukкод]Величина ентропії повинна не залежати від того, який поділ процесу на частини розглядається.Ця остання функційна залежність характеризує ентропію системи з підсистемами.', 'ukВона вимагає, щоби ентропію системи можливо було обчислити з ентропій її підсистем, якщо взаємодії між цими підсистемами є відомими.При заданому ансамбліnрівномірно розподілених елементів, які поділяються наkблоків (підсистем) зb1, ...,bkелементами в кожному, ентропія всього ансамблю повинна дорівнювати сумі ентропії системи блоків і окремих ентропій блоків, кожна з яких є зваженою на ймовірність знаходження в цьому відповідному блоці.Длядодатних цілих чиселbi, деb1+ … +bk=n,Hn(1n,…,1n)=Hk(b1n,…,bkn)=∑i=1kbinHbi(1bi,…,1bi).', 'uk_\\\\left(},\\\\ldots ,}\\\\right)=\\\\mathrm _\\\\left(}},\\\\ldots ,}}\\\\right)=\\\\sum _^}}\\\\,\\\\mathrm _}\\\\left(}},\\\\ldots ,}}\\\\right).', 'uk}При обранніk=n,b1= … =bn= 1це означає, що ентропія незмінного виходу дорівнює нулеві:Η1(1) = 0.', 'ukЦе означає, що ефективність первинної абетки зnсимволами може бути визначено просто як таку, що дорівнює їїn-арній ентропії.', 'ukДив.', 'ukтакожнадмірність інформації.Додаткові властивості[ред.|ред.', \"ukкод]Ентропія Шеннона задовольняє наступні властивості, для деяких з яких корисно інтерпретувати ентропію як кількість пізнаної інформації (або усуненої невизначеності) при розкритті значення випадкової величиниX:Додавання або усування подій з нульовою ймовірністю не впливає на ентропію:Hn+1(p1,…,pn,0)=Hn(p1,…,pn) _(p_,\\\\ldots ,p_,0)=\\\\mathrm _(p_,\\\\ldots ,p_)}.Ентропія дискретної випадкової змінної є невід'ємним числом:H(X)≥0 (X)\\\\geq 0}.\", 'uk[15]:15За допомогоюнерівності Єнсенаможе бути підтверджено, щоH(X)=E\\u2061[logb\\u2061(1p(X))]≤logb\\u2061(E\\u2061[1p(X)])=logb\\u2061(n) (X)=\\\\operatorname \\\\left[\\\\log _\\\\left(}\\\\right)\\\\right]\\\\leq \\\\log _\\\\left(\\\\operatorname \\\\left[}\\\\right]\\\\right)=\\\\log _(n)}.', 'uk[15]:29Ця максимальна ентропіяlogb(n)ефективно досягається первинною абеткою, яка має рівномірний розподіл ймовірності: невизначеність є максимальною, коли всі можливі події є однаково ймовірними.Ентропія, або обсяг інформації, розкритої шляхом визначення(X,Y)(тобто визначенняXтаYодночасно) дорівнює інформації, розкритої шляхом проведення двох послідовних експериментів: спершу виявлення значенняY, а потім виявлення значенняX, за умови, що ви знаєте значенняY.', 'ukЦе може бути записано якH(X,Y)=H(X|Y)+H(Y)=H(Y|X)+H(X).', 'uk(X,Y)=\\\\mathrm (X|Y)+\\\\mathrm (Y)=\\\\mathrm (Y|X)+\\\\mathrm (X).', 'uk}ЯкщоY=f(X), деfє функцією, тоH(f(X)|X)=0.', 'ukЗастосування попередньої формули доH(X,f(X))даєH(X)+H(f(X)|X)=H(f(X))+H(X|f(X)), (X)+\\\\mathrm (f(X)|X)=\\\\mathrm (f(X))+\\\\mathrm (X|f(X)),}отже,H(f(X)|X)≤H(X), таким чином, ентропія величини може тільки зменшуватися, коли остання проходить через функцію.ЯкщоXтаYв є двома незалежними випадковими змінними, то знання значенняYне впливає на наше знання про значенняX(оскільки ці двоє не впливають одне на одне в силу незалежності):H(X|Y)=H(X).', 'uk(X|Y)=\\\\mathrm (X).', 'uk}Ентропія двох одночасних подій є не більшою за суму ентропій кожної з окремих подій, і дорівнює їй, якщо ці дві події є незалежними.', 'ukТочніше, якщоXтаYє двома випадковими величинами на одному й тому ж імовірнісному просторі, а(X,Y)позначає їхній декартів добуток, тоH(X,Y)≤H(X)+H(Y).', 'uk(X,Y)\\\\leq \\\\mathrm (X)+\\\\mathrm (Y).', 'uk}ЕнтропіяH(p) (p)}єугнутоюза функцією маси ймовірностіp, тобто,H(λp1+(1−λ)p2)≥λH(p1)+(1−λ)H(p2) (\\\\lambda p_+(1-\\\\lambda )p_)\\\\geq \\\\lambda \\\\mathrm (p_)+(1-\\\\lambda )\\\\mathrm (p_)}для всіх функцій маси ймовірностіp1,p2,p_}та0≤λ≤1.', 'uk[15]:32Розширення дискретної ентропії до неперервного випадку[ред.|ред.', 'ukкод]Диференціальна ентропія[ред.|ред.', 'ukкод]Докладніше:Диференціальна ентропіяЕнтропію Шеннона обмежено випадковими величинами, які набувають дискретних значень.', 'ukВідповідна формула для неперервної випадкової величини зфункцією густини ймовірностіf(x)зі скінченною або нескінченною областю визначенняX }на дійсній осі визначається аналогічно, з допомогою наведеного вище вираження ентропії як математичного сподівання:h[f]=E\\u2061[−ln\\u2061(f(x))]=−∫Xf(x)ln\\u2061(f(x))dx.', 'uk[-\\\\ln(f(x))]=-\\\\int _ }f(x)\\\\ln(f(x))\\\\,dx.', 'uk}Цю формулу зазвичай називаютьнепере́рвною ентропі́єю(англ.continuous entropy), абодиференціальною ентропією.', 'ukПопередником неперервної ентропіїh[f]є вираз функціоналуΗвΗ-теоремі Больцмана[en].Хоча аналогія між обома функціями й наводить на роздуми, мусить бути поставлено наступне питання: чи є диференціальна ентропія обґрунтованим розширенням дискретної ентропії Шеннона?', \"ukДиференціальній ентропії бракує ряду властивостей, якими володіє дискретна ентропія Шеннона, — вона навіть може бути від'ємною, — і відтак було запропоновано поправки, зокрема,взяття границі щільності дискретних точок[en].Щоби відповісти на це питання, ми мусимо встановити зв'язок між цими двома функціями:Ми хочемо отримати загальну скінченну міру при прямуваннірозміру засіківдо нуля.\", 'ukВ дискретному випадку розмір засіків є (неявною) шириною кожного зn(скінченних або нескінченних) засіків, чиї ймовірності позначаються черезpn.', 'ukОскільки ми робимо узагальнення до неперервної області визначення, ми мусимо зробити цю ширину явною.Щоби зробити це, почнімо з неперервної функціїf, дискретизованої засіками розміруΔ.', 'ukЗгідно теореми про середнє значення, в кожному засіку існує таке значенняxi, щоf(xi)Δ=∫iΔ(i+1)Δf(x)dx)\\\\Delta =\\\\int _^f(x)\\\\,dx}і відтак інтеграл функціїfможе бути наближено (в рімановому сенсі) за допомогою∫−∞∞f(x)dx=limΔ→0∑i=−∞∞f(xi)Δ^f(x)\\\\,dx=\\\\lim _\\\\sum _^f(x_)\\\\Delta }де цяграницяі «розмір засіку прямує до нуля» є рівноцінними.ПозначмоHΔ:=−∑i=−∞∞f(xi)Δlog\\u2061(f(xi)Δ) ^:=-\\\\sum _^f(x_)\\\\Delta \\\\log \\\\left(f(x_)\\\\Delta \\\\right)}і, розкриваючи цього логарифма, ми маємоHΔ=−∑i=−∞∞f(xi)Δlog\\u2061(f(xi))−∑i=−∞∞f(xi)Δlog\\u2061(Δ).', 'uk^=-\\\\sum _^f(x_)\\\\Delta \\\\log(f(x_))-\\\\sum _^f(x_)\\\\Delta \\\\log(\\\\Delta ).', 'uk}При Δ → 0 ми маємо∑i=−∞∞f(xi)Δ→∫−∞∞f(x)dx=1∑i=−∞∞f(xi)Δlog\\u2061(f(xi))→∫−∞∞f(x)log\\u2061f(x)dx.\\\\sum _^f(x_)\\\\Delta &\\\\to \\\\int _^f(x)\\\\,dx=1\\\\\\\\\\\\sum _^f(x_)\\\\Delta \\\\log(f(x_))&\\\\to \\\\int _^f(x)\\\\log f(x)\\\\,dx.\\\\end}}Але зауважте, щоlog(Δ) → −∞приΔ → 0, отже, нам потрібне особливе визначення диференціалу або неперервної ентропії:h[f]=limΔ→0(HΔ+log\\u2061Δ)=−∫−∞∞f(x)log\\u2061f(x)dx,\\\\left(\\\\mathrm ^+\\\\log \\\\Delta \\\\right)=-\\\\int _^f(x)\\\\log f(x)\\\\,dx,}що, як було сказано раніше, називаютьдиференціа́льною ентропі́єю(англ.differential entropy).', 'ukЦе означає, що диференціальна ентропіяне єграницею ентропії Шеннона приn→ ∞.', 'ukВона радше відрізняється від границі ентропії Шеннона нескінченним зсувом (див.', 'ukтакож статтю проінформаційну розмірність[en]).Взяття границі щільності дискретних точок[ред.|ред.', 'ukкод]Детальніші відомості з цієї теми ви можете знайти в статтіВзяття границі щільності дискретних точок[en].В результаті виходить, що, на відміну від ентропії Шеннона, диференціальна ентропіянеє в цілому доброю мірою невизначеності або інформації.', \"ukНаприклад, диференціальна ентропія може бути від'ємною; крім того, вона не є інваріантною відносно неперервних перетворень координат.\", 'ukЦю проблему може бути проілюстровано зміною одиниць вимірювання, колиxє розмірною величиною.f(x)тоді матиме одиниці1/x.', 'ukАргумент логарифма мусить бути безрозмірним, інакше він є неправильним, бо така диференціальна ентропія, як наведено вище, буде неправильною.', 'ukЯкщоΔє деяким «стандартним» значеннямx(тобто, «розміром засіку»), і відтак має такі ж одиниці вимірювання, то видозмінену диференціальну ентропію може бути записано в належному вигляді якH=∫−∞∞f(x)log\\u2061(f(x)Δ)dx^f(x)\\\\log(f(x)\\\\,\\\\Delta )\\\\,dx}і результат буде однаковим при будь-якому виборі одиниць вимірювання дляx.', 'ukНасправді границя дискретної ентропії приN→ 0включала би також і членlog(N), який в загальному випадку був би нескінченним.', 'ukЦе є очікуваним, неперервні величини при дискретизації зазвичай матимуть нескінченну ентропію.Взяття границі щільності дискретних точок[en]в дійсності є мірою того, наскільки розподіл є простішим для опису за розподіл, який єрівномірнимнад своєю схемою квантування.Відносна ентропія[ред.|ред.', 'ukкод]Детальніші відомості з цієї теми ви можете знайти в статтіУзагальнена відносна ентропія[en].Іншою корисною мірою ентропії, яка однаково добре працює в дискретному та неперервному випадках, євідно́сна ентропі́я(англ.relative entropy) розподілу.', 'ukВона визначається яквідстань Кульбака — Лейблеравід розподілу до еталонної міриmнаступним чином.', \"ukПрипустімо, що розподіл імовірностіpєабсолютно неперервнимвідносно міриm, тобто має виглядp(dx) =f(x)m(dx)для деякої невід'ємноїm-інтегрованої функціїfзm-інтегралом 1, тоді відносну ентропію може бути визначено якDKL(p‖m)=∫log\\u2061(f(x))p(dx)=∫f(x)log\\u2061(f(x))m(dx).\", 'uk}(p\\\\|m)=\\\\int \\\\log(f(x))p(dx)=\\\\int f(x)\\\\log(f(x))m(dx).', 'uk}В такому вигляді відносна ентропія узагальнює (з точністю до зміни знака) як дискретну ентропію, коли міраmєлічильною мірою, так і диференціальну ентропію, коли міраmємірою Лебега.', \"ukЯкщо міраmі сама є розподілом ймовірності, то відносна ентропія є невід'ємною, і нульовою, якщоp=mяк міри.\", 'ukВона визначається для будь-якого простору мір, отже, не залежить від координат, і є інваріантною відносно перепараметризації координат, якщо перетворення міриmбереться до уваги правильно.', 'ukВідносна ентропія, і неявно ентропія та диференціальна ентропія, залежать від «базової» міриm.Застосування в комбінаториці[ред.|ред.', 'ukкод]Ентропія стала корисною величиною вкомбінаториці.Нерівність Люміса — Уїтні[ред.|ред.', 'ukкод]Простим прикладом цього є альтернативне доведеннянерівності Люміса — Вітні[en]: для будь-якої підмножиниA⊆Zdми маємо|A|d−1≤∏i=1d|Pi(A)|\\\\leq \\\\prod _^|P_(A)|}деPiєортогональною проєкцієювi-й координаті:Pi(A)=.', 'uk(A)=\\\\,\\\\ldots ,x_,x_,\\\\ldots ,x_):(x_,\\\\ldots ,x_)\\\\in A\\\\}.', 'uk}Це доведеня випливає як простий наслідокнерівності Ширера[en]: якщоX1, …,Xdє випадковими величинами, аS1, …,Snє підмножинами, такими, що кожне ціле число між 1 іdлежить точно вrз цих підмножин, тоH[(X1,…,Xd)]≤1r∑i=1nH[(Xj)j∈Si] [(X_,\\\\ldots ,X_)]\\\\leq }\\\\sum _^\\\\mathrm [(X_)_}]}де(Xj)j∈Si)_}}є декартовим добутком випадкових величинXjз індексамиjвSi(тому розмірність цього вектора дорівнює розміровіSi).Ми зробимо нарис, як з цього випливає Люміс — Уїтні: справді, нехайXє рівномірно розподіленою випадковою величиною зі значеннями вA, і такою, що кожна точка зAтрапляється з однаковою ймовірністю.', 'ukТоді (згідно додаткових властивостей ентропії, згадуваних вище)Η(X) = log|A|, де|A|позначаєпотужністьA.', 'ukНехайSi= .', 'ukДіапазон(Xj)j∈Si)_}}міститься вPi(A)і, отже,H[(Xj)j∈Si]≤log\\u2061|Pi(A)| [(X_)_}]\\\\leq \\\\log |P_(A)|}.', 'ukТепер застосуймо це, щоби обмежити праву частину нерівності Ширера, і піднести до степеня протилежні частини нерівності, яку ви отримали в результаті.Наближення до біноміального коефіцієнту[ред.|ред.', 'ukкод]Для цілих чисел0 <k<nпокладімоq=k/n.', 'ukТоді2nH(q)n+1≤(nk)≤2nH(q), (q)}}}\\\\leq }\\\\leq 2^ (q)},}деH(q)=−qlog2\\u2061(q)−(1−q)log2\\u2061(1−q).', 'uk(q)=-q\\\\log _(q)-(1-q)\\\\log _(1-q).', 'uk}[16]:43Нижче наведено схематичне доведення.', 'ukЗауважте, що(nk)qqn(1−q)n−nq}q^(1-q)^}є одним із членів суми∑i=0n(ni)qi(1−q)n−i=(q+(1−q))n=1.^}q^(1-q)^=(q+(1-q))^=1.', 'uk}Відповідно, з нерівності(nk)qk(1−q)n−k≤1}q^(1-q)^\\\\leq 1}після логарифмування отримується верхня межа.Для отримання нижньої межі спочатку шляхом певних перетворень показується, що доданок(nk)qqn(1−q)n−nq}q^(1-q)^}є найбільшим членом у наведеній сумі.', 'ukАле тоді(nk)qqn(1−q)n−nq≥1n+1}q^(1-q)^\\\\geq }}оскільки біном міститьn+ 1членів.', 'ukЗвідси випливає нижня межа.Наведені результати інтерпретуються, наприклад, таким чином: число двійкових стрічок довжиниn, які містять рівноkодиниць, можна оцінити як2nH(k/n) (k/n)}}.[17]Див.', 'ukтакож[ред.|ред.', 'ukкод]Портал «Математика»Взаємна інформаціяВипадковістьВідстань ГеммінгаВідстань ЛевенштейнаГеометрія інформації[en]Ентропійна швидкістьЕнтропійне кодування— схема кодування, яка призначає коди символам таким чином, щоби довжини кодів відповідали ймовірностям символівЕнтропія (вісь часу)[en]Ентропія графа[en]Ентропія Колмогорова — Синая[en]вдинамічних системахЕнтропія Реньї— узагальнення ентропії Шеннона; вона є однією з сімейства функціоналів для кількісної оцінки різноманіття, невизначеності або випадковості системи.Ентропія ЦаллісаІндекс різноманітності— альтернативні підходи до кількісної оцінки різноманітності в розподілі ймовірностіІндекс ТейлаІндекс ШеннонаІнформація за ФішеромІсторія ентропії[en]Історія теорії інформації[en]Квантова відносна ентропія[en]— міра розрізнюваності між двома квантовими станамиМножник ЛагранжаНегентропіяНерівність ентропійної потужності[en]ТипоглікеміяОцінка ентропії[en]Перехресна ентропія— є мірою усередненого числа бітів, необхідних для ідентифікації події з набору можливостей між двома розподілами ймовірностіПерплексивністьСпільна ентропія— це міра того, як багато ентропії міститься в спільній системі з двох випадкових величин.Умовна ентропіяЯкісна варіативність[en]— інші міристатистичної дисперсіїдляномінальних розподілівПримітки[ред.|ред.', 'ukкод]↑Pathria, R. K.; Beale, Paul (2011).Statistical Mechanics (Third Edition).', 'ukAcademic Press.', 'ukс.', 'uk51.ISBN978-0123821881.', 'ukАрхіворигіналуза 17 червня 2020.', 'ukПроцитовано 13 грудня 2018.(англ.', 'uk)↑Shannon, Claude E.(July–October 1948).A Mathematical Theory of Communication.Bell System Technical Journal[en].27(3): 379—423.doi:10.1002/j.1538-7305.1948.tb01338.x.', 'uk(PDF, заархівованозвідси[Архівовано20 червня 2014 уWayback Machine.])(англ.', 'uk)↑Shannon, Claude E. (1948).A Mathematical Theory of Communication(PDF).Bell System Technical Journal[en].27.', 'ukАрхіворигіналу(PDF)за 15 лютого 2019.', 'ukПроцитовано 13 грудня 2018., July and October(англ.', 'uk)↑Schneier, B:Applied Cryptography, Second edition, John Wiley and Sons.(англ.', 'uk)↑Borda, Monica (2011).Fundamentals in Information Theory and Coding.', 'ukSpringer.ISBN978-3-642-20346-6.', 'ukАрхіворигіналуза 15 лютого 2017.', 'ukПроцитовано 6 червня 2017.(англ.', 'uk)↑Han, Te Sun & Kobayashi, Kingo (2002).Mathematics of Information and Coding.', 'ukAmerican Mathematical Society.ISBN978-0-8218-4256-0.', 'ukАрхіворигіналуза 15 лютого 2017.', 'ukПроцитовано 6 червня 2017.(англ.', 'uk)↑Schneider, T.D,Information theory primer with an appendix on logarithms[недоступне посилання], National Cancer Institute, 14 April 2007.(англ.', 'uk)↑Carter, Tom (March 2014).An introduction to information theory and entropy(PDF).', 'ukSanta Fe.', 'ukАрхіворигіналу(PDF)за 4 червня 2016.', 'ukПроцитовано 4 серпня 2017.(англ.', 'uk)↑Compare: Boltzmann, Ludwig (1896, 1898).', 'ukVorlesungen über Gastheorie : 2 Volumes – Leipzig 1895/98 UB: O 5262-6.', 'ukEnglish version: Lectures on gas theory.', 'ukTranslated by Stephen G. Brush (1964) Berkeley: University of California Press; (1995) New York: DoverISBN0-486-68455-5(англ.', 'uk)↑Mark Nelson (24 August 2006).The Hutter Prize.', 'ukАрхіворигіналуза 1 березня 2018.', 'ukПроцитовано 27 листопада 2008.(англ.', 'uk)↑аб\"The World\\'s Technological Capacity to Store, Communicate, and Compute Information\"[Архівовано27 липня 2013 уWayback Machine.', 'uk], Martin Hilbert and Priscila López (2011),Science, 332(6025); free access to the article through here: martinhilbert.net/WorldInfoCapacity.html(англ.', 'uk)↑Massey, James (1994).Guessing and Entropy(PDF).Proc.', 'ukIEEE International Symposium on Information Theory.', 'ukАрхіворигіналу(PDF)за 1 січня 2014.', 'ukПроцитовано 31 грудня 2013.(англ.', 'uk)↑Malone, David; Sullivan, Wayne (2005).Guesswork is not a Substitute for Entropy(PDF).Proceedings of the Information Technology & Telecommunications Conference.', 'ukАрхіворигіналу(PDF)за 15 квітня 2016.', 'ukПроцитовано 31 грудня 2013.(англ.', 'uk)↑Pliam, John (1999).', 'ukGuesswork and variation distance as measures of cipher security.International Workshop on Selected Areas in Cryptography.doi:10.1007/3-540-46513-8_5.(англ.', 'uk)↑абвThomas M. Cover; Joy A. Thomas (18 липня 2006).Elements of Information Theory.', 'ukHoboken, New Jersey: Wiley.ISBN978-0-471-24195-9.(англ.', 'uk)↑Aoki, New Approaches to Macroeconomic Modeling.(англ.', 'uk)↑Probability and Computing, M. Mitzenmacher and E. Upfal, Cambridge University Press(англ.)Література[ред.|ред.', 'ukкод]Підручники з теорії інформації[ред.|ред.', 'ukкод]Теорія інформації і кодування: навч.', 'ukпосіб.', 'uk/ А. Я. Кулик, С. Г. Кривогубченко; Вінниц.', 'ukнац.', 'ukтехн.', 'ukун-т.', 'uk— Вінниця, 2008.', 'uk— 145 c.Теорія інформації: навч.', 'ukпосіб.', 'uk/ Н. О. Тулякова; Сум.', 'ukдерж.', 'ukун-т.', 'uk— Суми, 2008.', 'uk— 212 c.Теорія інформації: навч.', 'ukпосіб.', 'uk/ В. В. Дядичев, Б. М. Локотош, А. В. Колєсніков; Східноукр.', 'ukнац.', 'ukун-т ім.', 'ukВ. Даля.', 'uk— Луганськ, 2009.', 'uk— 249 c.Теорія інформації та кодування / В. С. Василенко, О. Я. Матов; НАН України,Ін-т пробл.', 'ukреєстрації інформації.', 'uk— Київ :ІПРІНАН України, 2014.', 'uk— 439 c.Теорія інформації та кодування: підручник / В. І. Барсов,В.', 'ukА. Краснобаєв, З. В. Барсова, О. І. Тиртишніков, І. В. Авдєєв; ред.', 'uk: В. І. Барсов; МОНМС України, Укр.', 'ukінж.-пед.', 'ukакад., Полтав.', 'ukнац.', 'ukтехн.', 'ukун-т ім.', 'ukЮ. Кондратюка.', 'uk— Полтава, 2011.', 'uk— 321 c.Теорія інформації та кодування: Підруч.', 'ukдля студ.', 'ukвищ.', 'ukтехн.', 'ukнавч.', 'ukзакл.', 'uk/ Ю. П. Жураковський, В. П. Полторак.', 'uk— К. : «Вища шк.», 2001.', 'uk— 255 c.Arndt, C. (2004),Information Measures: Information and its Description in Science and Engineering, Springer,ISBN978-3-540-40855-0(англ.', 'uk)Cover, T. M., Thomas, J.', 'ukA.', 'uk(2006),Elements of information theory, 2nd Edition.', 'ukWiley-Interscience.ISBN0-471-24195-4.(англ.', 'uk)Gray, R. M. (2011),Entropy and Information Theory, Springer.(англ.', 'uk)MacKay, David J. C.[en].Information Theory, Inference, and Learning Algorithms[Архівовано17 лютого 2016 уWayback Machine.', 'uk]Cambridge: Cambridge University Press, 2003.ISBN0-521-64298-1(англ.', 'uk)Martin, Nathaniel F.G. & England, James W. (2011).Mathematical Theory of Entropy.', 'ukCambridge University Press.ISBN978-0-521-17738-2.', 'ukАрхіворигіналуза 19 березня 2015.', 'ukПроцитовано 6 червня 2017.(англ.', 'uk)Мартин Н., Ингленд Дж.Математическая теория энтропии.', 'uk— М. : Мир, 1988.', 'uk— 350 с.(рос.', 'uk)Shannon, C.E.,Weaver, W.(1949)The Mathematical Theory of Communication, Univ of Illinois Press.ISBN0-252-72548-4(англ.', 'uk)Stone, J. V. (2014), Chapter 1 ofInformation Theory: A Tutorial Introduction[Архівовано3 червня 2016 уWayback Machine.', 'uk], University of Sheffield, England.ISBN978-0956372857.(англ.', 'uk)Хэмминг Р. В.', 'uk(1983).Теория информации и теория кодирования.', 'ukМосква: Радио и Связь.(рос.)Посилання[ред.|ред.', 'ukкод]Hazewinkel, Michiel, ред.', 'uk(2001),Entropy,Математична енциклопедія,Springer,ISBN978-1-55608-010-4(англ.', 'uk)Введення в ентропію та інформацію[Архівовано17 травня 2016 уWayback Machine.', 'uk]наPrincipia Cybernetica Web[en](англ.', 'uk)Entropy[Архівовано31 травня 2016 уWayback Machine.', 'uk], міждисциплінарний журнал про всі аспекти поняття ентропії.', 'ukВідкритий доступ.(англ.', 'uk)Опис інформаційної ентропії з «Tools for Thought» Говарда Райнгольда(англ.', 'uk)Аплет Java, який представляє експеримент Шеннона з обчислення ентропії англійської мови[Архівовано13 травня 2016 уWayback Machine.', 'uk]Слайди про приріст інформації та ентропію(англ.', 'uk)An Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science— вікікнига про інтерпретацію поняття ентропії.(англ.', 'uk)Network Event Detection With Entropy Measures[Архівовано13 квітня 2016 уWayback Machine.', 'uk], Dr. Raimund Eimann, University of Auckland, PDF; 5993 kB — докторська праця, яка демонструє, як вимірювання ентропії можуть застосовуватися для виявлення мережевих аномалій.(англ.', 'uk)Entropy[Архівовано4 червня 2016 уWayback Machine.', 'uk]— репозитарійRosetta Code[en]реалізацій ентропії Шеннона різними мовами програмування.', 'ukInformation Theory for Intelligent People[Архівовано13 червня 2020 уWayback Machine.].', 'ukКоротке введення до аксіом теорії інформації, ентропії, взаємної інформації, відстані Кульбака — Лейблера та відстані Єнсена — Шеннона.(англ.', 'uk)Інтерактивний інструмент для обчислення ентропії (простого тексту ASCII)[Архівовано15 квітня 2022 уWayback Machine.', \"uk]Інтерактивний інструмент для обчислення ентропії (двійкового входу)порМетодистиснення данихБезвтратніЕнтропійний типУнарнийАрифметичнийАсиметричнийГоломбаІнтервальнийТанстелла[en]ГаффманаАдаптивний[en]Канонічний[en]Видозмінений[en]ШеннонаШеннона — ФаноШеннона — Фано — Еліаса[en]Універсальні[en]Гамма[en]Експоненційний Голомба[en]Левенштейна[en]ФібоначчіСловниковий тип[en]Кодування пар байтів[en]DeflateSnappyЛемпеля — ЗіваLZ77 / LZ78 (LZ1 / LZ2)LZJB[en]LZMALZO[en]LZRW[en]LZS[en]LZSS[en]LZWLZWL[en]LZX[en]LZ4Brotli[en]Інші типиBWTCTW[en]ДельтаDMC[en]MTF[en]PAQPPMRLEЗвукуПоняттяБітова швидкістьусереднена (ABR)[en]постійна (CBR)[en]змінна (VBR)Динамічний діапазонДискретизаціяЗапізнювання[en]ЗгорткаКодування мовлення[en]КомпандуванняСмугове кодування[en]Теорема Віттакера — Найквіста — Котельникова — ШеннонаЯкість звуку[en]ЧастиникодеківA-законμ-законACELP[en]ADPCMADXCELP[en]DPCMLPCLAR[en]LSP[en]MDCT[en]Перетворення Фур'єПсихоакустична модельWLPC[en]ЗображеньПоняттяАртефакти стисненняКвантування[en]Колірна субдискретизаціяКолірний простірМакроблок[en]Одиниця дерева кодування[en]ПіксельPSNRРоздільна здатністьСтандартне перевірне зображенняМетодиВейвлетнийDCTEZW[en]Ланцюговий код[en]ПірамідаRLESPIHT[en]Теорема Карунена — ЛоеваФрактальнийВідеоПоняттяБітова швидкістьусереднена (ABR)[en]постійна (CBR)[en]змінна (VBR)КадрРоздільність дисплеяТипи кадрівХарактеристики відеоЧастота кадрівЧерезрядкова розгорткаЯкість відеоЧастиникодеківДеблокувальний фільтр[en]DCTКомпенсація рухуПеретворення з перекриттям[en]ТеоріяВтратніЕнтропіяКвантуванняНадмірністьКолмогоровська складністьШвидкість — спотворення[en]Хронологія розвитку теорії інформації[en]Формати стиснення данихПрограмне забезпечення для стиснення данихТематичні сайтиQuoraСловники та енциклопедіїDe Agostini·Encyclopedia Treccani·Encyclopædia Britannica·Encyclopædia Britannica·Treccanis Enciclopedia ItalianaДовідкові виданняBrilliant.org·KBpediaНормативний контрольBNE:XX535116·BNF:11985913j·FAST:912828·Freebase:/m/03zhv·GND:4743861-7·J9U:987007550784405171·LCCN:sh85044152·NDL:01191172·NKC:ph425914Отримано зhttps://uk.wikipedia.org/w/index.php?title=Інформаційна_ентропія&oldid=41892505Категорії:Ентропія й інформаціяТеорія інформаціїСтатистична випадковістьПриховані категорії:Шаблон:Webarchive:посилання на Wayback MachineСтатті з нечинними посиланнямиСтатті, написані занадто складно з грудня 2018Усі статті, написані занадто складноСтатті, що потребують додаткових посилань на джерела з травня 2016Усі статті, що потребують додаткових посилань на джерелаСтатті з твердженнями без джерелСтатті з цитатами без посилань на першоджерелаСтатті, що потребують проясненняНавігаційне менюОсобисті інструментиВи не увійшли до системиОбговоренняВнесокСтворити обліковий записУвійтиПростори назвСтаттяОбговоренняукраїнськаПереглядиЧитатиРедагуватиРедагувати кодПереглянути історіюБільшеПошукНавігаціяГоловна сторінкаПоточні подіїНові редагуванняНові сторінкиВипадкова статтяУчастьПортал спільнотиКнайпаДовідкаПожертвуватиСторінка для медіаІнструментиПосилання сюдиПов'язані редагуванняСпеціальні сторінкиПостійне посиланняІнформація про сторінкуЦитувати сторінкуОтримати вкорочену URL-адресуЗавантажити QR-кодЕлемент ВікіданихСтатистика відвідуваньДрук/експортСтворити книгуЗавантажити як PDFВерсія до друкуВ інших проєктахВікісховищеІншими мовамиAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeاردوTiếng Việt中文粵語Редагувати посиланняЦю сторінку востаннє відредаговано о 10:42, 2 березня 2024.Текст доступний на умовах ліцензіїCreative Commons Attribution-ShareAlike; також можуть діяти додаткові умови.\", \"ukДетальніше див.Умови використання.Політика конфіденційностіПро ВікіпедіюВідмова від відповідальностіЗворотний зв'язокКодекс поведінкиРозробникиСтатистикаКукиМобільний вигляд\", 'plEntropia (teoria informacji) – Wikipedia, wolna encyklopediaPrzejdź do zawartościMenu główneMenu główneprzypnijukryjNawigacjaStrona głównaLosuj artykułKategorie artykułówNajlepsze artykułyCzęste pytania (FAQ)Dla czytelnikówO WikipediiKontaktWspomóż WikipedięDla wikipedystówPierwsze krokiPortal wikipedystówOgłoszeniaZasadyPomocOstatnie zmianySzukajSzukajUtwórz kontoZaloguj sięNarzędzia osobisteUtwórz kontoZaloguj sięStrony dla anonimowych edytorówdowiedz się więcejEdycjeDyskusjaSpis treściprzypnijukryjPoczątek1Przykład2Zobacz też3Przypisy4Linki zewnętrznePrzełącz stan spisu treściEntropia (teoria informacji)45 językówAfrikaansالعربيةБългарскиBoarischBosanskiCatalàČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalego한국어Bahasa IndonesiaItalianoעבריתLietuviųMagyarNederlands日本語Олык марийਪੰਜਾਬੀPortuguêsRomânăРусскийShqipSimple EnglishSlovenčinaSlovenščinaکوردیСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt粵語中文Edytuj linkiArtykułDyskusjapolskiCzytajEdytujEdytuj kod źródłowyWyświetl historięNarzędziaNarzędziaprzypnijukryjDziałaniaCzytajEdytujEdytuj kod źródłowyWyświetl historięOgólneLinkująceZmiany w linkowanychPrześlij plikStrony specjalneLink do tej wersjiInformacje o tej stronieCytowanie tego artykułuZobacz skrócony adres URLPobierz kod QRElement WikidanychDrukuj lub eksportujUtwórz książkęPobierz jako PDFWersja do drukuW innych projektachWikimedia CommonsZ Wikipedii, wolnej encyklopediiTen artykuł dotyczy pojęcia z dziedziny teorii informacji.', 'plZobacz też:inne znaczenia tego terminu.Entropia– średniailość informacji, przypadająca na pojedynczą wiadomość ze źródła informacji.', 'plInnymi słowy jest tośrednia ważonailości informacji niesionej przez pojedynczą wiadomość, gdzie wagami sąprawdopodobieństwanadania poszczególnych wiadomości.Wzór na entropię zmiennej losowejXo zbiorze wartości,x_,\\\\dots ,x_\\\\}}[1]:H(X)=∑i=1np(xi)logr\\u20611p(xi)=−∑i=1np(xi)logr\\u2061p(xi),^p(x_)\\\\log _)}}=-\\\\sum _^p(x_)\\\\log _)},}gdziep(xi))}to prawdopodobieństwo zajścia zdarzeniaxi,,}arto podstawa logarytmu.', 'plW teorii informacji najczęściej stosuje sięlogarytmo podstawie 2, wówczas jednostką entropii jestbit.', 'plDlar=ejednostka ta nazywa sięnat(nit), natomiast dlar=10–ditlub hartley.', 'plW przypadku gdyp(xi)=0)=0}dla pewnegoi,wartość składnika0logr\\u20610}jest przyjmowana jako 0, co jest zgodne z granicą:limp→0+plog\\u2061(p)=0.p\\\\log(p)=0.', 'pl}W latach 60.', 'plXX wieku węgierski matematykAlfred Rényiuogólnił pojęcie entropii do zbioru funkcji za pomocą których można opisać ilościowo różnorodność, niepewność czylosowośćsystemu.', 'plMiara ta od jego nazwiska nazywana jestentropią Rényi.Entropię można interpretować jako niepewność wystąpienia danegozdarzenia elementarnegow następnej chwili.', 'plJeżeli jakieś zdarzenie w zbiorze zdarzeń występuje z prawdopodobieństwem równym 1, to entropia układu wynosi wówczas 0, gdyż z góry wiadomo, co się stanie – nie ma niepewności.Własności entropii:jest nieujemna;jest maksymalna, gdy prawdopodobieństwa zajść zdarzeń są takie same (maksymalna niepewność)[1];jest równa 0, gdy prawdopodobieństwa stanów systemu poza jednym wynoszą 0, a jednego stanu – 1 (maksymalna pewność)[1];własność superpozycji – gdy dwa systemy są niezależne, to entropia sumy systemów równa się sumie entropii[2];jeśli ze źródła danych pobierane są k-literowe ciągi, wówczas entropia wynosiH(X(k))=kH(X).)=kH(X).', 'pl}Definicja informacyjna była pierwotnie próbą ujęcia tradycyjnego pojęciaentropiiznanego ztermodynamikiw kategoriach teorii informacji.', 'plOkazało się jednak, że definicja ta jest przydatna w ramach samej teorii informacji.Pojęcie entropii jest bardzo przydatne np.', 'plw dziedziniekompresji danych.', 'plEntropię zerowego rzędu można obliczyć znająchistogramciągu symboli.', 'plJest to iloczyn entropii i liczby znaków w ciągu.', 'plOsiągikodowania Huffmanasą często zbliżone do tej granicy, jednak lepszą efektywnością charakteryzuje siękodowanie arytmetyczne.Przyjęcie modelu, w którym uwzględnia się kontekst znaku, pozwala zwykle na bardzo duże obniżenie entropii.Przykład[edytuj|edytuj kod]W przypadku, gdy prawdopodobieństwa poszczególnych zdarzeń w zbiorze są równe, powyższy wzór można stosować w postaci uproszczonej:H(X)=log2\\u2061(n),(n),}gdzie:noznacza wielkość zbioru.', 'plPrzykładowo dla zbioru 26 liter alfabetu(n=26)entropia każdej z nich wynosi około 4,7, więc ośmioznakowy ciąg liter wykorzystywany np.', 'pljakohasłobędzie miał entropię 37,6.Moneta, która wyrzuca z takim samym prawdopodobieństwem orły i reszki, ma 1 bit entropii na rzut:−pOlog2\\u2061pO−pRlog2\\u2061pR=−12log2\\u206112−12log2\\u206112=12+12=1.\\\\log _p_-p_\\\\log _p_=-}\\\\log _}-}\\\\log _}=}+}=1.', 'pl}Jednakże jeśli moneta z jakieś przyczyny daje zafałszowany wynik (statystycznie częściej daje albo orła albo reszkę z określonym prawdopodobieństwem) mamy do czynienia z sytuacją, w której jest mniejsza niepewność (możemy łatwiej przewidzieć wynik).', 'plObjawia się to niższą entropią.', 'plPrzykładowo, jeśli założymy, że z czterech rzutów wypadły 3 reszki to podstawiając do wzoru otrzymamy entropię równą 0,81.', 'plIdąc doekstremum, przy czterech rzutach i 4 reszkach lub 4 orłach entropia osiąga minimum, czyli 0, ponieważ nie ma niepewności (wiemy co wydarzy się w następnym rzucie).', 'plPrzedstawiony przykład jest skrajnie uproszczony i próba czterech rzutów jest za mała, aby wyciągać jakieś statystyczne wnioski, ale dobrze obrazuje problem.Ogólniej każde źródło dająceNrównie prawdopodobnych wyników malog2NN}bitów na symbol entropii:−∑i=1N1Nlog2\\u20611N=−N1Nlog2\\u20611N=−log2\\u20611N=log2\\u2061N.^}\\\\log _}=-N}\\\\log _}=-\\\\log _}=\\\\log _N.', 'pl}Ponadto inną miarą związaną z entropiąShannonajestentropia metryczna, która uwzględnia długość informacji (entropia dzielona jest przez długość wiadomości) i pozwala zmierzyć losowość informacji.Zobacz też[edytuj|edytuj kod]Zobacz hasłoentropiaw Wikisłownikuentropia produktowaentropia krzyżowaentropia warunkowainformacja wzajemnałańcuch Markowateoria informacjiPrzypisy[edytuj|edytuj kod]↑abcClaude ElwoodC.E.ShannonClaude ElwoodC.E.,A Mathematical Theory of Communication, University of Illinois Press, 1949, s. 11.↑Claude ElwoodC.E.ShannonClaude ElwoodC.E.,A Mathematical Theory of Communication, University of Illinois Press, 1949, s. 12.Linki zewnętrzne[edytuj|edytuj kod]kalkulator entropii– jak obliczyć i interpretować entropię dowolnej wiadomościM.', 'plBerta, O. Fawzi, M. Tomamichel,On Variational Expressions for Quantum Relative Entropies,arxiv.org/1512.02615Kontrola autorytatywna(wyrażenie matematyczne):LCCN:sh85044152GND:4743861-7NDL:01191172BnF:11985913jNKC:ph425914BNE:XX535116J9U:987007550784405171Encyklopedia internetowa:Britannica:topic/entropy-information-theory,topic/Shannons-entropyTreccani:entropiaŹródło: „https://pl.wikipedia.org/w/index.php?title=Entropia_(teoria_informacji)&oldid=70730932”Kategoria:Teoria informacjiTę stronę ostatnio edytowano 25 cze 2023, 23:50.Tekst udostępniany nalicencji Creative Commons: uznanie autorstwa, na tych samych warunkach, z możliwością obowiązywania dodatkowych ograniczeń.', 'plZobacz szczegółowe informacje owarunkach korzystania.Polityka prywatnościO WikipediiKorzystasz z Wikipedii tylko na własną odpowiedzialnośćPowszechne Zasady PostępowaniaDla deweloperówStatystykiKomunikat na temat ciasteczekWersja mobilnaPrzełącz ograniczenie szerokości strony', 'ptEntropia da informação – Wikipédia, a enciclopédia livreSaltar para o conteúdoMenu principalMenu principalmover para a barra lateralocultarNavegaçãoPágina principalConteúdo destacadoEventos atuaisEsplanadaPágina aleatóriaPortaisInformar um erroColaboraçãoBoas-vindasAjudaPágina de testesPortal comunitárioMudanças recentesManutençãoCriar páginaPáginas novasContatoDonativosBuscaPesquisarCriar uma contaEntrarFerramentas pessoaisCriar uma contaEntrarPáginas para editores sem sessão iniciadasaber maisContribuiçõesDiscussão[ocultar]Conteúdomover para a barra lateralocultarInício1Contexto Histórico2Aplicações diversas da Entropia3Entropia e Informação4Entropia como conceito da Teoria da Informação5Ver também6Referências7Ligações externasAlternar o índiceEntropia da informação45 línguasAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Editar hiperligaçõesArtigoDiscussãoportuguêsLerEditarEditar código-fonteVer históricoFerramentasFerramentasmover para a barra lateralocultarOperaçõesLerEditarEditar código-fonteVer históricoGeralPáginas afluentesAlterações relacionadasCarregar ficheiroPáginas especiaisHiperligação permanenteInformações da páginaCitar esta páginaObter URL encurtadoDownload QR codeElemento WikidataImprimir/exportarCriar um livroDescarregar como PDFVersão para impressãoNoutros projetosWikimedia CommonsOrigem: Wikipédia, a enciclopédia livre.Esta páginacita fontes, mas quenão cobremtodo o conteúdo.Ajude ainserir referências.', 'ptConteúdo nãoverificávelpode serremovido.—Encontre fontes:ABW•CAPES•Google(N•L•A)(Fevereiro de 2014)Entropia, quando relacionada àtermodinâmica, é a medida do grau de irreversibilidade de um determinado sistema.', 'ptEntão, quanto menor a chance do sistema voltar ao seu estado original, maior será o grau de entropia.', 'ptÉ considerada porEinsteincomo a primeira lei de todas a ciências.Para a área deTeoria da Informação, aentropiaé definida como sendo uma forma de medir o grau médio de incerteza a respeito de fontes de informação, o que consequentemente permite a quantificação da informação presente que flui no sistema.', 'ptEm termos simples, o conceito de entropia se associa à ideia de que, quanto mais incerto é o resultado de um experimento aleatório, maior é a informação que se obtém ao observar a sua ocorrência.Contexto Histórico[editar|editar código-fonte]O engenheiro e matemáticoClaude Shannoné considerado o pai da teoria da informação.', 'ptAmericano, atuou nas áreas deengenharia elétrica, comunicações, criptografia e xadrez computacional, instalando bases tanto para a indústria de computadores quanto para a de telecomunicações.', 'ptShannon introduziu, em 1948, no trabalhoA Mathematical Theory of Communication,[1]publicado na revistaBell System Technical Journal, conceitos primordiais que deram origem à teoria da informação, entre eles a entropia da informação e a capacidade de canal.Nesse trabalho, Shannon desenvolve um modelo teórico para o problema da comunicação envolvendo a transmissão de mensagens digitais.', 'ptEle parte do pressuposto de que uma mensagem transmitida para um receptor é invariavelmente recebida com ruídos, isto é, quando uma informação passa por um canal de comunicação ela sofre distorções e chega ao receptor precisando ser submetida a um processo de decodificação.', 'ptShannon então demonstrou matematicamente que, se por exemplo consideramos a transmissão de um sinal ruidoso como uma conversa barulhenta, este pode ser recuperado no transmissor sem distorção, respeitadas características como a capacidade de transmissão daquele meio (canal).', 'ptBasta nesse sentido empregar umcódigo corretor de errosadequado, de modo que os sinais serão recebidos com probabilidade de erro arbitrariamente pequena.', 'ptAs definições de Shannon tiveram impacto direto no desenvolvimento das comunicações, da criptografia, da computação digital, da codificação de sinais (vídeo, imagem, texto), assim como em problemas da linguística, psicologia e fonética.', 'ptA linguagem, por exemplo, é um mecanismo de comunicação que tem um código de correção de erros embutido nas suas regras de sintaxe, ortografia e semântica.Norbert Wiener, o fundador da Cibernética, na mesma época, generalizou o conceito de entropia, associando-a ao processo de comunicação ou informação, afirmando que, nos processos onde há perda de informação, há uma situação igual aos processos que ganham entropia.', 'ptSegundo Wiener, a soma de informação em umsistemaé a medida de seu grau de organização e, ao mesmo tempo, de seu grau de desorganização, sendo assim, um é o negativo do outro.Segundo Wierner:\"as mensagens são em si uma forma de padrão e de organização.', 'ptCom efeito é possível tratar conjuntos de mensagens como tendo uma entropia, tais como conjuntos de estados do mundo exterior.', 'ptAssim como a entropia é uma medida da desorganização, a informação transmitida por um conjunto de mensagens é uma medida de organização.', 'ptDe fato é possível interpretar a informação de uma mensagem essencialmente como o negativo de sua entropia e o logarítmo de sua probabilidade.', 'ptIsto é, quanto mais provável é a mensagem, menor é a informação fornecida.', 'ptLugares-comuns, por exemplo, são menos esclarecedores do que grandes poemas.', 'pt[2]Aplicações diversas da Entropia[editar|editar código-fonte]Nos anos seguintes a criação do conceito, outros pesquisadores o modificaram e generalizaram, criando outras formas de entropia.', 'ptLogo, o conceito da entropia de informação foi aplicado nas mais diversas áreas de conhecimento, como por exemploCiência da Informação, biologia, medicina, ecologia, economia e linguística, assim se tornando cada vez mais um método interdisciplinar.Um dos exemplos da consequência de entropia é o primeiro experimento relacionado à química que demonstra uma auto-organização de partículas duras, não-biológicas, sem ajuda de interações atrativas, como ligações químicas.', 'ptCristais complexos foram criados sem a ajuda de interações, como por exemplo os tetraedros se organizaram espontaneamente em um quasicristal, este, por vez, terá suas propriedades diferentes de um cristal ou de um sólido comum, ele poderá ter propriedade ópticas únicas.Na termodinâmica, a entropia tem um papel fundamental, pois é com não-conservação da entropia, ou seja, com a variação de desordem do sistema que máquinas térmicas funcionam.', 'ptA compreensão do funcionamento dessas máquinas leva ao conceito da entropia do sistema.Entropia e Informação[editar|editar código-fonte]Informação, de acordo com diferentes autores, é um termo que vem sendo usado mais a partir da década de1950.', 'ptÉ usado para significar mensagens, notícias, novidades, dados, conhecimento, literatura, símbolos, signos e, até mesmo, \"dicas\" e sugestões.', 'ptPode até parecer um termo vago, sendo necessário um contexto em que é empregada.', 'ptGeorges Ifrah apresenta pelo menos 26 diferentes conceitos de informaçãoDiferentemente da energia, a informação é algo que se cria e que existe cada vez mais em maior quantidade no nosso Universo.', 'ptSegundo Zdenek Zeman, \"a expressão da informação de um sistema tem por base, como se sabe, a fórmula matemática da entropia negativa\".', 'ptPartido dessa ideia, informação, isto é, deentropia negativa, pode exprimir, também, a medida da ordem de um sistema nervoso ou de um sistema social.', 'ptA entropia negativa é atingida quando a probabilidade de ocorrer todos os símbolos é igual, ou seja, é equiprovável e não há a tendência de ocorrer determinado grupo de símbolos.Shannon abordou, também o conceito de redundância é relacionado à entropia no sentido de que a redundância é tudo o que não é fundamental para ser entendido em uma determinada mensagem, ou seja, é entendida como algo complementar.', 'ptEntão é a medida de entropia para que a mensagem atinja a entropia máxima.No trabalho desenvolvido por ele em A Teoria Matemática da Comunicação onde é abordada a relação entre a entropia e a informação, Shannon faz a afirmação que a informação contida em uma mensagem pode ser medida pela quantidade de entropia que, por sua vez, é relacionada à frequência dos grupo de símbolos que são transmitidos.A teoria da informação afirma que quanto menos informações sobre um sistema, maior será sua entropia.', 'ptA quantidade de informação de uma mensagem é entendida na teoria da informação como sendo o menor número debits, unidade de informação, necessários para conter todos os valores ou significados desta mensagem.', 'ptPor exemplo, se quisermos transmitir ou armazenar os números dos meses do ano, serão necessários, no mínimo, 4 bits para representar esta informação.', 'ptPortanto, a quantidade de informação de uma mensagem é medida pela entropia da mensagem, a qual mede, também a sua incerteza, que é expressa pelo número de bits que precisam ser recuperados quando a mensagem está cifrada para obter novamente um texto claro.Entropia como conceito da Teoria da Informação[editar|editar código-fonte]A Teoria da Informação teve inicialmente como destaque as questões técnicas, sendo uma das primeiras teorias a separar com nitidez a informação da significação.', 'ptA Teoria da Informação está situada dentro da cibernética, onde a informação se mostra como uma medida probabilística.', 'ptEsta teoria tem um grande interesse pelo funcionamento dos sinais, pelas transformações energéticas mediante a codificação da mensagem e sua de codificação.', 'ptEla opera com os seguintes conceitos:ruído;redundância;entropia;imprevisibilidade.Em fontes contínuas, a codificação da informação gera ruído na mensagem, isso se dá pelo fato de que a fonte contínua precisaria de um vasto repertório de símbolos e que, como consequência, necessitaria uma capacidade de transmissão grande e, como é sabido, não existe um canal livre de ruído.Shannon abordou, também o conceito de redundância é relacionado à entropia no sentido de que a redundância é tudo o que não é fundamental para ser entendido em uma determinada mensagem, ou seja, é entendida como algo complementar.', 'ptEntão é a medida de entropia para que a mensagem atinja a entropia máxima.A entropia desejada de uma informação é a máxima que é dada pelas probabilidades equivalentes de ocorrer todos os símbolos.A teoria da informação não estuda uma língua pelo número de símbolos alfabéticos que a compõem, mas sim pela análise à redundância na língua, considerando que o inverso da entropia é a redundância, ou seja, a organização do sistema em questão.', 'ptUma língua entrópica dispõe de um vocabulário rico, com palavras diferenciadas, que mostram o poder das combinatórias; uma língua pouco entrópica é pobre e repetitiva.Em relação a imprevisibilidade, quanto maior for, será menor a chance de apreensão por parte do receptor, pois o receptor depende da ordem em que as mensagens são transmitidas.', 'ptA imprevisibilidade total é correspondente à informação nula, ou seja, não há informação.A medida da informação (ousurpresa) de um eventoEé uma função que decresce a medida em que a probabilidadep(E)do evento se eleva.', 'ptEla pode ser calculada a partir da fórmula de Hartley (1928):I(E)=−log2\\u2061(p(E))(p(E))}Aqui,bé a unidade da informação (Por exemplo: Informação binária,b=2).', 'ptJá entropia pode ser explicitamente escrita comoH(X)=−∑i=1nP(xi)logb\\u2061P(xi) (X)=-\\\\sum _^ (x_)\\\\log _\\\\mathrm (x_)}}ondeP(xi) (x_)}é a probabilidade do i-ésimo resultado para a variávelx.Uma maneira mais simples de medir a entropia é perceber que há duas possibilidades de ocorrência de um evento, como no caso do lançamento de uma moeda em um jogo de cara-ou-coroa com moeda viciada.', 'ptNesse caso temos p e q, onde q = 1-p, e a entropia do sistema é calculada comoH(X)=−(plog2\\u2061p+qlog2\\u2061q) (X)=-(p\\\\log _p+q\\\\log _q)}Ver também[editar|editar código-fonte]InformaçãoSociedade da InformaçãoEntropiaReferências↑Shannon, Claude E. (1948) \"A mathematical theory of communication\",Bell System Technical Journal.', 'pt27(3): 379–423.doi:10.1002/j.1538-7305.1948.tb01338.xhttp://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf↑WIERNER, Norbert (1970).Cibernética e sociedade: o uso humano de seres humanos.', 'ptSão Paulo: Cultrixhttps://www.universoracionalista.org/a-morte-e-a-vitoria-da-entropia/http://www.inovacaotecnologica.com.br/noticias/noticia.php?artigo=ordem-partir-desordem-entropia-sozinha-cria-cristais-complexos#.VyilS4-cHIULivro: Informação, Linguagem e Comunicação.', 'ptDécio Pignatari - Publicado por Atelie Editorial, 2003.Ligações externas[editar|editar código-fonte]Entropia (teoria da informação).Bernardo N. B. Lima, Leandro Martins Cioletti, Marcelo de O. Terra Cunha, Gastãoo A. Braga;Entropia: introdução à Teoria Matemática da (des)Informação; Departamento de Matemática - UFMG.Nestor Caticha -Física e Informação; IF USP ; 21/11/2006.Este artigo é umesboço.', 'ptVocê pode ajudar a Wikipédiaexpandindo-o.Editor: considere marcar com umesboço mais específico.Obtida de \"https://pt.wikipedia.org/w/index.php?title=Entropia_da_informação&oldid=64849427\"Categorias:Teoria da informaçãoAleatoriedadeEntropiaCategorias ocultas:!Artigos que carecem de notas de rodapé desde fevereiro de 2014!Artigos que carecem de notas de rodapé sem indicação de tema!Esboços!Esboços maiores que 10000 bytesEsta página foi editada pela última vez às 21h58min de 3 de dezembro de 2022.Este texto é disponibilizado nos termos da licençaAtribuição-CompartilhaIgual 4.0 Internacional (CC BY-SA 4.0) da Creative Commons; pode estar sujeito a condições adicionais.', \"ptPara mais detalhes, consulte ascondições de utilização.Política de privacidadeSobre a WikipédiaAvisos geraisCódigo de condutaProgramadoresEstatísticasDeclaração sobre ''cookies''Versão móvelAlternar a largura de conteúdo limitada\", \"arاعتلاج (نظرية المعلومات) - ويكيبيدياانتقل إلى المحتوىالقائمة الرئيسيةالقائمة الرئيسيةانقل للشريط الجانبيأخفالموسوعةالصفحة الرئيسيةالأحداث الجاريةأحدث التغييراتأحدث التغييرات الأساسيةتصفحالمواضيعأبجديبواباتمقالة عشوائيةتصفح من غير إنترنتمشاركةتواصل مع ويكيبيديامساعدةالميدانتبرعبحثبحثإنشاء حسابدخولأدوات شخصيةإنشاء حسابدخولصفحات للمحررين الذين سجَّلوا خروجهمتعلَّم المزيدمساهماتنقاشتضامنًامع حق الشعبالفلسطينيلاللإبادة الجماعية في غزة....لالقتل المدنيينلالاستهداف المستشفيات والمدارس....لاللتضليل والكيل بمكيالينأوقفوا الحرب.... وانشروا السلام العادل والشاملالمحتوياتانقل للشريط الجانبيأخفالمقدمة1المقدمة2تعريف3مثال4الأساس المنطقي5اعتباراتثبِّت القسم الفرعي اعتبارات5.1العلاقة مع الانتروبيا الديناميكية الحرارية5.2الانتروبيا كمحتوى للمعلومات5.3الانتروبيا كمقياس للتنوع5.4ضغط البيانات5.5القدرة التكنولوجية في العالم على تخزين ونقل المعلومات5.6حدود الانتروبيا كمحتوى المعلومات5.7نهايات الانتروبيا في التعمية5.8البيانات كعملية ماركوف5.9انتروبيb-ary6الكفاءة (الانتروبيا المقيسة)7تمييزثبِّت القسم الفرعي تمييز7.1استمرارية7.2تناظر7.3الحد الأقصى7.4الإضافة8خصائص أخرى9توسيع الإنتروبيا المتقطعة إلى الحالة المستمرةثبِّت القسم الفرعي توسيع الإنتروبيا المتقطعة إلى الحالة المستمرة9.1الانتروبيا التفاضلية9.2الحد من كثافة النقاط المنفصلة9.3الانتروبيا النسبية10استخدم في التوافيقثبِّت القسم الفرعي استخدم في التوافيق10.1متراجحة- لوميس ويتني10.2التقريب لمعامل ذي الحدين (binomial coefficient)11انظر أيضًا12المراجع13قراءة متعمقةثبِّت القسم الفرعي قراءة متعمقة13.1كتب عن نظرية المعلومات14روابط خارجيةتبديل عرض جدول المحتوياتاعتلاج (نظرية المعلومات)45 لغةAfrikaansBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語عدل الوصلاتمقالةنقاشالعربيةاقرأعدّلتاريخأدواتأدواتانقل للشريط الجانبيأخفإجراءاتاقرأعدّلتاريخعامماذا يصل هناتغييرات ذات علاقةرفع ملفالصفحات الخاصةوصلة دائمةمعلومات الصفحةاستشهد بهذه الصفحةاحصل على مسار مختصرتحميل رمز ال QRعنصر ويكي بياناتطباعة/تصديرإنشاء كتابتحميل PDFنسخة للطباعةفي مشاريع أخرىويكيميديا كومنزمن ويكيبيديا، الموسوعة الحرةاعتلاج المعلوماتمعلومات عامةصنف فرعي منالمضمون المعلوميكمية فيزيائيةجانب من جوانبمعلوماتيدرسهديناميكا حراريةنظرية المعلوماتالمكتشف أو المخترعكلود شانونالبعد حسب النظام الدولي للكميات1تعريف الصيغةH(X)=∑i=1np(xi)I(xi)^p(x_)I(x_)}[1]الرموز في الصيغةHpIرمز الكمية (LaTeX)H(X)[2]H[1]وحدة القياس الموصى بهاشانون (وحدة قياس)[3]هارتلي (وحدة)[4]نات (وحدة)[5]النقيضnegentropy(en)تعديل-تعديل مصدري-تعديل ويكي بياناتميّز عنترميز إنتروبي.نظرية المعلوماتاعتلاجاعتلاج تفاضلي[الإنجليزية]اعتلاج شرطياعتلاج مشتركمعلومات متبادلةConditional mutual informationتباعد كولباك - ليبليرEntropy rateإسرافLimiting density of discrete pointsترميزخاصية تصحيح الخطأكشف وتصحيح أخطاءAsymptotic equipartition propertyRate–distortion theoryShannon's source coding theoremسعة القناةنظرية تكويد القناة الصاخبةنظرية شانون هارتليعنتاعتلاج المعلومات(بالإنجليزية:information entropy)\\u200f أوإنتروبيأوأنتروبيا، هي كمية أساسية فينظرية المعلوماتمرتبطة بأيمتغير عشوائي، والتي يمكن تفسيرها على أنها متوسط مستوى «المعلومات» أو «المفاجأة» أو «عدم اليقين» المتأصل في النتائج المحتملة للمتغير.\", 'arتم تقديم مفهوم انتروبيا المعلومات من قبلكلود شانونفي ورقته عام 1948 «نظرية رياضية في الاتصال».', 'ar[6][7]الإنتروبي هيالقيمة المتوقعةللمعلومات الذاتية، وهي كمية ذات صلة قدمها شانون أيضًا.', 'arيقيس المعلومات الذاتية مستوى المعلومات أو المفاجأة المرتبطة بالنتيجة الواحدةالمعينة أو حدث متغير عشوائي، في حين أن تقيس انتروبيا كم هو«مفيد» أو «مدهش»المتغير العشوائي كله،الذي بلغت متوسط كل نتائجه المحتملة.تم إنشاء الإنتروبي في الأصل من قبل شانون كجزء من نظريته في الاتصال، حيث يتكون نظاماتصالات البياناتمن ثلاثة عناصر: مصدر البيانات،وقناة اتصال، وجهاز استقبال.', 'arفي نظرية شانون، إن «مشكلة الاتصال الأساسية» - كما عبر عنها شانون - هي أن يكون المتلقي قادرًا على تحديد البيانات التي تم إنشاؤها بواسطة المصدر، بناءً على الإشارة التي يتلقاها عبر القناة.', 'ar[6][7]نظر شانون في طرق مختلفة لتعمية الرسائل وضغطها ونقلها من مصدر بيانات، وأثبت في نظريته الشهيرة لترميز المصدر أن الإنتروبيا تمثل حدًا رياضيًا مطلقًا حول كيفية ضغط البيانات من المصدر دونضياععلى قناة خالية من التشويش تمامًا.', 'arعزز شانون هذه النتيجة بشكل كبير للقنوات الصاخبة في نظرية الترميز ذات القناة الصاخبة.يمكن تفسير الإنتروبيا أيضًا على أنهامتوسطمعدل إنتاجالمعلومات منخلال مصدراحصائيللبيانات.', 'arعندما ينتج مصدر البيانات قيمة احتمالية منخفضة (على سبيل المثال، عند وقوع حدث احتمالية منخفضة)، يحمل الحدث «معلومات» أكثر مما ينتج عندما ينتج مصدر البيانات قيمة احتمالية عالية.', 'arيُمَثَّل مفهوم «المعلومات» رسميًا من خلال كمية المعلومات الذاتية لشانون، ويتم أيضًا تفسيرها أحيانًا على أنها «مفاجأة».', 'arثم تصبح كمية المعلومات التي ينقلها كل حدث فرديمتغيرًا عشوائيًاقيمته المتوقعة هي إنتروبيا المعلومات.ليكن المتغير العشوائيX، مع النتائج المحتملةxi}، لكل منها احتمالPX(xi)(x_)}انتروبيH(X)منXعلى النحو التالي:H(X)=−∑iPX(xi)logb\\u2061PX(xi)=∑iPX(xi)IX(xi)=E\\u2061[IX]H(X)=-\\\\sum _P_(x_)\\\\log _(x_)}=\\\\sum _P_(x_)I_(x_)=\\\\operatorname [I_]حيثIX(xi)(x_)}هي المعلومات الذاتية المرتبطة بنتيجة معينة؛IX}هي المعلومات الذاتية للمتغير العشوائيXبشكل عام، يعامل كمتغير عشوائي مشتق جديد؛ وE\\u2061[IX] [I_]}هيالقيمة المتوقعةلهذا المتغير العشوائي الجديد، وهي تساوي مجموع المعلومات الذاتية لكل نتيجة، ومرجحة باحتمال حدوث كل نتيجة[8]؛ وb، أساس اللوغاريتم، هو معلمة جديدة يمكن تعيينها بطرق مختلفة لتحديد اختيار الوحدات لإنتروبيا المعلومات.يتم قياس انتروبي المعلومات عادة بوحدة البت(وتسمى بدلا من ذلك «شانون»)، الموافق أساس 2 في المعادلة المذكورة أعلاه.', 'arكما يتم قياسه أحيانًا «بوحدات طبيعية» (ناتسnats)، تقابل الأساسeأو الأرقام العشرية (تسمى \"dits\" أو \"bans\" أو \" hartleys \")، المقابلة للأساس 10.تعريف شانون فريد بشكل أساسي من حيث أنه هو الوحيد الذي له خصائص معينة: يتم تحديده بالكامل من خلال التوزيع الاحتمالي لمصدر البيانات، وهو اضافي للمصادر المستقلة، يتم تعظيمه في التوزيع الموحد، يتم تصغيره (وتساوي الصفر) عندما يكون هناك احتمال بنسبة 100٪ لحدوث حدث واحد فقط، ويطيع نسخة معينة مشتقة من قاعدة سلسلة الاحتمالات.', 'arيتم شرح الاشتقاقات البديهية للانتروبيا في الصفحة أدناه.يتشابه تعريف الإنتروبيا المستخدم في نظرية المعلومات بشكل مباشر مع التعريف المستخدم فيالديناميكا الحرارية الإحصائية، وهي علاقة تم تفصيلها في الصفحة إنتروبي في الديناميكا الحرارية ونظرية المعلومات.المقدمة[عدل]الفكرة الأساسية لنظرية المعلومات هي أن «القيمة المعلوماتية» للرسالة التي يتم توصيلها تعتمد على درجة مفاجأة محتوى الرسالة.', 'arإذا كان الحدث محتملاً للغاية، فليس من المستغرب (و غير مهم بشكل عام) عندما يحدث هذا الحدث كما هو متوقع؛ وبالتالي فإن إرسال مثل هذه الرسالة يحمل القليل من المعلومات الجديدة.', 'arومع ذلك، إذا كان من غير المحتمل حدوث حدث، فمن المفيد جدًا معرفة أن الحدث قد وقع أو سيحدث.', 'arعلى سبيل المثال، معرفة أن بعض الأرقام المحددةلنتكون الرقم الفائز في اليانصيب تقدم معلومات قليلة جدًا، لأن أي رقم محدد تم اختياره لن يكسب بالتأكيد.', 'arومع ذلك، معرفة أن عدد معينسيفوزاليانصيب لها قيمة عالية لأنه يتصل مع نتيجة لحدث احتمال ضعيف جدا.محتوى المعلومات (يسمى أيضًاالمفاجأة) للحدثEهي دالة متزايدة للتبادلية للاحتمالp(E)الحدث، على وجه التحديدI(E)=−log2\\u2061(p(E))=log2\\u2061(1/p(E))(p(E))=\\\\log _(1/p(E))}.', 'arيقيس الانتروبيا الكمية المتوقعة (أي المتوسطة) من المعلومات المنقولة عن طريق تحديد نتيجة تجربة عشوائية.', 'arهذا يعني أن رمي النرد له إنتروبيا أعلى من رمي عملة معدنية لأن كل نتيجة لرمي النرد لها احتمال أقل (حولp=1/6) من كل نتيجة رمي عملة (p=1/2).الإنتروبيا هي مقياس لعدمالقدرةعلىالتنبؤبالحالة، أو ما يعادلها منمتوسط محتوى المعلومات.', 'arللحصول على فهم بديهي لهذه المصطلحات، فكر في مثال استطلاع سياسي.', 'arعادةً، تحدث مثل هذه الاستطلاعات لأن نتيجة الاستطلاع غير معروفة بالفعل.', 'arوبعبارة أخرى، فإن نتائج الاستطلاع غيرمتوقعةنسبيًا، كما أن إجراء الاستطلاع وتعلم النتائج يعطي بعضالمعلوماتالجديدة؛ هذه مجرد طرق مختلفة لقول أن الإنتروبياالأوليةلنتائج الاستطلاع كبيرة.', 'arالآن، ضع في اعتبارك حالة إجراء نفس الاستطلاع مرة أخرى بعد وقت قصير من الاستطلاع الأول.', 'arبما أن نتيجة الاستطلاع الأول معروفة بالفعل، يمكن توقع نتائج الاستطلاع الثاني بشكل جيد ويجب ألا تحتوي النتائج على معلومات جديدة كثيرة؛ في هذه الحالة،يكونالانتروبياالأوليةلنتائج الاستطلاع الثانية صغيرة بالنسبة إلى النتيجة الأولى.تأمل في مثالرمي العملة.', 'arإذا كان احتمال الرأس هو نفسه احتمال ذيول، فإن الإنتروبيا في رمي العملة عالية كما يمكن أن تكون لمحاكمة ذات نتيجتين.', 'arلا توجد طريقة للتنبؤ بنتائج رمي العملة في وقت مبكر: إذا كان على المرء أن يختار، فلا يوجد متوسط ميزة يمكن اكتسابها من خلال التنبؤ بأن الرمي سيخرج رؤوسًا أو ذيول، حيث سيكون أي توقع صحيحًا مع الاحتمال1/2.', 'arمثل رمي العملة هذا لديه جزء من الانتروبيا حيث أن هناك نتيجتين محتملتين تحدثان باحتمالية متساوية، وتعلم النتيجة الفعلية تحتوي على جزء واحد من المعلومات.', 'arعلى النقيض من ذلك، فإن رمي العملة باستخدام عملة لها رأسان ولا ذيول لا يوجد أي إنتروبيا حيث أن العملة ستظهر دائمًا رؤوس، ويمكن توقع النتيجة بشكل مثالي.', 'arبالمقابل، فإن حدثًا ثنائيًا بنتائج قابلة للتوازن له إنتروبيا شانونlog2\\u20612=12=1}قليلا.', 'arوبالمثل،يحتوي النظام الثلاثي tritواحدبقيممتساويةlog2\\u206133}(حوالي 1.58496) بت من المعلومات لأنه يمكن أن يحتوي على واحدة من ثلاث قيم.النص الإنجليزي، الذي يعامل كسلسلة من الأحرف، لديه إنتروبيا منخفض إلى حد ما، أي يمكن التنبؤ به إلى حد ما.', 'arإذا لم نكن نعرف بالضبط ما الذي سيأتي بعد ذلك، فيمكننا أن نكون على يقين إلى حد ما، على سبيل المثال، \"e\" ستكون أكثر شيوعًا بكثير من \"z\"، وأن المجموعة \"qu\" ستكون أكثر شيوعًا من أي شيء آخر بالاشتراك مع \"q\"، وأن النسخة \"th\" ستكون أكثر شيوعًا من \"z\" أو \"q\" أو \"qu\".', 'arبعد الحروف القليلة الأولى، يمكن للمرء في كثير من الأحيان أن يخمن بقية الكلمة.', 'arيحتوي النص الإنجليزي على ما بين 0.6 و 1.3 بت من الإنتروبيا لكل حرف في الرسالة.', 'ar[9]:234إذا كان نظامالضغطبدون خسارة - حيث يمكنك دائمًا استرداد الرسالة الأصلية بالكامل عن طريق إزالة الضغط - عندئذٍ تحتوي الرسالة المضغوطة على نفس كمية المعلومات مثل الأصل، ولكن يتم توصيلها بأحرف أقل.', 'arلديها المزيد من المعلومات (إنتروبيا أعلى) لكل حرف.', 'arتحتوي الرسالة المضغوطة علىتكرارأقل.', 'arتنص نظرية شاندون لترميز المصدر على أن نظام الضغط بدون خسارة لا يمكنه ضغط الرسائل، في المتوسط، للحصول علىأكثرمن بت واحد من المعلومات لكل بت من الرسالة، ولكن يمكن تحقيق أي قيمةأقلمن بت واحد من المعلومات لكل بت من الرسالة عن طريق استخدام مخطط مناسب للترميز.', 'arإن إنتروبية الرسالة لكل بت مضروبة في طول تلك الرسالة هو مقياس لمقدار المعلومات الإجمالية التي تحتوي عليها الرسالة.إذا كان على المرء أن يرسل تسلسلات تشتمل على الأحرف الأربعة \"A\" و \"B\" و \"C\" و \"D\"، فقد تكون الرسالة المرسلة \"ABADDCAB\".', 'arتعطي نظرية المعلومات طريقة لحساب أصغر كمية ممكنة من المعلومات التي ستنقل ذلك.', 'arإذا كانت جميع الأحرف الأربعة متساوية (25٪)، فلا يمكن للمرء أن يقوم بأفضل (عبر قناة ثنائية) من أن يكون له ترميز 2 بت (عبر قناة ثنائية) كل حرف: \"A\" قد يرمز إلى \"00\"، \"B\" كـ \"01\" و \"C\" كـ \"10\" و \"D\" كـ \"11\".', 'arفي حالة حدوث \"A\" باحتمال 70٪، و \"B\" بنسبة 26٪، و \"C\" و \"D\" بنسبة 2٪ لكل منهما، يمكن للمرء تعيين رموز طول متغيرة، بحيث استقبال \"1\" يقول لنا أن ننظر في بت آخر ما لم يتم بالفعل استقبال بت 2 من المتسلسلة 1s.', 'arفي هذه الحالة، يتم ترميز \"A\" كـ \"0\" (بت واحد) و \"B\" كـ \"10\" و \"C\" و \"D\" كـ \"110\" و \"111\".', 'arمن السهل ملاحظة أن 70٪ من الوقت يحتاج إلى إرسال بت واحد فقط، و 26٪ من الوقت بتين، و 4٪ فقط من الوقت 3 بتات.', 'arفي المتوسط، يلزم أقل من 2 بت لأن الإنتروبيا أقل (بسبب الانتشار الواسع لـ \"A\" متبوعًا بـ \"B\" - معًا 96٪ من الأحرف).', 'arحساب مجموع الاحتمالية الموزنة للوغاريتم الخاص بالاحتمالات المقاسة والمأخوذة لهذا التأثير.تفترض نظرية شانون أيضًا أنه لا يوجد نظام ضغط بدون خسارة المخطط يمكن أن يقصرجميعالرسائل.', 'arإذا كانت بعض الرسائل أقصر، فيجب أن تظهر رسالة واحدة على الأقل لفترة أطول بسببمبدأ ثقب برج الحمام.', 'arفي الاستخدام العملي، هذه ليست مشكلة بشكل عام، لأن المرء عادة ما يهتم فقط بضغط أنواع معينة من الرسائل، مثل مستند باللغة الإنجليزية، على عكس نص لطلاسم أو صور رقمية بدلاً من الضجيج، وهو غير مهم إذا كانت خوارزمية الضغط تجعل بعض التسلسلات غير المتوقعة أو غير المثيرة للاهتمام أكبر.تعريف[عدل]لقطتان من الإنتروبيا: في حالة رمي قطعة نقدية عادلة ، فإن الإنتروبيا المعلوماتية في البتات هي اللوغاريتم الأساسي 2 لعدد النتائج المحتملة ؛ مع عملتين ، هناك أربع نتائج محتملة ، وقطعتين من انتروبي.', 'arبشكل عام ، تعتبر إنتروبيا المعلومات هي متوسط كمية المعلومات التي ينقلها الحدث ، عند النظر في جميع النتائج المحتملة.سميت نظرية شانونباسم نظرية بولتزمان، وقد عرّف شانون الانتربيΗ(Ηاليوناني الكبيرeta)لمتغير عشوائي متقطّعXمع القيم المحتملة,\\\\ldots ,x_\\\\right\\\\}}ووظيفة الكتلة الاحتماليةP(X) (X)}مثل:H(X)=E\\u2061[I\\u2061(X)]=E\\u2061[−log\\u2061(P(X))].', 'ar(X)=\\\\operatorname [\\\\operatorname (X)]=\\\\operatorname [-\\\\log(\\\\mathrm (X))].', 'ar}هناE }هوعامل القيمة المتوقعة،Iهو محتوى معلوماتX[10]:11[11]:19–20I(X)هو نفسه متغير عشوائي.يمكن كتابة الأنتربي بشكل صريح كـH(X)=−∑i=1nP(xi)logb\\u2061P(xi) (X)=-\\\\sum _^ (x_)\\\\log _\\\\mathrm (x_)}}حيثbهي أساساللوغاريتمالمستخدم.', 'arالقيم الشائعة لـbهي 2،رقم أويلرe، و 10، والوحدات المقابلة من الإنتروبيا هيالبتاتلـb= 2، nats لأجلb=e،bansلـb= 10.', 'ar[12]في حالةP(xi) = 0بالنسبة لبعضi، تُعتبر قيمة التلخيص المقابل (corresponding summand)؛0 logb(0)هي0، وهو ما يتوافق معالنهاية:limp→0+plog\\u2061(p)=0.}p\\\\log(p)=0.', 'ar}يمكن للمرء أيضًا تعريف الإنتروبي الشرطي لحدثينXوYأخذ القيمxi}وyj}على التواليH(X|Y)=−∑i,jp(xi,yj)log\\u2061p(xi,yj)p(yj) (X|Y)=-\\\\sum _p(x_,y_)\\\\log ,y_)})}}}حيثp(xi,yj),y_)}هو احتمال ذلكX=xi}وY=yj}.', 'arيجب فهم هذه الكمية على أنها مقدار العشوائية في المتغير العشوائيXبالنظر إلى المتغير العشوائيY.مثال[عدل]الانتروبياΗ(X)(أي المفاجأةالمتوقعة) لقلب عملة، مقاسة بالبتات، مقابل رسم بياني مقابل انحياز العملةPr(X= 1)، حيث تمثلX= 1نتيجة للرؤوس.هنا، يكون الإنتروبيا 1 بت على الأكثر، وسيتطلب توصيل نتيجة قلب العملة (قيمتان محتملتان) متوسط 1 بت على الأكثر (1 بت بالضبط لعملة عادلة).', 'arتتطلب نتيجة الموت العادل (6 قيم محتملة) في المتوسط2بت26 بت.ضع في اعتبارك رمي عملة معدنية باحتمالات ظهور رؤوس أو ذيول، وليس بالضرورة عملة عادلة؛ يمكن نمذجة هذا على أنه عملية برنولي.يتم تكبير الانتروبيا للنتيجة غير المعروفة للرمي التالي للعملة إذا كانت العملة عادلة (أي إذا كان لكل من الرؤوس والذيول احتمال متساوي 1/2).', 'arهذا هو وضع الحد الأقصى من عدم اليقين حيث يصعب التنبؤ بنتيجة الرمي التالي؛ نتيجة كل رمية لعملة واحدة توفربتكامل من المعلومات.', 'arهذا بسببH(X)=−∑i=1nP(xi)logb\\u2061P(xi)=−∑i=1212log2\\u206112=−∑i=1212⋅(−1)=1\\\\mathrm (X)&=-\\\\sum _^ (x_)\\\\log _\\\\mathrm (x_)}\\\\\\\\&=-\\\\sum _^}\\\\log _}}\\\\\\\\&=-\\\\sum _^}\\\\cdot (-1)}=1\\\\end}}ومع ذلك، إذا كنا نعلم أن العملة ليست عادلة، ولكن تأتي برؤوس أو ذيول مع الاحتمالاتpوq، حيثp≠q، فهناك قدر أقل من عدم اليقين.', 'arفي كل مرة يتم رميها، من المرجح أن يأتي أحد الجانبين أكثر من الآخر.', 'arيتم قياس عدم اليقين المنخفض في إنتروبيا أقل: في المتوسط تقدم كل رمية للعملة أقل منبتكامل من المعلومات.', 'arعلى سبيل المثال، إذا كانتp= 0.7، إذنH(X)=−plog2\\u2061(p)−qlog2\\u2061(q)=−0.7log2\\u2061(0.7)−0.3log2\\u2061(0.3)≈−0.7⋅(−0.515)−0.3⋅(−1.737)=0.8816<1\\\\mathrm (X)&=-p\\\\log _(p)-q\\\\log _(q)\\\\\\\\&=-0.7\\\\log _(0.7)-0.3\\\\log _(0.3)\\\\\\\\&\\\\approx -0.7\\\\cdot (-0.515)-0.3\\\\cdot (-1.737)\\\\\\\\&=0.8816<1\\\\end}}ينتج عن الاحتمال الموحد أقصى قدر من عدم اليقين وبالتالي أقصى انتروبيا.', 'arالإنتروبيا، إذن، يمكن أن تنخفض فقط من القيمة المرتبطة بالاحتمال المنتظم.', 'arالحالة القصوى هي عملة ذات رأسين لا تظهر ذيولًا أبدًا، أو عملة ذات ذيل مزدوج لا ينتج عنها رأس أبدًا.', 'arثم ليس هناك شك.', 'arانتروبي هو صفر: كل رمية للعملة لا تقدم أي معلومات جديدة حيث تكون نتيجة كل رمية للعملة مؤكدة دائمًا.يمكن تطبيع الانتروبيا بتقسيمها على طول المعلومات.', 'arتسمى هذه النسبة انتروبي المتري وهي مقياس لعشوائية المعلومات.الأساس المنطقي[عدل]لفهم معنى-∑pilog(pi)، قم أولاً بتعريف دالة المعلوماتIمن حيث الحدثiمع الاحتمالpi.', 'arكمية المعلومات المكتسبة نتيجة لمراقبة الحدثiالتالي من حل شانون الأساسي للخصائص من المعلومات:[13]I(p)هودالة رتيبةفيp.', 'arالزيادة في احتمالية حدث يثلل المعلومات من الحذف المراقب، والعكس بالعكس.I(p) ≥ 0: المعلومات ليست كمية سالبة.I(1) = 0: الأحداث التي تحدث دائماً لا توصل معلومات.I(p1p2) = I(p1) + I(p2): المعلومات عبر الاحداث المستقلة هي جمعية.الأخير هو خاصية حاسمة.', 'arوينص على أن الاحتمال المشترك للمصادر المستقلة للمعلومات ينقل الكثير من المعلومات مثل الحدثين الفرديين بشكل منفصل.', 'arخاصةً، إذا كان الحدث الأول يمكن أن يسفر عن واحدة منnمتوازن النتائج وآخر واحدة منmمتوازن نتائج ثم هناك نتائجmnالمحتملة لهذا الحدث المشترك.', 'arهذا يعني أنه إذا كانت هناك حاجة إلى بتاتlog2(n)لتعمية القيمة الأولىlog2(m)لترميز الثانية، يحتاج المرء إلىlog2(mn) = log2(m) + log2(n)لترميز كليهما .', \"arاكتشف شانون أن الاختيار الصحيح للدالة لقياس المعلومات، والحفاظ على هذه الإضافة، هو لوغاريتمي، أيI(p)=log\\u2061(1p)=−log\\u2061(p): (p)=\\\\log \\\\left(}\\\\right)=-\\\\log(p):}لتكنIهي دالة المعلومات التي يفترض المرء أنها قابلة للتمييز باستمرار مرتين، وعليه:I(p1p2)=I(p1)+I(p2)Starting from property 4p2I′(p1p2)=I′(p1)taking the derivative w.r.tp1I′(p1p2)+p1p2I″(p1p2)=0taking the derivative w.r.tp2I′(u)+uI″(u)=0introducingu=p1p2(u↦uI′(u))′=0&I(p_p_)&=\\\\ &I(p_)+I(p_)&&\\\\quad }\\\\\\\\&p_I'(p_p_)&=\\\\ &I'(p_)&&\\\\quad }\\\\ p_\\\\\\\\&I'(p_p_)+p_p_I''(p_p_)&=\\\\ &0&&\\\\quad }\\\\ p_\\\\\\\\&I'(u)+uI''(u)&=\\\\ &0&&\\\\quad }\\\\,u=p_p_\\\\\\\\&(u\\\\mapsto uI'(u))'&=\\\\ &0\\\\end}}هذهالمعادلة التفاضليةتؤدي إلى الحلI(u)=klog\\u2061uلأيk∈R }.\", 'arشرط 2.', 'arيؤدي إلىk<0وخاصة،kيمكن اختياره في النموذجk=−1/log\\u2061xمعx>1، وهو ما يعادل اختيارأساسمحددللوغاريتم.', 'arمختلفوحدات المعلومات(بتلوغاريتم ثنائيlog2ناتس للاللوغاريتم الطبيعيlnbansللوغاريتم العشريlog10وهلم جرا) هيمضاعفات ثابتةمن بعضها البعض.', 'arعلى سبيل المثال، في حالة رمي عملة عادلة، توفر الرؤوسlog2(2) = 1بت من المعلومات، وهو ما يقرب من 0.693 ناتس أو 0.301 أرقام عشرية.', 'arبسبب الجمع، وتوفرnnرميات من بت من المعلومات، وهو ما يقرب من0.693nناتس أو0.301nرقما0.301nعشري.إذا كان هناك توزيع حيث حدثiأن يحدث مع احتمالpiوتم أخذ عيناتNمرات مع نتائجiتحدثni=Npiمراتni=Npiفإن المبلغ الإجمالي من المعلومات التي تلقيناها من∑iniI(pi)=−∑iNpilog\\u2061pi\\\\mathrm (p_)}=-\\\\sum _\\\\log }}}.وبالتالي فإنمتوسطكمية المعلومات التي نتلقاها لكل حدث هو−∑ipilog\\u2061pi.\\\\log }}.', 'ar}نتيجة لما سبق، وبسبب استخدام اللوغاريتم في التعريف المشتق للانتروبيا، فإن الإنتروبيا مضافة أيضًا للمصادر المستقلة.', 'arعلى سبيل المثال، وانتروبي من رمية عملة عادلة هو 1 بت، وانتروبي من قذفاتmهوmبت.', 'arفي تمثيل مباشر، هناك حاجة لlog2(n)بت لتمثيل متغير الذي يمكن أن يأخذ واحدة من القيمnإذا كانnهو قوة 2، والذي هوn= 2kإذا كانت هذه القيم مُحتمَلة بنفس القدر، فإن الإنتروبيا (بالبتات) تساويlog2(n)=k.', 'arإذا كانت إحدى القيم أكثر احتمالية لحدوثها من القيم الأخرى، فإن ملاحظة أن هذه القيمة تحدث تكون أقل إفادة مما لو حدثت بعض النتائج الأقل شيوعًا.', 'arعلى العكس من ذلك، توفر الأحداث النادرة مزيدًا من المعلومات عند ملاحظتها.', 'arنظرًا لأن ملاحظة الأحداث الأقل احتمالًا تحدث بشكل أكثر ندرة، فإن التأثير الصافي هو أن الإنتروبيا (التي يُعتقد أنها متوسط المعلومات) الواردة من البيانات غير الموزعة بشكل منتظم تكون دائمًا أقل من أو تساويlog2(n).', 'arالانتروبيا هي صفر عندما تكون هناك نتيجة واحدة مؤكدة.', 'arتحدد الأنتروبيا هذه الاعتبارات عندما يُعرّف التوزيع الاحتمالي لبيانات المصدر.', 'arالمعنىمن الأحداث التي تمت ملاحظتها (معنىالرسائل) لا يهم في تعريف انتروبي.', 'arتأخذ إنتروبي بعين الاعتبار فقط احتمال ملاحظة حدث معين، لذا فإن المعلومات التي تتضمنها هي معلومات حول توزيع الاحتمال الأساسي، وليس معنى الأحداث نفسها.اعتبارات[عدل]العلاقة مع الانتروبيا الديناميكية الحرارية[عدل]جاء الإلهام لاعتماد كلمةانتروبيفي نظرية المعلومات من التشابه الوثيق بين صيغة شانون والصيغ المعروفة جدًا منالميكانيكا الإحصائية.فيالديناميكا الحرارية الإحصائية، الصيغة الأكثر شيوعًاللانتروبياالديناميكية الحراريةSلنظام الديناميكا الحراريةهيإنتروبي جيبس،S=−kB∑piln\\u2061pi}\\\\sum p_\\\\ln p_\\\\,}حيثkB″p″<sub>″i″</sub>هيثابت بولتزمان، وpiهي احتمالية الحالة في حجم المايكرو (ميكروستت).', 'arالنتروبي جيبس كان معرف من قبل ج.ويلارد جيبس في عام 1878 بعد عمل سابق من قبل بولتزمان 1872.تترجم إنتروبس جيبس بشكل غير متغير تقريبًا إلى عالمفيزياء الكملإعطاءانتروبي فون نيومان، الذي قدمهجون فون نيومانفي عام 1927،S=−kBTr(ρln\\u2061ρ)}\\\\,}(\\\\rho \\\\ln \\\\rho )\\\\,}حيث ρ هيمصفوفة كثافةالنظام الميكانيكي الكمومي و Tr هوالأثر.على المستوى العملي اليومي ، الروابط بين إنتروبيا المعلومات وإنتروبيا الديناميكا الحرارية ليست واضحة.', 'arيميل الفيزيائيون والكيميائيون إلى أن يكونوا أكثر اهتمامًابالتغيراتفي الإنتروبيا حيث يتطور النظام تلقائيًا من ظروفه الأولية ، وفقًاللقانون الثاني للديناميكا الحرارية، بدلاً من التوزيع الاحتمالي الذي لا يتغير.', 'arكما يشير الدقيق (minuteness) منثابت بولتزمانkB، التغييرات فيS/kBللكميات حتى الصغيرة من المواد في العمليات الكيميائية والفيزيائية تمثل كميات الانتروبي التي تكون كبيرة للغاية بالمقارنة مع أي شيء فيضغط البياناتأومعالجة الإشارات.', 'arفي الديناميكا الحرارية الكلاسيكية ، يتم تعريف الإنتروبيا من حيث القياسات العيانية ولا تشير إلى أي توزيع احتمالي ، وهو أمر أساسي لتعريف إنتروبيا المعلومات.تم الربط بين الديناميكا الحرارية وما يعرف الآن باسم نظرية المعلومات لأول مرة بواسطةلودفيج بولتزمانوتم التعبير عنها بواسطة معادلته الشهيرة :S=kBln\\u2061(W)}\\\\ln(W)}حيثSهو الانتروبيا الديناميكية الحرارية الميكروستت (macrostate) المعينة (معرّف بمعلمات ديناميكية حرارية مثل درجة الحرارة والحجم والطاقة وما إلى ذلك.', 'ar)،Wهو عدد الميكروستات (مجموعات مختلفة من الجسيمات في حالات الطاقة المختلفة) التي يمكن أن تنتج ماكروستات معين ، وKBهوثابت بولتزمان.', 'arيُفترض أن كل ميكروستات متساوٍ على الأرجح ، بحيث يكون احتمال أي ميكروستات معين هوpi= 1/W.عندما يتم استبدال هذه الاحتمالات في التعبير أعلاه الجيبس انتروبي (أو ما يعادلkBضرب إنتروبيا شانون)، نتائج معادلة بولتزمان.', 'arمن حيث النظرية المعلوماتية ، فإن إنتروبيا المعلومات في النظام هي كمية المعلومات «المفقودة» اللازمة لتحديد ميكروستت (microstate)، بالنظر إلى الماكروستت.في رأيجاينس(1957)، يجب النظر إلى الانتروبي الحراري الديناميكي ، كما هو موضح بواسطةالميكانيكا الإحصائية،كتطبيقلنظرية المعلومات لشانون: يتم تفسير الانتروبي الديناميكي الحراري على أنه يتناسب مع كمية معلومات شانون الإضافية اللازمة لتحديد حالة المجهر التفصيلي microscopic state للنظام، التي لا تزال غير موصوفة من خلال وصف فقط من حيث المتغيرات المجهرية للديناميكا الحرارية الكلاسيكية ، مع انتروبي ثابت التناسب ليس الاثابت بولتزمان.', 'arتؤدي إضافة الحرارة إلى النظام إلى زيادة الإنتروبيا الديناميكية الحرارية لأنه يزيد من عدد الحالات المجهرية المحتملة للنظام التي تتوافق مع القيم القابلة للقياس لمتغيراته العيانية ، مما يجعل أي وصف كامل للحالة أطول.', 'ar(انظر المادة:الديناميكا الحرارية القصوى الانتروبيا).', 'arيمكن أن يقللشيطان ماكسويل(افتراضيًا) من الانتروبيا الديناميكية الحرارية للنظام باستخدام معلومات حول حالات الجزيئات الفردية (individual molecules)؛ ولكن ، كما أظهرلانداور(من عام 1961) وزملاءه في العمل ، لكي يعمل الشيطان نفسه يجب أن يزيد من الانتروبيا الديناميكية الحرارية في هذه العملية ، على الأقل من خلال كمية معلومات شانون التي يقترح الحصول عليها وتخزينها أولاً ؛ وبالتالي فإن الانتروبيا الديناميكية الحرارية الكلية لا تقل (مما يحل التناقض).', 'arيفرضلانداورمبدأ لانداور حدًا أدنى على كمية الحرارة التي يجب أن يولدها الكمبيوتر لمعالجة كمية معينة من المعلومات ، على الرغم من أن أجهزة الكمبيوتر الحديثة أقل كفاءة بكثير.الانتروبيا كمحتوى للمعلومات[عدل]يتم تعريف الانتروبيا في سياق نموذج احتمالي.', 'arتقلبات عملة عادلة مستقلة لها إنتروبيا 1 بت لكل قلبة.', 'arالمصدر الذي يولد دائمًا سلسلة نصية طويلة من B يحتوي على إنتروبيا من 0، حيث أن الحرف التالي سيكون دائمًا \"B\".معدل الإنتروبيا لمصدر بيانات يعني متوسط عددالبتاتلكل رمز اللازمة لتعمية ه.', 'arتُظهر تجارب شانون مع المتنبئين البشريين معدل معلومات يتراوح بين 0.6 و 1.3 بت لكل حرف باللغة الإنجليزية؛[14]يمكن لخوارزمية ضغط PPM أن تحقق نسبة ضغط تبلغ 1.5 بت لكل حرف في النص الإنجليزي.من المثال السابق ، لاحظ النقاط التالية:مقدار الانتروبي ليس دائمًا عددًا صحيحًا من البتات.قد لا تنقل العديد من بتات البيانات معلومات.', 'arعلى سبيل المثال ، غالبًا ما تقوم هياكل البيانات بتخزين المعلومات بشكل متكرر ، أو تحتوي على أقسام متطابقة بغض النظر عن المعلومات الموجودة في هياكل البيانات.تعريف شانون للانتروبيا ، عند تطبيقه على مصدر المعلومات ، يمكن أن يحدد الحد الأدنى لسعة القناة المطلوبة لنقل المصدر بشكل موثوق كأرقام ثنائية مشفرة (انظر التحذير أدناه بخط مائل).', 'arيمكن اشتقاق الصيغة عن طريق حساب التوقع الرياضيلمقدار المعلوماتالواردة في رقم من مصدر المعلومات.انظر أيضًانظرية شانون هارتلي .تقيس إنتروبيا شانون المعلومات الواردة في الرسالة بدلاً من الجزء المحدد من الرسالة (أو الذي يمكن التنبؤ به).تشمل الأمثلة الأخيرة التكرار في بنية اللغة أو الخصائص الإحصائية المتعلقة بترددات حدوث أزواج الحروف أو الكلمات، والثلاثيات (triplets) وما إلى ذلك.انظرسلسلة ماركوف.الانتروبيا كمقياس للتنوع[عدل]الانتروبيا هي واحدة من عدة طرق لقياس التنوع.', 'arعلى وجه التحديد ، فإن إنتروبي شانون هو لوغاريتم1D، وهو مؤشر التنوع الحقيقي ببارامتر تساوي 1.ضغط البيانات[عدل]يحيط الانتروبي بشكل فعال بأداء أقوى ضغط بدون خسارة ممكنة ، والذي يمكن تحقيقه نظريًا باستخدام المجموعة النموذجية أو في الممارسة العملية باستخدامترميز هوفمانأوLempel – Zivأو التعمية الحسابي .', 'arانظر أيضًا تعقيد كولموجوروف .', 'arمن الناحية العملية ، تتضمن خوارزميات الضغط عمداً بعض التكرار الحكيم في شكلاختباريللحماية من الأخطاء.القدرة التكنولوجية في العالم على تخزين ونقل المعلومات[عدل]تقدر دراسة عام 2011 فيالعلومالقدرة التكنولوجية العالمية على تخزين وايصال المعلومات المضغوطة بشكل مثالي المقيسة على أكثر خوارزميات الضغط فاعلية المتاحة في عام 2007، وبالتالي تقدير إنتروبيا المصادر المتاحة تقنيًا.', 'ar[15]:60–65جميع الأعداد مضغوطة انتروبياً في وحداتأكسابايتنوع المعلومات19862007تخزين2.6295بث4321900الاتصالات0.28165يقدر المؤلفون القدرة البشرية على تخزين المعلومات (مضغوطة بالكامل انتروبياً) في عام 1986 ومرة أخرى في عام 2007.', 'arيقسمون المعلومات إلى ثلاث فئات - لتخزين المعلومات على وسيط ، لتلقي المعلومات من خلال شبكاتالبث أحاديةالاتجاه ، أو لتبادل المعلومات من خلال شبكاتالاتصالاتثنائية الاتجاه.', 'ar[15]حدود الانتروبيا كمحتوى المعلومات[عدل]هناك عدد من المفاهيم المتعلقة بالأنتروبيا التي تحدد كمية محتوى المعلومات رياضياً بطريقة ما:المعلومات الذاتيةلرسالة أو رمز فردي مأخوذ من توزيع احتمالي معين ،الانتروبيالتوزيع احتمالي معين للرسائل أو الرموز ، ومعدل الانتروبيالعملية عشوائية.', 'ar(يمكن أيضًا تحديد «معدل المعلومات الذاتية» لتسلسل معين من الرسائل أو الرموز الناتجة عن عملية عشوائية معينة: سيكون هذا دائمًا مساويًا لمعدل الإنتروبيا في حالة العملية الثابتة .)', 'arتُستخدم كميات أخرى من المعلومات أيضًا لمقارنة أو ربط مصادر مختلفة من المعلومات.من المهم عدم الخلط بين المفاهيم المذكورة أعلاه.', 'arغالبًا ما يكون واضحًا من السياق فقط ما هو المقصود.', 'arعلى سبيل المثال ، عندما يقول شخص ما أن «إنتروبيا» اللغة الإنجليزية حوالي 1 بت لكل حرف، فإنهم في الواقع يقومون بنمذجة اللغة الإنجليزية كعملية عشوائية ويتحدثون عنمعدلالإنتروبيا الخاص بها.', 'arاستخدم شانون نفسه المصطلح بهذه الطريقة.إذا تم استخدام كتل كبيرة جدًا ، فقد يصبح تقدير معدل الإنتروبيا لكل حرف منخفضًا اصطناعياً لأن التوزيع الاحتمالي للتسلسل غير معروف تمامًا ؛ إنه مجرد تقدير.', 'arإذا نظر المرء إلى نص كل كتاب تم نشره على أنه تسلسل، مع كل رمز الذي هو نص كتاب كامل ، وإذا كان هناكNكتب منشورة ، وتم نشر كل كتاب مرة واحدة فقط، فإن تقدير احتمال كل كتاب هو1/N، والانتروبيا (−log2(1/N) = log2(N)) هي−log2(1/N) = log2(N).', 'arكرمز عملي ، يتوافق هذا مع تعيينمعرّف فريدلكل كتاب واستخدامه بدلاً من نص الكتاب كلما أراد الشخص الرجوع إلى الكتاب.', 'arهذا مفيد للغاية للتحدث عن الكتب، ولكنه ليس مفيدًا جدًا لتوصيف محتوى المعلومات لكتاب فردي، أو للغة بشكل عام: لا يمكن إعادة إنشاء الكتاب من معرّفه دون معرفة توزيع الاحتمالات ، أي ، النص الكامل لجميع الكتب.', 'arالفكرة الرئيسية هي أنه يجب مراعاة تعقيد النموذج الاحتمالي.', 'arتعقيد كولموغروف Kolmogorov هو تعميم نظري لهذه الفكرة يسمح بالنظر في محتوى المعلومات في تسلسل مستقل عن أي نموذج احتمالي معين ؛ يعتبرالبرنامجالأقصر لجهازآلة تورنغيخرج التسلسل.', 'arإن الكود الذي يحقق معدل الانتروبيا لتسلسل لنموذج معين ، بالإضافة إلى دفتر الكود (أي النموذج الاحتمالي)، هو أحد هذه البرامج ، ولكنه قد لا يكون الأقصر.تسلسل فيبوناتشي هو 1، 1، 2، 3، 5، 8، 13... يعامل التسلسل كرسالة وكل رقم كرمز ، هناك عدد من الرموز تقريبًا مثل عدد الأحرف في الرسالة ، مما يعطي إنتروبيا ما يقرب منlog2(n).', 'arيحتوي أول 128 رمزًا لتسلسل فيبوناتشي على إنتروبيا تقريبًا 7 بتات / رمز ، ولكن يمكن التعبير عن التسلسل باستخدام صيغة [F(n) = F(n−1) + F(n−2)لـn= 3, 4, 5, …،F(1) =1،F(2) = 1] وهذه الصيغة لها إنتروبيا أقل بكثير وتنطبق على أي طول لتسلسل فيبوناتشي.نهايات الانتروبيا في التعمية[عدل]فيتحليل التعمية، غالبًا ما يتم استخدام الإنتروبيا تقريبًا كمقياسلعدمالقدرة على التنبؤ بمفتاح التعمية، على الرغم من أنعدم اليقينالحقيقي لا يمكن قياسه.', 'arعلى سبيل المثال ، يحتوي مفتاح 128 بت الذي يتم إنشاؤه بشكل موحد وعشوائي على 128 بت من انتروبي.', 'arيستغرق أيضًا (في المتوسط)2128−1}التخمين لكسر بالقوة الغاشمة (brute force).', 'arفشل الانتروبي في التقاط عدد التخمينات المطلوبة إذا لم يتم اختيار المفاتيح المحتملة بشكل موحد.', 'arبدلاً من ذلك ، يمكن استخدام مقياس يسمىالتخمينلقياس الجهد المطلوب لهجوم القوة الغاشمة.قد تنشأ مشاكل أخرى من التوزيعات غير المنتظمة المستخدمة في التعمية.', 'arعلى سبيل المثال ، لوحة ثنائية رقمية مكونة من 1,000,000 تستخدملوحة المرة الواحدةفقط.', 'arإذا كانت اللوحة تحتوي على 1,000,000 بت من انتروبي ، فهي مثالية.', 'arإذا كانتاللوحةتحتوي على 999,999 بت من الإنتروبيا ، موزعة بالتساوي (كل بتة فردية من اللوحة بها 0.999999 بت من الإنتروبيا) فقد يوفر أمانًا جيدًا.', 'arولكن إذا كانت اللوحة تحتوي على 999,999 بت من الإنتروبيا ، حيث يتم إصلاح البت الأول و 999999 بت المتبقية بشكل عشوائي تمامًا ، فلن يتم تعمية البت الأول من نص التعمية على الإطلاق.البيانات كعملية ماركوف[عدل]طريقة شائعة لتعريف الانتروبي لنص مبنية على نموذج ماركوف الخاص بالنص.', 'arبالنسبة لمصدر ترتيب-0 (order-0) (يتم تحديد كل حرف بشكل مستقل عن الأحرف الأخيرة)، فإن الانتروبي الثنائي هو:Hb(S)=−∑i=1npilogb\\u2061pi, _(})=-\\\\sum _^p_\\\\log _p_,}حيثpiهو احتمالi.', 'arبالنسبة لمصدر ماركوف من الترتيب-الأول (first-order)(مصدر يعتمد فيه احتمال اختيار الحرف فقط على الحرف السابق مباشرة)، فإنمعدل الإنتروبياهو:H(S)=−∑ipi∑jpi(j)∑kpi,j(k)log\\u2061pi,j(k).', 'ar(})=-\\\\sum _p_\\\\sum _p_(j)\\\\sum _p_(k)\\\\ \\\\log \\\\ p_(k).', 'ar}حيثiحالة (بعض الأحرف السابقة) وpi(j)(j)}هو احتمالjبالنظر إلى الحرفiالسابق.بالنسبة لمصدر ماركوف من الترتيب الثاني (second order)، فإن معدل الإنتروبيا هوH(S)=−∑ipi∑jpi(j)∑kpi,j(k)log\\u2061pi,j(k).', 'ar(})=-\\\\sum _p_\\\\sum _p_(j)\\\\sum _p_(k)\\\\ \\\\log \\\\ p_(k).', 'ar}انتروبيb-ary[عدل]بشكل عامالانتروبياbلمصدرS}}= (S,P)مع مصدر لأبجديةS= _(})=-\\\\sum _^p_\\\\log _p_,}ملاحظة:bفي \"انتروبيb-ary \" هو عدد الرموز المختلفةللأبجدية المثاليةالمستخدمة كمقياس قياسي لقياس الحروف الأبجدية المصدر.', 'arفي نظرية المعلومات ، هناك رمزانضروريان وكافانلأبجدية لترمز المعلومات.', 'arلذلك ، فإن الوضع الافتراضي هو اعتبارb= 2(«إنتروبيا ثنائية»).', 'arوبالتالي ، فإن إنتروبيا الأبجدية المصدر ، مع توزيع الاحتمال التجريبي المعطى لها، هو رقم يساوي عدد (ربما كسري) لرموز «الأبجدية المثالية» ، مع توزيع احتمالي مثالي ، ضروري للترميز لكل رمز من رموز الأبجدية المصدر.', 'arلاحظ أيضًا: «التوزيع الاحتمالي الأمثل» هنا يعنيتوزيعًا موحدًا: الأبجدية المصدر برموزnلها أعلى إنتروبي ممكن (لأبجدية برموزn) عندما يكون التوزيع الاحتمالي للأبجدية موحدًا.', 'arوتبين أن هذا الانتروبي الأمثل هوlogb(n).الكفاءة (الانتروبيا المقيسة)[عدل]الأبجدية المصدر ذات التوزيع غير الموحد سيكون لها إنتروبيا أقل مما لو كانت هذه الرموز ذات توزيع منتظم (أي «الأبجدية المحسنة»).', 'arيمكن التعبير عن هذا النقص في الإنتروبيا كنسبة تسمى الكفاءة :η(X)=HHmax=−∑i=1np(xi)logb\\u2061(p(xi))logb\\u2061(n)}}=-\\\\sum _^)\\\\log _(p(x_))}(n)}}}بتطبيق الخصائص الأساسية للوغاريتم ، يمكن التعبير عن هذه الكمية أيضًا على النحو التالي:η(X)=−∑i=1np(xi)logb\\u2061(p(xi))logb\\u2061(n)=∑i=1nlogb\\u2061(p(xi)−p(xi))logb\\u2061(n)=∑i=1nlogn\\u2061(p(xi)−p(xi))=logn\\u2061(∏i=1np(xi)−p(xi))^)\\\\log _(p(x_))}(n)}}=\\\\sum _^(p(x_)^)})}(n)}}=\\\\sum _^\\\\log _(p(x_)^)})=\\\\log _(\\\\prod _^p(x_)^)})}للكفاءة فائدة في تحديد الاستخدام الفعاللقناة اتصال.', 'arيشار إلى هذه الصيغة أيضًا باسم الإنتروبيا المقيسة ، حيث يتم تقسيم الإنتروبيا على أقصى إنتروبياlogb\\u2061(n)(n)}}.', 'arعلاوة على ذلك ، فإن الكفاءة غير مبالية في اختيار الاساس (الإيجابي)b، كما يتضح من عدم الحساسية في اللوغاريتم النهائي أعلاه.تمييز[عدل]تتميز إنتروبيا شانون بعدد قليل من المعايير المذكورة أدناه.', 'arأي تعريف للانتروبيا يستوفي هذه الافتراضات له الشكلDKL(p‖m)=∫log\\u2061(f(x))p(dx)=∫f(x)log\\u2061(f(x))m(dx).', 'ar}(p\\\\|m)=\\\\int \\\\log(f(x))p(dx)=\\\\int f(x)\\\\log(f(x))m(dx).', 'ar}−K∑i=1npilog\\u2061(pi)^p_\\\\log(p_)}حيثKهو ثابت مطابق لاختيار وحدات القياس.في ما يلي ،pi= Pr(X=xi)وΗn(p1, …,pn) = Η(X).استمرارية[عدل]يجب أن يكون المقياسمتواصلاً، بحيث لا يؤدي تغيير قيم الاحتمالات بمقدار صغير جدًا إلا إلى تغيير الانتروبي بمقدار صغير.تناظر[عدل]يجب ألا يتغير القياس إذا تم إعادة ترتيب النتائجxi.Hn(p1,p2,…)=Hn(p2,p1,…) _\\\\left(p_,p_,\\\\ldots \\\\right)=\\\\mathrm _\\\\left(p_,p_,\\\\ldots \\\\right)}إلخالحد الأقصى[عدل]يجب أن يكون المقياس بأقصى حد إذا كانت جميع النتائج محتملة على قدم المساواة (يكون عدم اليقين أعلى عندما تكون جميع الأحداث المحتملة قابلة للتوازن).Hn(p1,…,pn)≤Hn(1n,…,1n)=logb\\u2061(n).', 'ar_(p_,\\\\ldots ,p_)\\\\leq \\\\mathrm _\\\\left(},\\\\ldots ,}\\\\right)=\\\\log _(n).', 'ar}بالنسبة للأحداث المتوازنة ، يجب أن يزيد الإنتروبيا مع عدد النتائج.Hn(1n,…,1n⏟n)=logb\\u2061(n)<logb\\u2061(n+1)=Hn+1(1n+1,…,1n+1⏟n+1).', 'ar_\\\\underbrace },\\\\ldots ,}} _=\\\\log _(n)<\\\\log _(n+1)=\\\\mathrm _\\\\underbrace },\\\\ldots ,}} _.}Hn(1n,…,1n⏟n)=logb\\u2061(n)<logb\\u2061(n+1)=Hn+1(1n+1,…,1n+1⏟n+1).', 'ar_\\\\underbrace },\\\\ldots ,}} _=\\\\log _(n)<\\\\log _(n+1)=\\\\mathrm _\\\\underbrace },\\\\ldots ,}} _.', 'ar}للمتغيرات العشوائية المستمرة ، فإن الغاوسي (Gaussian) متعدد المتغيرات هو التوزيع مع أقصى إنتروبيا تفاضلية .الإضافة[عدل]يجب أن تكون كمية الإنتروبيا مستقلة عن كيفية اعتبار العملية مقسمة إلى أجزاء.تميز هذه العلاقة الوظيفية الأخيرة إنتروبيا النظام مع الأنظمة الفرعية.', 'arتتطلب أن يتم حساب إنتروبيا النظام من إنتروبيا أنظمته الفرعية إذا كانت التفاعلات بين الأنظمة الفرعية معروفة.لتكن فرقة من عناصرnموزعة بشكل متجانس التي تُقسم إلى مربعاتk(النظم الفرعية) معb1, ...,bkلكل عناصرb1, ...,bk، يجب أن يكون الانتروبي من الفرقة بأكملها مساويا لمجموع الانتروبي من نظام الصناديق والانتروبيا الفردية للمربعات ، كل منها مرجح باحتمال وجوده في هذا الصندوق بالذات.حيث للأعداد الصحيحة الموجبةbiحيثb1+ … +bk=nHn(1n,…,1n)=Hk(b1n,…,bkn)+∑i=1kbinHbi(1bi,…,1bi).', 'ar_\\\\left(},\\\\ldots ,}\\\\right)=\\\\mathrm _\\\\left(}},\\\\ldots ,}}\\\\right)+\\\\sum _^}}\\\\,\\\\mathrm _}\\\\left(}},\\\\ldots ,}}\\\\right).', 'ar}اختيارk=n،b1= … =bn= 1يعني أن إنتروبيا نتيجة معينة صفر:Η1(1) = 0.', 'arهذا يعني أن كفاءة أبجدية المصدر برموزnيمكن تعريفها ببساطة على أنها تساوي إنتروبياn-ary.', 'arانظر أيضًاالفائض (نظرية المعلومات).خصائص أخرى[عدل]تفي إنتروبيا شانون بالخصائص التالية ، بالنسبة لبعضها من المفيد تفسير الإنتروبيا على أنها كمية المعلومات المستفاد منها (أوالتخلص من الريبة) من خلال الكشف عن قيمة المتغير العشوائيX:لا تساهم إضافة أو إزالة حدث باحتمال الصفر في الإنتروبيا:H(X,Y)=H(X|Y)+H(Y)=H(Y|X)+H(X).', 'ar(X,Y)=\\\\mathrm (X|Y)+\\\\mathrm (Y)=\\\\mathrm (Y|X)+\\\\mathrm (X).', 'ar}انتروبيا متغير عشوائي متقطع هو رقم غير سالب:H(X)≥0 (X)\\\\geq 0}.', 'ar[16]:15يمكن تأكيده باستخداممتراجحة ينسنكالتاليH(X)=E\\u2061[logb\\u2061(1p(X))]≤logb\\u2061(E\\u2061[1p(X)])=logb\\u2061(n) (X)=\\\\operatorname \\\\left[\\\\log _\\\\left(}\\\\right)\\\\right]\\\\leq \\\\log _\\\\left(\\\\operatorname \\\\left[}\\\\right]\\\\right)=\\\\log _(n)}.', 'ar[16]:29الانتروبيا الأقصى للوغاريتمlogb(n)يتم تحقيقه بشكل فعال من خلال الأبجدية المصدر ذي التوزيع الاحتمالي الموحد: الريبة هي الحد الأقصى عندما تكون جميع الأحداث المحتملة قابلة للتوازن.الانتروبيا أو كمية المعلومات التي كشف عنها عن طريق تقييم(X,Y)(أي تقييمXوYتزامنياً) تساوي المعلومات التي تم الكشف عنها من خلال إجراء تجربتين متتاليتين: أولاً تقييم قيمةY، ثم الكشف عن قيمةXبالنظر إلى أنك تعرف قيمةY.', 'arيمكن كتابة هذا كـH(X,Y)=H(X|Y)+H(Y)=H(Y|X)+H(X).', 'ar(X,Y)=\\\\mathrm (X|Y)+\\\\mathrm (Y)=\\\\mathrm (Y|X)+\\\\mathrm (X).', 'ar}وبالتالي ، يمكن أن ينخفض انتروبيا المتغير فقط عندما يتم تمرير الأخير من خلال دالة.إذاY=f(X)أينfهي وظيفة ، إذنH(f(X)|X)=0.', 'arتطبيق الصيغة السابقة علىH(X,f(X))عائداتH(X)+H(f(X)|X)=H(f(X))+H(X|f(X)), (X)+\\\\mathrm (f(X)|X)=\\\\mathrm (f(X))+\\\\mathrm (X|f(X)),}H(X)+H(f(X)|X)=H(f(X))+H(X|f(X)), (X)+\\\\mathrm (f(X)|X)=\\\\mathrm (f(X))+\\\\mathrm (X|f(X)),}H(f(X))≤H(X)وبالتالي ، يمكن أن ينخفض انتروبيا المتغير فقط عندما يتم تمرير الأخير من خلال دالة.إذا كانXوYمتغيرين عشوائيين مستقلين ، فإن معرفة قيمةYلا تؤثر على معرفتنا بقيمة (نظرًا لأن الاثنين لا يؤثران على بعضهما البعض بالاستقلالية):H(X|Y)=H(X).', 'ar(X|Y)=\\\\mathrm (X).}H(X|Y)=H(X).', 'ar(X|Y)=\\\\mathrm (X).', 'ar}لا يعد إنتروبيا حدثين متزامنين أكثر من مجموع إنتروبيا كل حدث فردي ، وهي متساوية إذا كان الحدثان مستقلين.', 'arوبشكل أكثر تحديدًا ، إذا كانXوYمتغيرين عشوائيين في نفس مساحة الاحتمال ، و(X,Y)يشيران إلى منتجهما الديكارتي ،H(X,Y)≤H(X)+H(Y).', 'ar(X,Y)\\\\leq \\\\mathrm (X)+\\\\mathrm (Y).', 'ar}الانتروبيH(p) (p)}مقعرفي دالة الكتلة الاحتماليةp، بمعنى آخرH(λp1+(1−λ)p2)≥λH(p1)+(1−λ)H(p2) (\\\\lambda p_+(1-\\\\lambda )p_)\\\\geq \\\\lambda \\\\mathrm (p_)+(1-\\\\lambda )\\\\mathrm (p_)}لجميع دوال (وظائف) الكتلة الاحتماليةp1,p2,p_}و0≤λ≤1.', 'ar[16]:32وفقًا لذلك ، تكون وظيفة الإنتروبيا السالبة (سلبي) محدبة ، ويكون اقترانها المحدب هو تعبير جمع لوغرايتم LogSumExp .توسيع الإنتروبيا المتقطعة إلى الحالة المستمرة[عدل]الانتروبيا التفاضلية[عدل]يقتصر انتروبيا شانون على متغيرات عشوائية تأخذ قيمًا متقطعة.', 'arالصيغة المقابلة لمتغير عشوائي مستمر معدالة كثافة الاحتمالf(x)مع دعم نهائي أو لانهائيX }على الخط الحقيقي (real line) يتم تعريفه عن طريق القياس ، باستخدام الشكل أعلاه من الانتروبي كما هو متوقع:h[f]=E\\u2061[−ln\\u2061(f(x))]=−∫Xf(x)ln\\u2061(f(x))dx.', 'ar[-\\\\ln(f(x))]=-\\\\int _ }f(x)\\\\ln(f(x))\\\\,dx.', 'ar}يشار إلى هذه الصيغة عادةً باسمالإنتروبيا المستمرةأو الانتروبي التفاضلي .', 'arو السوابق (precursor) منالانتروبي مستمرةh[f]هو التعبير عن الدالة (وظيفية)Ηفينظرية-Hلبولتزمان.على الرغم من أن التشابه بين كلتا الدالتين (الوظيفتين) إيحائي (suggestive)، إلا أنه يجب تحديد السؤال التالي: هل الإنتروبيا التفاضلية امتداد صحيح للانتروبيا المتقطعة لشانون؟ تفتقر الإنتروبيا التفاضلية إلى عدد من الخصائص التي تمتلكها إنتروبون شانون المتقطعة - يمكن أن تكون سلبية وقد اقترحت تصحيحات، لا سيما الحد من كثافة النقاط المنفصلة .للإجابة على هذا السؤال ،التوصيل بين الدالتين (الوظيفتين) :من أجل الحصول على مقياس نهائي (محدود) بشكل عام حيث يصلحجم الحاوية(bin size) إلى الصفر.', 'arفي الحالة المتقطعة، يكون حجم الحاوية هو العرض (الضمني) لكل من صناديقn(النهائية أو اللانهائية) التي يُشار إلى احتمالاتها بواسطةpn.', 'arنظرًا لتعميم المجال المستمر ، يجب أن يكون العرض صريحًا.للقيام بذلك ، ابدأ بدالة مستمرةfمقسمة إلى حاويات الحجمΔ.', 'arمن خلال نظرية القيمة المتوسطة ، توجد قيمةxiفي كل صندوق بحيثf(xi)Δ=∫iΔ(i+1)Δf(x)dx)\\\\Delta =\\\\int _^f(x)\\\\,dx}تكامل الدالةfيمكن تقريبه (بالمعنى الريماني) بواسطة∫−∞∞f(x)dx=limΔ→0∑i=−∞∞f(xi)Δ^f(x)\\\\,dx=\\\\lim _\\\\sum _^f(x_)\\\\Delta }حيث النهاية هذه و «حجم الحاوية إلى الصفر» متساويين.و سنستدل بـHΔ:=−∑i=−∞∞f(xi)Δlog\\u2061(f(xi)Δ) ^:=-\\\\sum _^f(x_)\\\\Delta \\\\log \\\\left(f(x_)\\\\Delta \\\\right)}وتوسيع اللوغاريتم ، لديناHΔ=−∑i=−∞∞f(xi)Δlog\\u2061(f(xi))−∑i=−∞∞f(xi)Δlog\\u2061(Δ).', 'ar^=-\\\\sum _^f(x_)\\\\Delta \\\\log(f(x_))-\\\\sum _^f(x_)\\\\Delta \\\\log(\\\\Delta ).', 'ar}كما لدينا Δ → 0 ،∑i=−∞∞f(xi)Δ→∫−∞∞f(x)dx=1∑i=−∞∞f(xi)Δlog\\u2061(f(xi))→∫−∞∞f(x)log\\u2061f(x)dx.\\\\sum _^f(x_)\\\\Delta &\\\\to \\\\int _^f(x)\\\\,dx=1\\\\\\\\\\\\sum _^f(x_)\\\\Delta \\\\log(f(x_))&\\\\to \\\\int _^f(x)\\\\log f(x)\\\\,dx.\\\\end}}ملحوظة؛log(Δ) → −∞كـΔ → 0، يتطلب تعريفًا خاصًا للانتروبيا التفاضلية أو المستمرة:h[f]=limΔ→0(HΔ+log\\u2061Δ)=−∫−∞∞f(x)log\\u2061f(x)dx,\\\\left(\\\\mathrm ^+\\\\log \\\\Delta \\\\right)=-\\\\int _^f(x)\\\\log f(x)\\\\,dx,}والتي ، كما قيل من قبل ، يشار إليها باسمالانتروبي التفاضلي.', 'arهذا يعني أن الإنتروبيا التفاضليةليستنهاية (حد) لإنتروبيا شانون لـn→ ∞.', 'arبدلاً من ذلك ، فإنه يختلف عن نهاية إنتروبيا شانون من خلال إزاحة لانهائية (an infinite offset) (انظر أيضًا المقالة حول بُعد المعلومات )الحد من كثافة النقاط المنفصلة[عدل]أتضح كنتيجة أنه، على عكس إنتروبيا شانون ، فإن الانتروبيا التفاضليةليستبشكل عام مقياسًا جيدًا للريبة أو المعلومات.', 'arعلى سبيل المثال ، يمكن أن تكون الإنتروبيا التفاضلية سلبية ؛ كما أنها ليست ثابتة تحت التحولات المنسقة المستمرة ( continuous co-ordinate transformations).', 'arيمكن توضيح هذه المشكلة عن طريق تغيير الوحدات عندما تكونxمتغيرة ذات أبعاد.وثمf(x)تأخذ الواحدات1/x.', 'arيجب أن تكون قيمة اللوغاريتم بلا أبعاد ، وإلا فهي غير مناسبة ، بحيث تكون الإنتروبيا التفاضلية كما هو موضح أعلاه غير مناسبة.', 'arإذا كانتΔهي قيمة «معيارية» لـx(أي «حجم الحاوية») وبالتالي لديها نفس الوحدات ، عندئذ يمكن كتابة الانتروبي التفاضلي المعدل (modified differential entropy) في الشكل الصحيح على النحو التالي:H=∫−∞∞f(x)log\\u2061(f(x)Δ)dx^f(x)\\\\log(f(x)\\\\,\\\\Delta )\\\\,dx}وستكون النتيجة هي نفسها لأي اختيار للوحدات لـx.', 'arفي الواقع ، حد الانتروبيا المتقطعة كماN→∞سيشمل أيضا مصطلحlog\\u2061(N)، والتي ستكون بشكل عام لا نهائية.', 'arهذا أمر متوقع ، المتغيرات المستمرة عادة ما يكون لها إنتروبيا لانهائية عندما يتم تقسيمها.', 'arالكثافة المحدودة للنقاط المتقطعة هي في الواقع مقياس لمدى سهولة وصف التوزيع من التوزيع الموحد على مخطط القياس.الانتروبيا النسبية[عدل]مقياس آخر مفيد للإنتروبيا يعمل بشكل جيد على قدم المساواة في الحالة المتقطعة والمستمرة هوالانتروبيا النسبيةللتوزيع.', 'arيتم تعريفه على أنهانحراف كلباك-لايبا Kullback-Leiblerمن التوزيع إلى مقياس مرجعيmكالتالي.', 'arنفترض أنpتوزيع الاحتمال المستمر على الإطلاق (absolutely continuous) فيما يتعلق مقياسmأي هو من شكلp(dx) =f(x)m(dx)لبعض الدوال غير السلبية القابلة للتكاملmمرة (m-integrable) دالةfمع القابلة للتكاملmمرة (m-integral) 1، ثم يمكن تعريف الإنتروبيا النسبية على أنهاPi(A)=.', 'ar(A)=\\\\,\\\\ldots ,x_,x_,\\\\ldots ,x_):(x_,\\\\ldots ,x_)\\\\in A\\\\}.', 'ar}في هذا الشكل ، يعمم الإنتروبيا النسبية (حتى التغيير في الإشارة) كلاً من الإنتروبيا المتقطعة، حيث يكون المقياسmهو مقياس العد ، والانتروبيا التفاضلية ، حيث المقياسmهومقياس ليبيج Lebesgue.', 'arإذا كان المقياسmهو نفسه توزيع احتمالي ، فإن الإنتروبيا النسبية غير سالبة ، وصفر إذا كانp=mكمقاييس.', 'arيتم تعريفه لأي مساحة قياس، وبالتالي تنسيق مستقل وثابت تحت إعادة تنسيق المعلمات (reparameterizations) إذا أخذ المرء في الاعتبار بشكل صحيح تحويل المقياسm.', 'arالإنتروبيا النسبية ، والانتروبيا الضمنية والانتروبيا التفاضلية ، تعتمد على مقياس «المرجع»m.استخدم في التوافيق[عدل]أصبحت الانتروبيا كمية مفيدة فيالتوافيق.متراجحة- لوميس ويتني[عدل]مثال بسيط على ذلك هو دليل بديل على متراجحة لوميس ويتني : لكل مجموعة فرعيةA⊆Zd، لدينا|A|d−1≤∏i=1d|Pi(A)|\\\\leq \\\\prod _^|P_(A)|}حيثPiهوالإسقاط المتعامدفي الإحداثيةi:H[(X1,…,Xd)]≤1r∑i=1nH[(Xj)j∈Si] [(X_,\\\\ldots ,X_)]\\\\leq }\\\\sum _^\\\\mathrm [(X_)_}]}يتبع الدليل كنتيجة طبيعية بسيطة من عدم مساواة شيرر : إذاX1, …,Xdمتغيرات عشوائيةS1, …,Snهي مجموعات فرعية من)_}}هو المنتج الديكارتي للمتغيرات العشوائيةXjمع الفهارسjفيSi(لذا فإن أبعاد هذا المتجه يساوي حجمSi).نحن نرسم كيف يتبع لوميس ويتني من هذا: في الواقع ، لتكنXيكون متغيرًا عشوائيًا موزعًا بشكل موحد مع قيم فيAبحيث تحدث كل نقطة فيAباحتمال متساوٍ.', 'arثم (من خلال خصائص الانتروبيا المذكورة أعلاه)Η(X) = log|A|، حيث|A|يدل على طويلةA(عدد العناصر في المجموعةA).', 'arلتكنSi= .', 'arمجال لـ(Xj)j∈Si)_}}محتوى فيPi(A)وبالتاليH[(Xj)j∈Si]≤log\\u2061|Pi(A)| [(X_)_}]\\\\leq \\\\log |P_(A)|}.', \"arاستخدم هذا الآن لتقييد الجانب الأيمن من متراجحة شيرر ( Shearer's inequality) و أرفع للأس الطرف المقابل للنتيجة التي تحصل عليها.التقريب لمعامل ذي الحدين (binomial coefficient)[عدل]للأعداد الصحيحة0 <k<nلتكنq=k/n.أي2nH(q)n+1≤(nk)≤2nH(q), (q)}}}\\\\leq }\\\\leq 2^ (q)},}حيثH(q)=−qlog2\\u2061(q)−(1−q)log2\\u2061(1−q).\", 'ar(q)=-q\\\\log _(q)-(1-q)\\\\log _(1-q).', 'ar}[17]:43هنا برهان تخطيطي.', 'arلاحظ أن(nk)qqn(1−q)n−nq}q^(1-q)^}هو مصطلح واحد للتعبير∑i=0n(ni)qi(1−q)n−i=(q+(1−q))n=1.^}q^(1-q)^=(q+(1-q))^=1.', 'ar}إعادة الترتيب يعطي الحد الأعلى (upper bound).', 'arبالنسبة إلى الحد السفلي ، يُظهر أولاً ، باستخدام بعض الجبر ، الذي هو أكبر مصطلح في الجمع.', 'arولكن بعد ذلك ،(nk)qqn(1−q)n−nq≥1n+1}q^(1-q)^\\\\geq }}نظرًا لوجود مصطلحاتn+ 1في التجميع.', 'arإعادة الترتيب يعطي الحد الأدنى.التفسير الدقيق لهذا هو أن عدد السلاسل الثنائية للطولnمعkالعديد من 1 هو تقريبًا2nH(k/n) (k/n)}}.', 'ar[18]انظر أيضًا[عدل]إنتروبيا شرطيةإنتروبيا وسهم الزمنترميز إنتروبي– a coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols.تاريخ الإنتروبياهندسة معلوميةإنتروبي مشترك– is the measure how much entropy is contained in a joint system of two random variables.Kolmogorov–Sinai entropyinنظام تحريكيsمعلومات متبادلةالإرباكQualitative variation– other measures ofتشتتforمستويات القياسعشوائيةالمراجع[عدل]^أبمذكور في:IEC 80000-13:2008 Quantities and units — Part 13: Information science and technology.', 'arقسم أو آية أو فقرة أو بند: 13-25.', 'arالناشر: المنظمة الدولية للمعايير.', 'arلغة العمل أو لغة الاسم: الإنجليزية.', 'arتاريخ النشر: 1 مارس 2008.^مذكور في:مفردة كهروتقنية دولية.', 'arرقم مفردة لدى تقنية كهربائية دولية (IEV):171-07-15.', 'arالناشر: اللجنة الكهروتقنية الدولية.^مذكور في:IEC 80000-13:2008 Quantities and units — Part 13: Information science and technology.', 'arقسم أو آية أو فقرة أو بند: 13-25.a.', 'arالناشر: المنظمة الدولية للمعايير.', 'arلغة العمل أو لغة الاسم: الإنجليزية.', 'arتاريخ النشر: 1 مارس 2008.^مذكور في:IEC 80000-13:2008 Quantities and units — Part 13: Information science and technology.', 'arقسم أو آية أو فقرة أو بند: 13-25.b.', 'arالناشر: المنظمة الدولية للمعايير.', 'arلغة العمل أو لغة الاسم: الإنجليزية.', 'arتاريخ النشر: 1 مارس 2008.^مذكور في:IEC 80000-13:2008 Quantities and units — Part 13: Information science and technology.', 'arقسم أو آية أو فقرة أو بند: 13-25.c.', 'arالناشر: المنظمة الدولية للمعايير.', 'arلغة العمل أو لغة الاسم: الإنجليزية.', 'arتاريخ النشر: 1 مارس 2008.^أبShannon، Claude E.(يوليو 1948).', 'arA Mathematical Theory of Communication.Bell System Technical Journal.', 'arج.', 'ar27 ع.', 'ar3: 379–423.DOI:10.1002/j.1538-7305.1948.tb01338.x.^أبShannon، Claude E.(أكتوبر 1948).', 'arA Mathematical Theory of Communication.Bell System Technical Journal.', 'arج.', 'ar27 ع.', 'ar4: 623–656.DOI:10.1002/j.1538-7305.1948.tb00917.x.', 'ar(PDF, archived fromhere)^Pathria، R. K.؛ Beale، Paul (2011).Statistical Mechanics (Third Edition).', 'arAcademic Press.', 'arص.', 'ar51.ISBN:978-0123821881.', 'arمؤرشف منالأصلفي 2020-06-17.^Schneier, B:Applied Cryptography, Second edition, John Wiley and Sons.^Borda, Monica (2011).Fundamentals in Information Theory and Coding.', 'arSpringer.ISBN:978-3-642-20346-6.', 'arمؤرشف منالأصلفي 2017-02-15.^Han, Te Sun & Kobayashi, Kingo (2002).Mathematics of Information and Coding.', 'arAmerican Mathematical Society.ISBN:978-0-8218-4256-0.', 'arمؤرشف منالأصلفي 2017-02-15.', 'ar}: صيانة الاستشهاد: أسماء متعددة: قائمة المؤلفين (link)^Schneider, T.D,Information theory primer with an appendix on logarithms, National Cancer Institute, 14 April 2007.نسخة محفوظة14 يونيو 2020 على موقعواي باك مشين.^Carter، Tom (مارس 2014).An introduction to information theory and entropy(PDF).', 'arSanta Fe.', 'arمؤرشف منالأصل(PDF)في 2016-06-04.', 'arاطلع عليه بتاريخ2017-08-04.', 'ar}: صيانة الاستشهاد: مكان بدون ناشر (link)^Mark Nelson (24 أغسطس 2006).', 'arThe Hutter Prize.', 'arمؤرشف منالأصلفي 2018-03-01.', 'arاطلع عليه بتاريخ2008-11-27.^أب\"The World\\'s Technological Capacity to Store, Communicate, and Compute Information\", Martin Hilbert and Priscila López (2011),ساينس, 332(6025); free access to the article through here: martinhilbert.net/WorldInfoCapacity.htmlنسخة محفوظة7 يناير 2016 على موقعواي باك مشين.^أبتThomas M. Cover؛ Joy A. Thomas (18 يوليو 2006).Elements of Information Theory.', \"arHoboken, New Jersey: Wiley.ISBN:978-0-471-24195-9.^Aoki, New Approaches to Macroeconomic Modeling.^Probability and Computing, M. Mitzenmacher and E. Upfal, Cambridge University PressThis article incorporates material from Shannon's entropy on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.قراءة متعمقة[عدل]كتب عن نظرية المعلومات[عدل]Arndt، C. (2004) ،مقاييس المعلومات: المعلومات ووصفها في العلوم والهندسة، Springer ، (ردمك978-3-540-40855-0)Cover ، TM، Thomas ، JA (2006) ،عناصر نظرية المعلومات، الطبعة الثانية.\", 'arوايلي-إنترسينس.', 'ar(ردمك0-471-24195-4)رقم ISBN0-471-24195-4.Gray ، RM (2011) ،نظرية الانتروبيا والمعلومات، Springer.ماكاي ، ديفيد جي سي.نظرية المعلومات والاستدلال وخوارزميات التعلمكامبريدج:مطبعة جامعة كامبريدج، 2003.', 'ar(ردمك0-521-64298-1)رقم ISBN0-521-64298-1Martin, Nathaniel F.G. & England, James W. (2011).Mathematical Theory of Entropy.', 'arCambridge University Press.ISBN:978-0-521-17738-2.', 'arمؤرشف منالأصلفي 2020-08-03.', 'ar}: صيانة الاستشهاد: أسماء متعددة: قائمة المؤلفين (link)Martin, Nathaniel F.G. & England, James W. (2011).Mathematical Theory of Entropy.', 'arCambridge University Press.ISBN:978-0-521-17738-2.', 'arمؤرشف منالأصلفي 2020-08-03.', 'ar}: صيانة الاستشهاد: أسماء متعددة: قائمة المؤلفين (link)Martin, Nathaniel F.G. & England, James W. (2011).Mathematical Theory of Entropy.', 'arCambridge University Press.ISBN:978-0-521-17738-2.', 'arمؤرشف منالأصلفي 2020-08-03.', 'ar}: صيانة الاستشهاد: أسماء متعددة: قائمة المؤلفين (link)شانون ، CE،Weaver ، W.(1949)النظرية الرياضية للاتصال، مطبعة جامعة إلينوي.', 'ar(ردمك0-252-72548-4)رقم ISBN0-252-72548-4Stone ، JV (2014) ، الفصل الأول مننظرية المعلومات: مقدمة تعليمية، جامعة شيفيلد ، إنجلترا.', 'ar(ردمك978-0956372857)رقم ISBN978-0956372857.روابط خارجية[عدل]Hazewinkel, Michiel, ed.', 'ar(2001),\"Entropy\",Encyclopedia of Mathematics[لغات أخرى]\\u200f(بالإنجليزية),Springer,ISBN:978-1-55608-010-4Hazewinkel, Michiel, ed.', 'ar(2001),\"Entropy\",Encyclopedia of Mathematics[لغات أخرى]\\u200f(بالإنجليزية),Springer,ISBN:978-1-55608-010-4مقدمة في الإنتروبيا ومعلوماتعن Principia Cybernetica Webالانتروبيامجلة متعددة التخصصات حول جميع جوانب مفهوم انتروبي.', 'arالوصول المفتوح.وصف انتروبي من المعلومات من «أدوات للفكر» هوارد راينجولدبريمج جافا يمثل تجربة شانون لحساب الانتروبيا من اللغة الإنجليزيةشرائح على كسب المعلومات و إنتروبيادليلبديهيلمفهوم الانتروبيا الناشئة في قطاعات مختلفة من العلوم- كتاب ويكي على تفسير مفهوم انتروبي.اكتشاف أحداث الشبكة باستخدام تدابير الانتروبيا، د.', 'arرايموند إيمان ، جامعة أوكلاند ، PDF ؛ 5993 كيلو بايت - أطروحة دكتوراه توضح كيفية استخدام تدابير الانتروبيا في الكشف عن الشذوذ في الشبكة.', 'arEntropyفي Rosetta Code - مراجعة تطبيقات إنتروبي شانون بلغات برمجة مختلفة.«نظرية المعلومات للأشخاص الأذكياء».', 'arمقدمة قصيرة لبديهيات نظرية المعلومات ، الإنتروبيا ، المعلومات المتبادلة ، اختلاف كولباك - ليبلر ، ومسافة جنسن - شانون.أداة على الإنترنت لحساب الإنتروبيا (نص عادي)أداة على الإنترنت لحساب الإنتروبيا (ثنائي)ضبط استناديدوليةالمُعرِّف مُتعدِّد الأوجه (FAST)وطنيةالمكتبة القومية الإسبانية (BNE)المكتبة الوطنية الفرنسية (BnF)الملف الاستنادي المتكامِل (GND)المكتبة القومية الإسرائيلية (J9U)مكتبة الكونغرس (LCNAF)قاعدة البيانات الوطنية التشيكية (NLCR AUT)اعتلاجفيالمشاريع الشقيقة:صور وملفات صوتيةمن كومنز.عنتطرائق ضغط البياناتتراميزعديمةالخسارةبالاعتلاجواحدي[الإنجليزية]حسابي[الإنجليزية]غلومب[الإنجليزية]شانون[الإنجليزية]وفانووفانو وإلياس[الإنجليزية]هوفمانالمُتكيِّفالمعياري[الإنجليزية]المُعدَّل[الإنجليزية]النطاق[الإنجليزية]تانستال[الإنجليزية]الرموز العامة[الإنجليزية]غلومب الأُسِّي[الإنجليزية]فيبوناتشي[الإنجليزية]إلياس غاما[الإنجليزية]ليفينشتاينبالاستبدال[الإنجليزية]زوج البايتاتلامبل وزيف[الإنجليزية]842[الإنجليزية]خوارزمية بروتلي[الإنجليزية]خوارزمية الانكماش[الإنجليزية]الرابعة[الإنجليزية](LZ4)اعتلاج محدود الحالات[الإنجليزية](LZFSE)وجيف بونويك[الإنجليزية](LZJB)وفق سلسلة ماركوف[الإنجليزية](LZMA)وأوبيرهامر[الإنجليزية](LZO)وروز ويليامز[الإنجليزية](LZRW)وستاك[الإنجليزية](LZS)وستورير وتشيمانسكي[الإنجليزية](LZSS)وويلش(LZW)وويلش المقطعيَّة[الإنجليزية](LZWL)المُوسَّعة[الإنجليزية](LZX)خوارزمية سنابي[الإنجليزية]المعيار زد[الإنجليزية](Zstd)أنواع أخرىتحويل بوروس وويلر[الإنجليزية](BWT)وزن شجرة المحتوى[الإنجليزية](CTW)ترميز دلتا[الإنجليزية]ضغط ماركوف المتغير[الإنجليزية](DMC)تحويل النقل للمقدمة[الإنجليزية](MTF)التوقع بالتطابق الجزئي[الإنجليزية](PPM)وفق طول التتابع(RLE)تراميزذاتخسارةبالتحويل[الإنجليزية]مُتقطِّع[الإنجليزية]الجيب وجيب التمامجيب التمام المُتقطِّع(DCT)جيب التمام المُتقطِّع المُعدَّل[الإنجليزية](MDCT)الجيب المُتقطِّع[الإنجليزية](DST)تحويل فورييه السريع(FFT)المُويجة[الإنجليزية]المُتقطِّع[الإنجليزية](DWT)مويجات دوبشيه[الإنجليزية]تجزئة المجموعة في الأشجار الهرمية[الإنجليزية](SPIHT)بالتوقُّعالتضمين النبضي المُرمَّز التفاضلي(DPCM)المُتكيِّف[الإنجليزية](ADPCM)ترميز خطي بالتوقع(LPC)ملتف[الإنجليزية](WLPC)مُثار بالرمز[الإنجليزية](CELP)جبري مُثار بالرمز[الإنجليزية](ACELP)نسبة لوغاريتم المنطقة[الإنجليزية](LAR)أزواج الخط الطيفية[الإنجليزية](LSP)الحركةالتعويضتقدير الاتجاهالشعاع[الإنجليزية]نموذج علم الصوتيات النفسي[الإنجليزية]ضغطالصوتالمفاهيممعدل نقل البتاتوسطي[الإنجليزية](ABR)ثابت[الإنجليزية](CBR)مُتغيِّر[الإنجليزية](VBR)الضغط والتمديد[الإنجليزية]الطينسبة تغير المجال[الإنجليزية]التأخير[الإنجليزية]الجودةالاستعيانمبرهنة نايكويست وشانونترميز الكلامترميز أجزاء النطاق[الإنجليزية]أجزاءمُرمِّزالصوتخوارزميات القوانينA[الإنجليزية]μ[الإنجليزية]المُثار بالرمز[الإنجليزية](CELP)الجبري المُثار بالرمز[الإنجليزية](ACELP)التضمين النبضي المُرمَّز التفاضلي(DPCM)المُتكيِّف[الإنجليزية](ADPCM)تحويل فورييهترميز خطي بالتوقع(LPC)ملتف[الإنجليزية](WLPC)نسبة لوغاريتم المنطقة[الإنجليزية](LAR)أزواج الخط الطيفية[الإنجليزية](LSP)تحويل جيب التمام المعدل[الإنجليزية](MDCT)نموذج علم الصوتيات النفسي[الإنجليزية]ضغطالصورةالمفاهيماستعيان جزئي لوني[الإنجليزية]وحدة شجرة الترميز[الإنجليزية]فضاء لونيأثر الضغط[الإنجليزية]الدِّقةوحدة تجميعية كبرى[الإنجليزية]بكسلPSNRتكميمصورة الاختبار المعياري[الإنجليزية]الطُّرقترميز السلسلة[الإنجليزية]تحويل جيب التمام المتقطع(DCT)تحويل الأشجار الصفرية المُضمَّنة للمويجات[الإنجليزية](EZW)ضغط كُسيريمبرهنة كارهونين ولويف[الإنجليزية](KLT)تمثيل هرميترميز وفق طول التتابع(RLE)تجزئة المجموعة في الأشجار الهرمية[الإنجليزية](SPIHT)ضغط المويجات[الإنجليزية]ضغطالفيديوالمفاهيممعدل نقل البتاتوسطي[الإنجليزية](ABR)ثابت[الإنجليزية](CBR)مُتغيِّر[الإنجليزية](VBR)دقة شاشة العرضالإطارمُغدَّل الأطرen[الإنجليزية]المسح المُتداخِلجودة الفيديوأجزاءمُرمِّزالفيديوتحويل متراكب الحدود[الإنجليزية]تحويل جيب التمام المتقطع(DCT)مرشح تفكيك التجميع[الإنجليزية]تعويض الحركةالنظريةنظرية المعلوماتالاعتلاجتعقيد كولموغروفضغط ذو خسارةتكميمنظرية المُعدَّل والتشوُّه[الإنجليزية]فائضية}}بوابة اتصال عن بعدبوابة الفيزياءبوابة رياضياتبوابة علم الحاسوبمجلوبة من «https://ar.wikipedia.org/w/index.php?title=اعتلاج_(نظرية_المعلومات)&oldid=66331471»تصنيفات:إنتروبيا ومعلوماتضغط بياناتنظرية الأنظمة المعقدةنظرية المعلوماتتصنيفات مخفية:صيانة الاستشهاد: أسماء متعددة: قائمة المؤلفينقالب أرشيف الإنترنت بوصلات واي باكصيانة الاستشهاد: مكان بدون ناشرصيانة الاستشهاد: استشهادات بمسارات غير مؤرشفةصفحات تستخدم خاصية P279صفحات تستخدم خاصية P1269صفحات تستخدم خاصية P2579صفحات تستخدم خاصية P61صفحات تستخدم خاصية P4020صفحات بها مراجع ويكي بياناتصفحات تستخدم خاصية P2534صفحات تستخدم خاصية P7235صفحات تستخدم خاصية P7973صفحات تستخدم خاصية P8111صفحات تستخدم خاصية P461صفحات تستخدم وحدة بطاقة/بلا مدخلاتصفحات تستخدم وحدة بطاقةمقالات تستعمل قوالب معلوماتمقالات تحتاج إلى صورصفحات بها وصلات إنترويكيمقالات تحوي نصا بالإنجليزيةالاستشهاد بمصادر باللغة الإنجليزية (en)مقالات فيها معرفات FASTمقالات فيها معرفات BNEمقالات فيها معرفات BNFمقالات فيها معرفات GNDمقالات فيها معرفات J9Uمقالات فيها معرفات LCCNمقالات فيها معرفات NDL خاطئةكل المقالات التي تحوي معلومات ضبط استنادي خاطئةمقالات فيها معرفات NKCبوابة اتصال عن بعد/مقالات متعلقةبوابة الفيزياء/مقالات متعلقةبوابة رياضيات/مقالات متعلقةبوابة علم الحاسوب/مقالات متعلقةجميع المقالات التي تستخدم شريط بواباتآخر تعديل لهذه الصفحة كان يوم 6 مارس 2024، الساعة 05:48.النصوص متاحة تحترخصة المشاع الإبداعي الملزمة بنسبة العمل لمؤلفه وبترخيص الأعمال المشتقة بالمثل 4.0؛ قد تُطبّق شروط إضافية.', 'arاستخدامُك هذا الموقع هو موافقةٌ علىشروط الاستخداموسياسة الخصوصية.', 'arويكيبيديا ® هي علامة تجارية مسجلةلمؤسسة ويكيميديا، وهي منظمة غير ربحية.سياسة الخصوصيةحول ويكيبيدياإخلاء مسؤوليةالقواعد السلوكيةالمطورونإحصائياتبيان تعريف الارتباطاتنسخة للأجهزة المحمولةتبديل عرض المحتوى المحدود', 'urدرمائلت (اطلاعاتی نظریہ) - آزاد دائرۃ المعارف، ویکیپیڈیامندرجات کا رخ کریںمرکزی مینومرکزی مینوبغلی پٹی پر جائیںچھپائیںویکی پیمائیصفحۂ اولجستہ جستہ مطالعہنیا مضمون تحریر کریںرابطہ کریںتعاملویکیپیڈیا پر آغاز کریںمعاونتدیوان عامحالیہ تبدیلیاںمشاہدات اعلیٰتعداد ناظرینعطیہ دیجیےاپلوڈ تصویرتلاشتلاشکھاتہ بنائیںداخل ہوںذاتی آلاتکھاتہ بنائیںداخل ہوںPages for logged out editorsمزید تفصیلاتشراکتیںتبادلۂ خیالمندرجاتبغلی پٹی پر جائیںچھپائیںدیباچہ1تعریففہرست عناوین دیکھیںدرمائلت (اطلاعاتی نظریہ)45 زبانیںالعربيةفارسیEnglishAfrikaansБългарскиBoarischBosanskiCatalàČeštinaCymraegDanskDeutschΕλληνικάEspañolEuskaraFrançaisGalego한국어Bahasa IndonesiaItalianoעבריתLietuviųMagyarNederlands日本語Олык марийਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийShqipSimple EnglishSlovenčinaSlovenščinaکوردیСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаTiếng Việt粵語中文ترمیم روابطصفحہتبادلۂ خیالاردومطالعہترمیمتاریخچہآلاتآلاتبغلی پٹی پر جائیںچھپائیںاقداماتمطالعہترمیمتاریخچہعمومیمربوط صفحاتمتعلقہ تبدیلیاںخصوصی صفحاتمستقل ربطمعلومات صفحہمضمون کا حوالہمختصر یوآرایلڈاؤن لوڈ کیوآرمختصر یوآرایلڈیٹا آئٹمطباعت/برآمدتخلیق کتابڈاؤن لوڈ بشکلPDFقابل طبع نسخہدیگر منصوبےویکی ذخائرآزاد دائرۃ المعارف، ویکیپیڈیا سےاصطلاحtermاطلاعاتلاتیقندرمائلتInformationuncertaintyentropyاطلاعاتی نظریات میں کسیتصادفی متغیرکیدرمائلتاس سے وابستہ لاتیقن کو ناپتی ہوتی ہے۔تعریف[ترمیم]کسی متفرد تصادفی متغیرXجو ممکنہ اقدار,x_,\\\\cdots ,x_\\\\}}لے سکے، کیاطلاعاتی درمائلتH(X)، یوں تعریف ہوتی ہےH(X)=E\\u2061(I(X))=−∑i=1np(xi)log2\\u2061p(xi) (I(X))=-\\\\sum _^)\\\\log _p(x_)}}جہاںI(X)تصادفی متغیرXکا اطلاعاتی متن ہے، جو خود بھی تصادفی متغیر ہےE(X)تصادفی متغیرXکامتوقع قدرتفاعل ہےp(xi) = Pr(X=xi)تصادفی متغیرXکی \"احتمال کمیت دالہ\" ہےلاگرتھم کی اساس 2 ہے اور0log\\u20610=0E=mc2اردو ویکیپیڈیا پر ریاضی مساوات کو بائیں سے دائیںLTRپڑھیٔےریاضی علاماتاخذ کردہ از «https://ur.wikipedia.org/w/index.php?title=درمائلت_(اطلاعاتی_نظریہ)&oldid=5820022»زمرہ جات:اطلاعاتی نظریاتنظریہ احصاءپوشیدہ زمرہ:ویکی ڈیٹا سے مختلف مختصر وضاحتاس صفحہ میں آخری بار مورخہ 14 جنوری 2024ء کو 12:31 بجے ترمیم کی گئی۔تمام متنکری ئیٹیو کامنز انتساب / یکساں-شراکت اجازت نامہکے تحت دستیاب ہے، اضافی شرائط بھی عائد ہو سکتی ہیں۔ تفصیل کے لیےاستعمال کی شرائطملاحظہ فرمائیں۔ خیال رہے کہ ویکیپیڈیا® ایک غیر منفعت بخش تنظیمویکی میڈیا فاؤنڈیشن انکارپوریشنکا تجارتی مارکہ ہے۔اخفائے راز کے اصولویکیپیڈیا کا تعارفعمومی اظہار لاتعلقیضابطۂ اخلاقترقی دہندگانشماریاتکوکیز کی تفصیلاتموبائل سائٹToggle limited content width', 'ruИнформационная энтропия — ВикипедияИнформационная энтропия[править|править код]Материал из Википедии — свободной энциклопедииТекущая версия страницы покане проверяласьопытными участниками и может значительно отличаться отверсии, проверенной 26 августа 2023 года; проверки требуют7 правок.Текущая версия страницы покане проверяласьопытными участниками и может значительно отличаться отверсии, проверенной 26 августа 2023 года; проверки требуют7 правок.Перейти к навигацииПерейти к поискуТеория информацииЭнтропияДифференциальная энтропияУсловная энтропияСовместная энтропияВзаимная информацияУсловная взаимная информацияОтносительная энтропияЭнтропийная скоростьСвойство асимптотической равнораспределенностиТеория частотных искаженийТеорема Шеннона об источнике шифрованияПропускная способностьТеоремы Шеннона для канала с шумамиТеорема Шеннона — ХартлиИнформацио́нная энтропи́я— мера неопределённости некоторой системы (встатистической физикеилитеории информации), в частности, непредсказуемость появления какого-либо символапервичного алфавита.', 'ruВ последнем случае при отсутствии информационных потерь энтропия численно равна количествуинформациина символ передаваемого сообщения.Например, в последовательности букв, составляющих какое-либо предложение на русском языке, разные буквы появляются с разнойчастотностью, поэтому неопределённость появления для некоторых букв меньше, чем для других.', 'ruЕсли же учесть, что некоторые сочетания букв (в этом случае говорят об энтропииn-го порядка, см.ниже) встречаются очень редко, то неопределённость уменьшается еще сильнее.Содержание1Формальные определения1.1Определение по Шеннону1.2Определение с помощью собственной информации1.3Единицы измерения информационной энтропии2Свойства2.1Математические свойства2.2Эффективность3Вариации и обобщения3.1b-арная энтропия3.2Условная энтропия3.3Взаимная энтропия4История5См.', 'ruтакже6Примечания7Литература8СсылкиФормальные определения[править|править код]Информационнаядвоичная энтропия, при отсутствии информационных потерь, рассчитывается поформуле Хартли:I=log2\\u2061NN},гдеN—мощностьалфавита,I— количество информации в каждом символе сообщения.', 'ruДля случайной величиныx, принимающейnнезависимых случайных значенийxi}с вероятностямиpi}(i=1,...n), формула Хартли переходит в формулу Шеннона:H(x)=−∑i=1npilog2\\u2061pi.^p_\\\\log _p_.', 'ru}Здесь(−log2\\u2061pi)p_)}означает измеряемое вбитахколичество информации, содержащейся в том событии, что случайная величина приняла значениеxi}(для предложений на русском языке — количество информации, содержащейся в конкретной букве, имеющей номерiв русском алфавите,i=1,...33),H(x)— количество информации, котороев среднемприходится на одно событие (для предложений на русском языке — количество информации в среднем на одну букву).Эта величина также называетсясредней энтропией сообщенияи означает измеряемое в битах среднее количествоинформациина символ передаваемого сообщения.', 'ruВеличинаHi=−log2\\u2061pi=-\\\\log _}}называетсячастной энтропией, характеризующей толькоi-e состояние.Таким образом, энтропия системыxявляется суммой с противоположным знаком всех относительных частотностей появления состояния (события) с номеромi, умноженных на их жедвоичные логарифмы[1].', 'ruЭто определение для дискретных случайных событий можно формально расширить для непрерывных распределений, заданныхплотностью распределения вероятностей, однако полученный функционал будет обладать несколько иными свойствами (см.дифференциальная энтропия).В общем случае, основание логарифма в определении энтропии может быть любым, большим 1 (так как алфавитом, состоящим только из одного символа, нельзя передавать информацию); выбор основания логарифма определяет единицу измерения энтропии.', 'ruДля информационных систем, основанных на двоичной системе счисления, единицей измерения информационной энтропии (собственно, информации) являетсябит.', 'ruВ задачах математической статистики более удобным может оказаться применениенатурального логарифма, в этом случае единицей измерения информационной энтропии являетсянат.Определение по Шеннону[править|править код]Клод Шеннонпредположил, что прирост информации равен утраченной неопределённости, и задал требования к её измерению:мера должна быть непрерывной; то есть изменение значения величины вероятности на малую величину должно вызывать малое результирующее изменение функции;в случае, когда все варианты (буквы в приведённом примере) равновероятны, увеличение количества вариантов (букв) должно всегда увеличивать значение функции;должна быть возможность сделать выбор (в нашем примере — букв) в два шага, в которых значение функции конечного результата должно являться суммой функций промежуточных результатов.', 'ru[прояснить]Поэтому функция энтропииHдолжна удовлетворять условиямH(p1,…,pn),\\\\;\\\\ldots ,\\\\;p_)}определена и непрерывна для всехp1,…,pn,\\\\dotsc ,p_}, гдеpi∈[0,1]\\\\in [0,\\\\;1]}для всехi=1,…,nиp1+⋯+pn=1+\\\\dotsb +p_=1}.', 'ru(Эта функция зависит только от распределения вероятностей, но не от алфавита.', 'ru)Для целых положительныхn, должно выполняться следующее неравенство:H(1n,…,1n)⏟n<H(1n+1,…,1n+1)⏟n+1.', 'ru},\\\\;\\\\ldots ,\\\\;}\\\\right)} _<H\\\\underbrace },\\\\;\\\\ldots ,\\\\;}\\\\right)} _.', 'ru}Для целых положительныхbi}, гдеb1+…+bk=n+\\\\ldots +b_=n}, должно выполняться равенствоH(1n,…,1n)⏟n=H(b1n,…,bkn)+∑i=1kbinH(1bi,…,1bi)⏟bi.', 'ru},\\\\;\\\\ldots ,\\\\;}\\\\right)} _=H\\\\left(}},\\\\;\\\\ldots ,\\\\;}}\\\\right)+\\\\sum _^}}H\\\\underbrace }},\\\\;\\\\ldots ,\\\\;}}\\\\right)} _}.', 'ru}Шеннон показал,[2]что единственная функция, удовлетворяющая этим требованиям, имеет вид−K∑i=1np(i)log2\\u2061p(i),^p(i)\\\\log _p(i),}гдеK— положительная константа (и в действительности нужна только для выбора единицы измерения энтропии; изменение этой константы равносильно изменению основания логарифма).Шеннон определил, что измерение энтропии (H=−p1log2\\u2061p1−…−pnlog2\\u2061pn\\\\log _p_-\\\\ldots -p_\\\\log _p_}), применяемое к источнику информации, может определить требования к минимальной пропускной способности канала, требуемой для надёжной передачи информации в виде закодированных двоичных чисел.', 'ruДля вывода формулы Шеннона необходимо вычислитьматематическое ожидание«количества информации», содержащегося в цифре из источника информации.', 'ruМера энтропии Шеннона выражает неуверенность реализации случайной переменной.', 'ruТаким образом, энтропия является разницей между информацией, содержащейся в сообщении, и той частью информации, которая точно известна (или хорошо предсказуема) в сообщении.', 'ruПримером этого являетсяизбыточность языка— имеются явные статистические закономерности в появлении букв, пар последовательных букв, троек и т. д.', 'ru(см.цепи Маркова).Определение энтропии Шеннона связано с понятиемтермодинамической энтропии.БольцманиГиббспроделали большую работу по статистической термодинамике, которая способствовала принятию слова «энтропия» в информационную теорию.', 'ruСуществует связь между термодинамической и информационной энтропией.', 'ruНапример,демон Максвеллатакже противопоставляет термодинамическую энтропию информации, и получение какого-либо количества информации равно потерянной энтропии.Определение с помощью собственной информации[править|править код]Также можно определить энтропию случайной величины, предварительно введя понятие распределения случайной величиныX, имеющей конечное число значений:[3]PX(xi)=pi,pi⩾0,i=1,2,…,n(x_)=p_,\\\\quad p_\\\\geqslant 0,\\\\;i=1,\\\\;2,\\\\;\\\\ldots ,\\\\;n}∑i=1npi=1^p_=1}исобственной информации:I(X)=−log\\u2061PX(X).(X).', 'ru}Тогда энтропия определяется как:H(X)=E(I(X))=−∑i=1np(i)log\\u2061p(i).', 'ru(I(X))=-\\\\sum _^p(i)\\\\log p(i).', 'ru}Единицы измерения информационной энтропии[править|править код]От основания логарифма зависит единица измерения количества информации и энтропии:бит,нат,тритилихартли.Свойства[править|править код]Энтропия является количеством, определённым в контексте вероятностной модели дляисточника данных.', 'ruНапример, кидание монеты имеет энтропию:−2(12log2\\u206112)=−log2\\u206112=log2\\u20612=1}\\\\log _}\\\\right)=-\\\\log _}=\\\\log _2=1}битна одно кидание (при условии его независимости), а количествовозможных состоянийравно:21=2=2}возможных состояния(значения) («орёл» и «решка»).У источника, который генерирует строку, состоящую только из букв «А», энтропия равна нулю:−∑i=1∞log2\\u20611=0^\\\\log _1=0}, а количествовозможных состоянийравно:20=1=1}возможное состояние(значение) («А») и от основания логарифма не зависит.Это тоже информация, которую тоже надо учитывать.', 'ruПримеромзапоминающих устройств, в которых используются разряды с энтропией, равной нулю, но сколичеством информации, равным одномувозможному состоянию, то есть не равным нулю, являются разряды данных записанных вПЗУ, в которых каждый разряд имеет только одновозможное состояние.Так, например, опытным путём можно установить, что энтропия английского текста равна 1,5 бит на символ, что будет варьироваться для разных текстов.', 'ruСтепень энтропии источника данных означает среднее число битов на элемент данных, требуемых для их (данных) зашифровки без потери информации, при оптимальном кодировании.Некоторые биты данных могут не нести информации.', 'ruНапример, структуры данных часто хранят избыточную информацию или имеют идентичные секции независимо от информации в структуре данных.Количество энтропии не всегда выражается целым числом битов.Математические свойства[править|править код]Неотрицательность:H(X)⩾0.Ограниченность:H(X)=−E\\u2061(log2\\u2061pi)=∑i=1npilog2\\u20611pi=∑i=1npif(gi)⩽f(∑i=1npigi)=log2\\u2061n } (\\\\log _p_)=\\\\sum _^p_\\\\log _}}=\\\\sum _^p_f(g_)\\\\leqslant f\\\\left(\\\\sum _^p_g_\\\\right)=\\\\log _n}, что вытекает изнеравенства Йенсенадля вогнутой функцииf(gi)=log2\\u2061gi)=\\\\log _g_}иgi=1pi=}}}.', 'ruЕсли всеnэлементов изXравновероятны,H(X)=log2\\u2061nn}.ЕслиX,Yнезависимы, тоH(X⋅Y)=H(X)+H(Y).Энтропия — выпуклая вверх функция распределения вероятностей элементов.ЕслиX,Yимеют одинаковое распределение вероятностей элементов, тоH(X)=H(Y).Эффективность[править|править код]Алфавит может иметь вероятностное распределение, далекое отравномерного.', 'ruЕсли исходный алфавит содержитnсимволов, тогда его можно сравнить с «оптимизированным алфавитом», вероятностное распределение которого — равномерное.', 'ruСоотношение энтропии исходного и оптимизированного алфавита — этоэффективностьисходного алфавита, которая может быть выражена в процентах.', 'ruЭффективность исходного алфавита сnсимволами может быть также определена как егоn-арная энтропия.Энтропия ограничивает максимально возможное сжатие без потерь (или почти без потерь), которое может быть реализовано при использовании теоретически —типичного набораили, на практике, —кодирования Хаффмана,кодирования Лемпеля — Зива — Велчаилиарифметического кодирования.Вариации и обобщения[править|править код]b-арная энтропия[править|править код]В общем случаеb-арная энтропия(гдеbравно 2, 3, …) источникаS=(S,P)}=(S,\\\\;P)}сисходным алфавитомS=,\\\\;\\\\ldots ,\\\\;a_\\\\}}идискретным распределением вероятностиP=,,\\\\;\\\\ldots ,\\\\;p_\\\\},}гдеpi}является вероятностьюai}(pi=p(ai)=p(a_)}), определяется формулой:Hb(S)=−∑i=1npilogb\\u2061pi.', 'ru(})=-\\\\sum _^p_\\\\log _p_.', 'ru}В частности, приb=2, мы получаем обычную двоичную энтропию, измеряемую вбитах.', 'ruПриb=3, мы получаем тринарную энтропию, измеряемую втритах(один трит имеет источник информации с тремя равновероятными состояниями).', 'ruПриb=eмы получаем информацию, измеряемую внатах.Условная энтропия[править|править код]Если следование символов алфавита не независимо (например, вофранцузском языкепосле буквы «q» почти всегда следует «u», а после слова «передовик» в советских газетах обычно следовало слово «производства» или «труда»), количество информации, которую несёт последовательность таких символов (а, следовательно, и энтропия) меньше.', 'ruДля учёта таких фактов используется условная энтропия.Условной энтропиейпервого порядка (аналогично дляМарковской моделипервого порядка) называется энтропия для алфавита, где известнывероятностипоявления одной буквы после другой (то есть вероятности двухбуквенных сочетаний):H1(S)=−∑ipi∑jpi(j)log2\\u2061pi(j),(})=-\\\\sum _p_\\\\sum _p_(j)\\\\log _p_(j),}гдеi— это состояние, зависящее от предшествующего символа, иpi(j)(j)}— это вероятностьjпри условии, чтоiбыл предыдущим символом.Например, для русского языка без буквы «ё»H0=5,H1=4,358,H2=3,52,H3=3,01=5,\\\\;H_=4358,\\\\;H_=352,\\\\;H_=301}[4].Через частную и общую условные энтропии полностью описываются информационные потери припередаче данныхв канале с помехами.', 'ruДля этого применяются так называемыеканальные матрицы.', 'ruДля описания потерь со стороны источника (то есть известен посланный сигнал) рассматриваютусловную вероятностьp(bj∣ai)\\\\mid a_)}получения приёмником символаbj}при условии, что был отправлен символai}.', 'ruПри этом канальная матрица имеет следующий вид:b1}b2}…bj}…bm}a1}p(b1∣a1)\\\\mid a_)}p(b2∣a1)\\\\mid a_)}…p(bj∣a1)\\\\mid a_)}…p(bm∣a1)\\\\mid a_)}a2}p(b1∣a2)\\\\mid a_)}p(b2∣a2)\\\\mid a_)}…p(bj∣a2)\\\\mid a_)}…p(bm∣a2)\\\\mid a_)}…………………ai}p(b1∣ai)\\\\mid a_)}p(b2∣ai)\\\\mid a_)}…p(bj∣ai)\\\\mid a_)}…p(bm∣ai)\\\\mid a_)}…………………am}p(b1∣am)\\\\mid a_)}p(b2∣am)\\\\mid a_)}…p(bj∣am)\\\\mid a_)}…p(bm∣am)\\\\mid a_)}Вероятности, расположенные по диагонали, описывают вероятность правильного приёма, а сумма всех элементов любой строки даёт 1.', 'ruПотери, приходящиеся на передаваемый сигналai}, описываются через частную условную энтропию:H(B∣ai)=−∑j=1mp(bj∣ai)log2\\u2061p(bj∣ai).', 'ru)=-\\\\sum _^p(b_\\\\mid a_)\\\\log _p(b_\\\\mid a_).', 'ru}Для вычисления потерь при передаче всех сигналов используется общая условная энтропия:H(B∣A)=∑ip(ai)H(B∣ai).p(a_)H(B\\\\mid a_).', 'ru}H(B∣A)означает энтропию со стороны источника, аналогично рассматриваетсяH(A∣B)— энтропия со стороны приёмника: вместоp(bj∣ai)\\\\mid a_)}всюду указываетсяp(ai∣bj)\\\\mid b_)}(суммируя элементы строки можно получитьp(ai))}, а элементы диагонали означают вероятность того, что был отправлен именно тот символ, который получен, то есть вероятность правильной передачи).Взаимная энтропия[править|править код]Взаимная энтропия илиэнтропия объединенияпредназначена для расчёта энтропии взаимосвязанных систем (энтропии совместного появления статистически зависимых сообщений) и обозначаетсяH(AB), гдеAхарактеризует передатчик, аB— приёмник.Взаимосвязь переданных и полученных сигналов описывается вероятностями совместных событийp(aibj)b_)}, и для полного описания характеристик канала требуется только одна матрица:p(a1b1)b_)}p(a1b2)b_)}…p(a1bj)b_)}…p(a1bm)b_)}p(a2b1)b_)}p(a2b2)b_)}…p(a2bj)b_)}…p(a2bm)b_)}………………p(aib1)b_)}p(aib2)b_)}…p(aibj)b_)}…p(aibm)b_)}………………p(amb1)b_)}p(amb2)b_)}…p(ambj)b_)}…p(ambm)b_)}Для более общего случая, когда описывается не канал, а в целом взаимодействующие системы, матрица необязательно должна быть квадратной.', 'ruСумма всех элементов столбца с номеромjдаётp(bj))}, сумма строки с номеромiестьp(ai))}, а сумма всех элементов матрицы равна 1.', 'ruСовместная вероятностьp(aibj)b_)}событийai}иbj}вычисляется как произведение исходной и условной вероятности:p(aibj)=p(ai)p(bj∣ai)=p(bj)p(ai∣bj).b_)=p(a_)p(b_\\\\mid a_)=p(b_)p(a_\\\\mid b_).', 'ru}Условные вероятности производятся поформуле Байеса.', 'ruТаким образом, имеются все данные для вычисления энтропий источника и приёмника:H(A)=−∑i(∑jp(aibj)log\\u2061∑jp(aibj)),\\\\left(\\\\sum _p(a_b_)\\\\log \\\\sum _p(a_b_)\\\\right),}H(B)=−∑j(∑ip(aibj)log\\u2061∑ip(aibj)).\\\\left(\\\\sum _p(a_b_)\\\\log \\\\sum _p(a_b_)\\\\right).', 'ru}Взаимная энтропия вычисляется последовательным суммированием по строкам (или по столбцам) всех вероятностей матрицы, умноженных на их логарифм:H(AB)=−∑i∑jp(aibj)log\\u2061p(aibj).\\\\sum _p(a_b_)\\\\log p(a_b_).', 'ru}Единица измерения — бит/два символа, это объясняется тем, что взаимная энтропия описывает неопределённость на пару символов: отправленного и полученного.', 'ruПутём несложных преобразований также получаемH(AB)=H(A)+H(B∣A)=H(B)+H(A∣B).Взаимная энтропия обладает свойствоминформационной полноты— из неё можно получить все рассматриваемые величины.История[править|править код]В1948 году, исследуя проблему рациональной передачи информации через зашумлённый коммуникационный канал,Клод Шеннонпредложил революционный вероятностный подход к пониманию коммуникаций и создал первую, истинно математическую, теориюэнтропии.', 'ruЕго сенсационные идеи быстро послужили основой разработки двух основных направлений:теории информации, которая использует понятиевероятностииэргодическую теориюдля изучения статистических характеристик данных и коммуникационных систем, итеории кодирования, в которой используются главным образом алгебраические и геометрические инструменты для разработки эффективных кодов.Понятие энтропии как меры случайности введено Шенноном в его статье «Математическая теория связи» (англ.A Mathematical Theory of Communication), опубликованной в двух частях вBell System Technical Journalв 1948 году.См.', 'ruтакже[править|править код]Дифференциальная энтропия(энтропия для непрерывного распределения)Взаимная информацияЭнтропийное кодированиеЦепь МарковаРасстояние Кульбака — ЛейблераПримечания[править|править код]↑Данное представление удобно для работы с информацией, представленной в двоичной форме; в общем случае основание логарифма может быть другим.↑Shannon, Claude E.A Mathematical Theory of Communication(неопр.', 'ru)//Bell System Technical Journal(англ.)(рус..', 'ru— 1948.', 'ru— July (т.', 'ru27,№ 3).', 'ru—С.', 'ru419.', 'ru—doi:10.1002/j.1538-7305.1948.tb01338.x.Архивировано1 августа 2016 года.↑Габидулин Э. М.,Пилипчук Н. И.Лекции по теории информации—МФТИ, 2007.', 'ru— С.', 'ru16.', 'ru— 214 с.', 'ru—ISBN 978-5-7417-0197-3↑Лебедев Д. С., Гармаш В. А.О возможности увеличения скорости передачи телеграфных сообщений.', 'ru— М.: Электросвязь, 1958.', 'ru— № 1.', 'ru— С.', 'ru68—69.Литература[править|править код]Шеннон К.Работы по теории информации и кибернетике.', 'ru—М.', 'ru:Издательство иностранной литературы, 2002.Волькенштейн М. В.Энтропия и информация.', 'ru—М.', 'ru: Наука, 2006.Цымбал В. П.Теория информации и кодирование.', 'ru—К.', 'ru: Вища Школа, 2003.Martin, Nathaniel F.G. & England, James W.Mathematical Theory of Entropy.', 'ru—Cambridge University Press, 2011.', 'ru—ISBN 978-0-521-17738-2.Шамбадаль П.Развитие и приложение понятия энтропии.', 'ru—М.', 'ru: Наука, 1967.', 'ru— 280 с.Мартин Н., Ингленд Дж.Математическая теория энтропии.', 'ru—М.', 'ru: Мир, 1988.', 'ru— 350 с.Хинчин А. Я.Понятие энтропии в теории вероятностей(рус.', 'ru)//Успехи математических наук.', 'ru—Российская академия наук, 1953.', 'ru—Т.', 'ru8,вып.', 'ru3(55).', 'ru—С.', 'ru3—20.Брюллюэн Л.Наука и теория информации.', 'ru—М., 1960.Винер Н.Кибернетика и общество.', 'ru—М., 1958.Винер Н.Кибернетика или управление и связь в животном и машине.', 'ru—М., 1968.Петрушенко Л. А.Самодвижение материи в свете кибернетики.', 'ru—М., 1974.Эшби У. Р.Введение в кибернетику.', 'ru—М., 1965.Яглом А. М.,Яглом И. М.Вероятность и информация.', 'ru—М., 1973.Волькенштейн М. В.Энтропия и информация.', 'ru—М.', 'ru: Наука, 1986.', 'ru— 192 с.Верещагин Н. К., Щепин Е. В.Информация, кодирование и предсказание.', 'ru—М.', 'ru: ФМОП, МЦНМО, 2012.', 'ru— 238 с.', 'ru—ISBN 978-5-94057-920-5.Ссылки[править|править код]Shannon Claude E.A Mathematical Theory of CommunicationАрхивная копияот 31 января 1998 наWayback Machine(англ.', 'ru)Коротаев С. М.Энтропия и информация — универсальные естественнонаучные понятия.Ссылки на внешние ресурсыТематические сайтыMathWorldСловари и энциклопедииБольшая китайскаяБольшая китайскаяБольшая китайскаяБольшая российская (научно-образовательный портал)ИтальянскаяBritannica (онлайн)Britannica (онлайн)De AgostiniTreccaniВ библиографических каталогахBNE:XX535116BNF:11985913jGND:4743861-7J9U:987007550784405171LCCN:sh85044152NDL:01191172NKC:ph425914Источник —https://ru.wikipedia.org/w/index.php?title=Информационная_энтропия&oldid=136172185Категории:Теория информацииКибернетикаТеория вероятностейЭнтропияСкрытые категории:Страницы, использующие волшебные ссылки ISBNВикипедия:Статьи, требующие внесения ясностиВикипедия:Статьи с шаблонами недостатков по алфавитуВикипедия:Статьи с источниками из ВикиданныхНавигацияПерсональные инструментыВы не представились системеОбсуждениеВкладСоздать учётную записьВойтиПространства имёнСтатьяОбсуждениерусскийПросмотрыЧитатьТекущая версияПравитьПравить кодИсторияЕщёПоискНавигацияЗаглавная страницаСодержаниеИзбранные статьиСлучайная статьяТекущие событияПожертвоватьУчастиеСообщить об ошибкеКак править статьиСообществоФорумСвежие правкиНовые страницыСправкаИнструментыСсылки сюдаСвязанные правкиСлужебные страницыПостоянная ссылкаСведения о страницеЦитировать страницуПолучить короткий URLСкачать QR-кодПечать/экспортСкачать как PDFВерсия для печатиВ других проектахВикискладЭлемент ВикиданныхНа других языкахAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Править ссылкиЭта страница в последний раз была отредактирована 15 февраля 2024 в 12:01.Текст доступен полицензии Creative Commons «С указанием авторства — С сохранением условий» (CC BY-SA); в отдельных случаях могут действовать дополнительные условия.Подробнее см.Условия использования.Wikipedia® — зарегистрированный товарный знак некоммерческой организации«Фонд Викимедиа» (Wikimedia Foundation, Inc.)Политика конфиденциальностиОписание ВикипедииОтказ от ответственностиСвяжитесь с намиКодекс поведенияРазработчикиСтатистикаЗаявление о кукиМобильная версия', 'idEntropi (teori informasi) - Wikipedia bahasa Indonesia, ensiklopedia bebasLompat ke isiMenu utamaMenu utamapindah ke bilah sisisembunyikanNavigasiHalaman UtamaDaftar isiPerubahan terbaruArtikel pilihanPeristiwa terkiniHalaman baruHalaman sembarangKomunitasWarung KopiPortal komunitasBantuanWikipediaTentang WikipediaPancapilarKebijakanMenyumbangHubungi kamiBak pasirBagikanPencarianCariBuat akun baruMasuk logPerkakas pribadiBuat akun baruMasuk logHalaman penyunting yang telah keluar logpelajari lebih lanjutKontribusiPembicaraanDaftar isipindah ke bilah sisisembunyikanAwal1Referensi2Bacaan lebih lanjut3Pranala luarGulingkan daftar isiEntropi (teori informasi)45 bahasaAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Sunting pranalaHalamanPembicaraanBahasa IndonesiaBacaSuntingSunting sumberLihat riwayatPerkakasPerkakaspindah ke bilah sisisembunyikanTindakanBacaSuntingSunting sumberLihat riwayatUmumPranala balikPerubahan terkaitHalaman istimewaPranala permanenInformasi halamanKutip halaman iniLihat URL pendekUnduh kode QRButir di WikidataPranala menurut IDCetak/eksporBuat bukuUnduh versi PDFVersi cetakDalam proyek lainWikimedia CommonsDari Wikipedia bahasa Indonesia, ensiklopedia bebasDalamteori informasi,entropivariabel acakadalah tingkat rata-rata \"informasi\", \"kejutan\", atau \"ketidakpastian\" yang melekat pada kemungkinan hasil variabel.', 'idDiberikan variabel acak diskritX, yang mengambil nilai dalam alfabetX}}dan didistribusikan menurutp:X→[0,1]}\\\\to [0,1]}:H(X)=−∑i=1nP(xi)log\\u2061P(xi) (X)=-\\\\sum _^ (x_)\\\\log \\\\mathrm (x_)}}di manaΣmenunjukkan jumlah atas nilai-nilai variabel yang mungkin.', 'idPilihan dasar untuklog,logaritma, bervariasi untuk aplikasi yang berbeda.', 'idBasis 2 memberikan satuanbit(atau \"shannon\"), sedangkan basisememberikan \"satuan alami\"nat, dan basis 10 memberikan satuan \"dit\", \"ban\", atau \"hartley\".', 'idDefinisi entropi yang setara adalah nilai yangdiharapkandariinformasi dirisuatu variabel.', 'id[1]Konsep entropi informasi diperkenalkan olehClaude Shannondalam makalahnya tahun 1948 \"Sebuah Teori Matematika Komunikasi\",[2][3]dan juga disebut sebagaientropi Shannon.', 'idTeori Shannon mendefinisikan sistemkomunikasi datayang terdiri dari tiga elemen: sumber data,saluran komunikasi, dan penerima.', 'idMasalah mendasar komunikasi - seperti yang diungkapkan oleh Shannon - adalah agar penerima dapat mengidentifikasi data apa yang dihasilkan oleh sumber, berdasarkan sinyal yang diterimanya melalui saluran.', 'id[2][3]Shannon mempertimbangkan berbagai cara untuk mengkodekan, memampatkan, dan mengirimkan pesan dari sumber data, dan membuktikan dalamteorema pengkodean sumbernyayang terkenal bahwa entropi mewakili batas matematis absolut tentang seberapa baik data dari sumber dapat dimampatkantanpa mengalami kehilanganke saluran tanpa derau sama sekali.', 'idShannon memperkuat hasil ini secara signifikan untuk saluran berderau dalamteorema pengkodean saluran berderaunya.Entropi dalam teori informasi secara langsung dianalogikan denganentropidalamtermodinamika statistik.', 'idHasil analogi ketika nilai-nilai variabel acak menunjukkan energi keadaan mikro, sehingga rumus Gibbs untuk entropi secara formal identik dengan rumus Shannon.', 'idEntropi memiliki relevansi dengan bidang matematika lainnya sepertikombinatorikadanpembelajaran mesin.', 'idDefinisi tersebut dapat diturunkan dari serangkaianaksiomayang menetapkan bahwa entropi harus menjadi ukuran seberapa \"mengejutkan\" hasil rata-rata suatu variabel.', 'idUntuk variabel acak kontinu,entropi diferensialanalog dengan entropi.Referensi[sunting|sunting sumber]^Pathria, R. K.; Beale, Paul (2011).Statistical Mechanics(edisi ke-Third).', 'idAcademic Press.', 'idhlm.', 'id51.ISBN978-0123821881.^abShannon, Claude E.(July 1948).', 'idA Mathematical Theory of Communication.Bell System Technical Journal.27(3): 379–423.doi:10.1002/j.1538-7305.1948.tb01338.x.', 'id(PDF, archived fromhere)^abShannon, Claude E.(October 1948).', 'idA Mathematical Theory of Communication.Bell System Technical Journal.27(4): 623–656.doi:10.1002/j.1538-7305.1948.tb00917.x.', \"id(PDF, archived fromhere)This article incorporates material from Shannon's entropy on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.Bacaan lebih lanjut[sunting|sunting sumber]Cover, T.M., Thomas, J.A.\", 'id(2006),Elements of Information Theory - 2nd Ed., Wiley-Interscience,ISBN978-0-471-24195-9MacKay, D.J.C.', 'id(2003),Information Theory, Inference and Learning Algorithms, Cambridge University Press,ISBN978-0-521-64298-9Arndt, C. (2004),Information Measures: Information and its Description in Science and Engineering, Springer,ISBN978-3-540-40855-0Gray, R. M. (2011),Entropy and Information Theory, Springer.Martin, Nathaniel F.G. & England, James W. (2011).Mathematical Theory of Entropy.', 'idCambridge University Press.ISBN978-0-521-17738-2.Pemeliharaan CS1: Menggunakan parameter penulis (link)Shannon, C.E., Weaver, W. (1949)The Mathematical Theory of Communication, Univ of Illinois Press.ISBN0-252-72548-4ISBN0-252-72548-4Stone, J. V. (2014), Chapter 1 ofInformation Theory: A Tutorial Introduction, University of Sheffield, England.ISBN978-0956372857ISBN978-0956372857.Pranala luar[sunting|sunting sumber]Hazewinkel, Michiel, ed.', 'id(2001) [1994],\"Entropy\",Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers,ISBN978-1-55608-010-4\"Entropy\"at Rosetta Code—repository of implementations of Shannon entropy in different programming languages.Entropyan interdisciplinary journal on all aspects of the entropy concept.', 'idOpen access.Artikel bertopikmatematikaini adalah sebuahrintisan.', 'idAnda dapat membantu Wikipedia denganmengembangkannya.lbsDiperoleh dari \"https://id.wikipedia.org/w/index.php?title=Entropi_(teori_informasi)&oldid=21702189\"Kategori:Keserampangan statistikTeori informasiEntropi dan informasiKompresi dataKategori tersembunyi:Pemeliharaan CS1: Menggunakan parameter penulisSemua artikel rintisanRintisan bertopik matematikaSemua artikel rintisan September 2022Halaman ini terakhir diubah pada 25 September 2022, pukul 03.45.Teks tersedia di bawahLisensi Atribusi-BerbagiSerupa Creative Commons; ketentuan tambahan mungkin berlaku.', 'idLihatKetentuan Penggunaanuntuk rincian lebih lanjut.Kebijakan privasiTentang WikipediaPenyangkalanKode EtikPengembangStatistikPernyataan kukiTampilan selulerGulingkan lebar konten terbatas', 'suÉntropi informasi - Wikipédia Sunda, énsiklopédi bébasLompat ke isiMenu utamaMenu utamapindah ke bilah sisisumputkeunPituduhTepasPanglawunganKeur lumangsungAnyar robahKaca acakPitulungSumbanganSawalaPaluruhPaluruhJieun akunAsup logParabot pribadiJieun akunAsup logHalaman penyunting yang telah keluar logpelajari lebih lanjutKontribusiObrolanEusipindah ke bilah sisisumputkeunMimiti1Bubuka2Tumbu luarGulingkan daftar isiÉntropi informasi45 basaAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Édit tutumbuKacaSawalaSundaBacaÉditÉdit sumberTémbongkeun jujutanParabotParabotpindah ke bilah sisisumputkeunTindakanBacaÉditÉdit sumberTémbongkeun jujutanUmumAnu nutumbu ka dieuParobahan nu pataliUnjal berkasKaca hususTutumbu permanénÉmbaran kacaCutat ieu artikelLihat URL pendekUnduh kode QRButir di WikidataCitak/éksporJieun hiji pustakaUndeur minangka PDFVérsi citakeunDi proyék liannaWikimedia CommonsTi Wikipédia Sunda, énsiklopédi bébasEntropipamariksaan Bernoulisalaku fungsi probabilitas suksés, ilahar disebutfungsi éntropi binér.Entropinyaéta konsép dinatérmodinamika(tempoéntropi termodinamika),mekanika statistikajeungtiori information.', 'suKonsép éntropi jeung informasi kacida pakait raket, sanajan perlu sababaraha waktu keur ngawangun tiorimekanika statistikajeungtiori informasisangkan jadi nyata.', 'suArtikel ieu ngeunaanéntropi informasi, formulasi tiori-informasiéntropi.', \"suÉntropi informasi leuwih ilahar disebutÉntropi Shannonkeur ngahargaan kaClaude E. Shannon.Bubuka[édit|édit sumber]Konsép éntropi dinatiori informasingajelaskeun sabaraha lobatingkat acak(atawa, 'kateupastian') anu aya dinasinyalatawa kajadian acak.\", 'suCara lian pikeun nempo ngeunaan ieu nyaéta ku ngabahas ngeunaan sabaraha loba informasi anu dibawa ku hiji sinyal.Contona, aya sababaraha téks dinabasa Inggris, dikodekeun mangrupa runtuyanhurup, spasi, jeungtanda baca.', \"suLantaran frékuénsi atawa lobana kajadian pikeun sababaraha hurup ngan saeutik (conto, 'z'), sedengkeun hurup-hurup lianna mah murudul (conto 'e'), runtuyan hurup téh henteu acak-acak teuing.\", \"suSanajan kitu, lantaran hurup nu nuturkeunana henteu bisa dikira-kira, bisa ogé disebut 'acak' (nepikeun ka tingkatan nu tangtu).\", 'suÉntropi téh minangka nu jadi ukuran tingkat acak ieu, saperti anu diusulkeun ku Shannon dina makalah \"A Mathematical Theory of CommunicationArchived1998-01-31 diWayback Machine\" nu medar dina taun1948Shannon nawarkeun hiji définisi éntropi anu nyumponan asumsi-asumsi, yén:Ukuran ieu kudu proporsional (kontinu atawa silisambung) - contona, lamun probabilitasna ngaganti saeutik, éntropina ogé ngan kaganti saeutik.Upama sakabéh kamungkinan (contona hurup-hurup di luhur) siga-siga rék sarua, mangka nambahan hurup anu kaluar ogé bakal naékkeun harga éntropi.Lamun aya dua tahapan kajadian, harga éntropi ahir ogé kudu dihasilkeun tina pangjumlahan éntropi tina dua tahapan kajadian éta.', 'su(Catetan: Buku nu ditulis ku Shannon/Wéaver ngarujuk ka tulisanTolman(1938) anu ngarujuk ogé kaPauli(1933) anu nuliskeun définisi éntropi nu dipaké ku Shannon.', \"suSalian ti éta, mekanika statistika ngarujuk ogé kavon Neumannsanggeus manéhna nurunkeun éntropi dina taun1927, anu bisa ngajelaskeun naha von Neumann resep pisan ngagunakeun istilah 'entropi' anu geus aya.\", 'su)Tumbu luar[édit|édit sumber]Information is not entropy, information is not uncertainty !- a discussion of the use of the terms \"information\" and \"entropy\".I\\'m Confused: How Could Information Equal Entropy?Archived2011-07-17 diWayback Machine- a similar discussion on the bionet.info-théory FAQ.Java \"entropy pool\" for cryptographically-secure unguessable random numbersDescription of information entropy from \"Tools for Thought\" by Howard RheingoldArchived2011-05-15 diWayback MachineA cool little java applet representing Shannon\\'s Experiment to Calculate the Entropy of EnglishArtikelieu mangrupataratas, perlu disampurnakeun.', 'suUpami sadérék uninga langkung paos perkawis ieu, dihaturan kanggongalengkepan.Dicomot ti \"https://su.wikipedia.org/w/index.php?title=Éntropi_informasi&oldid=652556\"Kategori:Webarchive template wayback linksÉntropiTéori informasiStatistikaAcakKategori nyumput:TaratasKaca ieu panungtungan diédit 9 April 2023, jam 21.25.Téks sadia dinaLisénsi Creative Commons Attribution-ShareAlike; katangtuan tambahan kamungkinan lumaku.', 'suTémbongKatangtuan pamakéanpikeun detailna.Kawijakan privasiNgeunaan WikipediaBantahanKode EtikPamekarStatistikPernyataan réréméhPidangan sélulérGulingkan lebar konten terbatas', 'daEntropi (informationsteori) - Wikipedia, den frie encyklopædiSpring til indholdHovedmenuHovedmenuflyt til sidebjælkenskjulNavigationForsideKategorierFremhævet indholdTilfældig sideTilfældige artiklerAktueltdeltagelseVelkommenSkribentforsideLandsbybrøndenProjekterPortalerØnskede artiklerOprydningKalenderSeneste ændringerHjælpSøgSøgOpret kontoLog påPersonlige værktøjerOpret kontoLog påSider for redaktører som er logget udlær mereBidragDiskussion[Luk]Du kan være med til at gøre Wikipedia bedre –læs her hvordan!Dansk Wikipedia har en Discord-server, hvor du kan chatte.', 'daSe mere påWikipedia:Discord(Læs her om sitenotice)Indholdflyt til sidebjælkenskjulStarten1Simpelt eksempelVis/skjul underafsnit Simpelt eksempel1.1Møntkast1.2Bernoulli-proces2KildehenvisningerVis/skjul indholdsfortegnelsenEntropi (informationsteori)45 sprogAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Redigér linksArtikelDiskussiondanskLæsRedigérRediger kildekodeSe historikVærktøjerVærktøjerflyt til sidebjælkenskjulHandlingerLæsRedigérRediger kildekodeSe historikGenereltHvad henviser hertilBeslægtede ændringerUpload filSpecialsiderPermanent linkSideinformationReferer til denne sideHent forkortet URLDownload QR codeWikidata-elementOrganisationDonationKontakt WikipediaWikimedia DanmarkGLAMUdskriv/eksportérLav en bogDownload som PDFUdskriftsvenlig udgaveI andre projekterWikimedia CommonsFra Wikipedia, den frie encyklopædiFor andre betydninger, seEntropi (flertydig)Der er for få eller ingenkildehenvisningeri denne artikel,hvilket er et problem.', 'daDu kan hjælpe ved at angivetroværdige kildertil de påstande, som fremføres i artiklen.Iinformationsteorierentropi(ogsåinformationsentropiellerShannon-entropi) en måde at betegne og give værdi til evolution og vækst i viden.', 'daIsærKI-applikationergør brug af entropi til at læseinformationer.', 'daDe sammenligner simpelthen systemets dele og vælger det stykkedatamed mindst (~0) entropi.EntropienSer givet ved en sum over alle mulige tilstande:S=−∑iPilog2\\u2061PiP_\\\\log _P_}hvorPi}ersandsynlighedenfor tilstandeni.', 'da[1]Entropien opnås være at tage gennemsnittet af informationsmængden for hvert udfald:Ii=−log2\\u2061Pi=-\\\\log _P_}For et system med forskellige udfaldier entropien altså den gennemsnitligeinformationsmængde, der opnås ved en måling.', 'daJo højere entropien er, jo større usikkerhed er der omkring udfaldet.', \"da[2]Inden forfysikkenkaldes den tilsvarende ligning forGibbs' entropiformel.\", 'da[3]Simpelt eksempel[redigér|rediger kildetekst]I det følgende gives eksempler på beregning af entropi.Møntkast[redigér|rediger kildetekst]To bit entropi: For to ærlige møntkast er der 4 mulige udfald, og informationsentropien er to-tals-logaritmen til 4, hvilket giver 2 bit.', 'daForNmøntkast er entropienNbit.Når enærligmøntbruges til at slåplat eller krone, har den 50 % - dvs.12}}- sandsynlighed for at lande på krone og 50 % sandsynlighed for at lande på plat.', 'daInformationsmængden for hver udfald er derfor:I=−log2\\u206112=log2\\u20612=1bit}=\\\\log _2=1}}Den gennemsnitlige informationsmængde - entropien - for ét mønstkast er derfor også 1:S1=−[12log2\\u2061(12)+12log2\\u2061(12)]=−log2\\u206112=log2\\u20612=1bit=-\\\\left[}\\\\log _\\\\left(}\\\\right)+}\\\\log _\\\\left(}\\\\right)\\\\right]=-\\\\log _}=\\\\log _2=1}}For to mønter fordobles informationsmængden ,og derfor bliver entropien 2.', 'daDer er nemlig 4 mulige udfald med to mønter, og hvert udfald har 25 % sandsynlighed, så:S2=−4[14log2\\u2061(14)]=log2\\u20614=log2\\u206122=2log2\\u20612=2bit=-4\\\\left[}\\\\log _\\\\left(}\\\\right)\\\\right]=\\\\log _4=\\\\log _2^=2\\\\log _2=2}}Da antallet af mulige udfald fordobles med hver mønt, må antallet af mulige udfald for et arbitrært antalNmønter være2N}.', 'daSandsynligheden per udfaldier derfor:Pi=12N=}}}Og derfor er entropien:SN=−2N[12Nlog2\\u2061(12N)]=log2\\u2061(2N)=Nlog2\\u20612=-2^\\\\left[}}\\\\log _\\\\left(}}\\\\right)\\\\right]=\\\\log _\\\\left(2^\\\\right)=N\\\\log _2}Entropien forNmøntkast er altså simpelthenN.SN=Nbit=N}}Så jo flere mønter, jo højere entropi, da hvert udfald bliver mere og mere usandsynligt, og informationen omvendt bliver større og større.Bernoulli-proces[redigér|rediger kildetekst]Entropi som funktion af sandsynligheden for udfald 1.', 'daFor en Bernoulli-proces er entropien maksimal, når begge udfald er lige sandsynlige, mens entropien er nul, når kun ét af udfaldene er muligt.', 'da[2]EnBernoulli-proceser en måling, hvor der er to mulige udfald med sandsynlighedernepog1−pP1=pP2=1−pP_&=p\\\\\\\\P_&=1-p\\\\end}}hvorper konstant.', 'daDette er en generalisering af den ærlige mønt, hvorp=12}}.', 'daEntropien er:S(p)=−plog2\\u2061(p)−(1−p)log2\\u2061(1−p)(p)-(1-p)\\\\log _(1-p)}Forp=12}}er entropien 1 som før, men forp=0- dvs.', 'dahvis udfald 1 er umuligt - bliver entropien:S(0)=−0log2\\u2061(0)−1log2\\u2061(1)=0bit(0)-1\\\\log _(1)=0}}Entropien ville også være 0 bit, hvis kun udfald 2 var muligt.', 'daHvis kun ét udfald er muligt, er der ikke længere nogen usikkerhed, mens usikkerheden er størst, hvis begge udfald er lige sandsynlige (se figur).', 'da[2]Kildehenvisninger[redigér|rediger kildetekst]^Pathria, R. K.; Beale, Paul (2011).Statistical Mechanics (Third Edition).', 'daAcademic Press.', 'das. 51.ISBN978-0123821881.Arkiveretfra originalen 17. juni 2020.', 'daHentet 16. december 2019.^abcBlundell, Stephen J.; Blundell, Katherine M. (2006).', 'da15.1 Information and Shannon entropy.Concepts in Thermal Physics(engelsk) (1. udgave).', 'daOxford University Press.', 'das.153-155.ISBN978-0-19-856770-7.^Blundell, Stephen J.; Blundell, Katherine M. (2006).', 'da14.8 Entropy and probability.Concepts in Thermal Physics(engelsk) (1. udgave).', 'daOxford University Press.', 'das.146-148.ISBN978-0-19-856770-7.SpireDenne artikel ommatematiker enspiresom bør udbygges.', 'daDu er velkommen til athjælpeWikipedia ved atudvide den.AutoritetsdataLCCN:sh85044152GND:4743861-7BNF:cb11985913j(data)NDL:01191172NKC:ph425914BNE:XX535116Hentet fra \"https://da.wikipedia.org/w/index.php?title=Entropi_(informationsteori)&oldid=11217751\"Kategori:InformationsteoriSkjulte kategorier:CS1: Kilder på engelsk (en)Kilder mangler (samlet liste)Kilder mangler siden januar 2020Påbegyndte artikler om matematikPåbegyndte artikler (samlet liste)Wikipedia artikler med LCCN autoritetsdata-IDWikipedia artikler med GND autoritetsdata-IDWikipedia artikler med BNF autoritetsdata-IDWikipedia artikler med NDL autoritetsdata-IDDenne side blev senest ændret den 2. august 2022 kl.', 'da09:54.Tekst er tilgængelig underCreative Commons Navngivelse/Del på samme vilkår 4.0; yderligere betingelser kan være gældende.', 'daSebrugsbetingelsernefor flere oplysninger.PrivatlivspolitikOm WikipediaForbeholdCode of ConductUdviklereStatistikCookie-erklæringMobilvisningToggle limited content width', 'sqEntropia e informacionit - WikipediaJump to contentMain menuMain menumove to sidebarfshiheLëvizjeShkruaj një artikullArtikull i rastitNdryshimet më të funditDhuroniPërmbajtjaArtikuj të përkryerArtikuj të mirëKomunitetiKuvendiPyetje e PërgjigjeZyra e AnkesaveAdministrataOfiçinaForumi i GrisjesAmbasadatÇmimetLivadhiKërkoKërkoKrijo llogariHyniMjete vetjakeContributeKrijo llogariHyniPages for logged out editorslearn moreDiskutimi[fshihe]Nga 21 marsi deri më 31 maj, Wikimedia CEE Spring zhvillon fushatën masiveWikimedia CEE Spring 2024.', 'sqBashkohuni edhe ju të shkruajmë e të përkthejmë artikuj për kulturën e traditat të Evropës Qendrore-Lindore për të rritur bashkëpunimin në Wikipedia.GARA E FOTOGRAFISË SË USHQIMIT TRADICIONAL SHQIPTARJu pëlqen ushqimi?', 'sqKjo është gara për ju!', 'sq3 më të mirët shpërblehen.Contentsmove to sidebarfshiheFillim1E ç\\'është entropia e informacionit?2ShembullToggle the table of contentsEntropia e informacionit45 gjuhëAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Përpunoni lidhjeArtikulliDiskutimshqipLexoRedaktoRedakto nëpërmjet koditShihni historikunMjeteMjetemove to sidebarfshiheActionsLexoRedaktoRedakto nëpërmjet koditShihni historikunTë përgjithshmeLidhjet këtuNdryshime të ndërvaruraNgarkoni materiale multimedialeFaqet e veçantaLidhja e përhershmeInformacioni i faqesCito artikullinGet shortened URLDownload QR codeObjekt WikidataPërpunoni lidhje ndërgjuhësoreShtyp/eksportoKrijo një libërShkarkoje si PDFVersion shtypiNë projekte të tjeraWikimedia CommonsNga Wikipedia, enciklopedia e lirëNë teorinë e informacionit,entropiae njëndryshoreje të rastitështë niveli mesatar i \"informacionit\", \"befasisë\" ose \"pasigurisë\" i natyrshëm për rezultatet e mundshme të ndryshores.', 'sqJepet një ndryshore e rastit diskreteX, e cila merr vlera në bashkësinëX}}dhe shpërndahet sipasp:X→[0,1]}\\\\to [0,1]}:H(X):=−∑x∈Xp(x)log\\u2061p(x)=E[−log\\u2061p(X)], (X):=-\\\\sum _}}p(x)\\\\log p(x)=\\\\mathbb [-\\\\log p(X)],}kuΣtregon shumën mbi vlerat e mundshme të ndryshores.', 'sqZgjedhja e bazës përlog,logaritmi, ndryshon për zbatime të ndryshme.', 'sqBaza 2 jep njësinë ebiteve(ose \" shannons \"), ndërsa bazaejep \"njësi natyrore\" nat, dhe baza 10 jep njësi \"dits\", \"bans\" ose \" hartleys \".', 'sqNjë përkufizim i njëvlershëm i entropisë ështëvlera e priture vetë-informimit të një ndryshoreje.', 'sq[1]Dy bite entropie: Në rastin e dy hedhjeve të ndershme të monedhave, entropia e informacionit në bit është logaritmi bazë-2 i numrit të rezultateve të mundshme; me dy monedha ka katër rezultate të mundshme dhe dy pjesë entropie.', 'sqNë përgjithësi, entropia e informacionit është sasia mesatare e informacionit të përcjellë nga një ngjarje, kur merren parasysh të gjitha rezultatet e mundshme.Koncepti i entropisë së informacionit u prezantua nga Claude Shannon në punimin e tij të vitit 1948 \" A Mathematical Teory of Communication \",[2][3]dhe referohet gjithashtu sientropia Shannon.Entropia në teorinë e informacionit është drejtpërdrejt analoge me entropinë nëtermodinamikën statistikore.', 'sqAnalogjia rezulton kur vlerat e ndryshores së rastësishme përcaktojnë energjitë e mikrogjendjeve, kështu që formula e Gibbs-it për entropinë është zyrtarisht identike me formulën e Shannon-it.', 'sqEntropia ka lidhje me fusha të tjera të matematikës sikombinatorikadhemësimi makinerik.', 'sqPërkufizimi mund të rrjedhë nga një grupaksiomashqë vërtetojnë se entropia duhet të jetë një masë se sa informative është rezultati mesatar i një ndryshoreje.', \"sqPër një ndryshore të rastit të vazhdueshme, entropia diferenciale është analoge me entropinë.E ç'është entropia e informacionit?\", 'sq[Redakto|Redakto nëpërmjet kodit]I emëruar sipas teoremës Η të Boltzmann-it, Shannon përcaktoi entropinëH(gërma e madhe greke eta ) të njëndryshoreje diskrete tërastit.X, e cila merr vlera në alfabetX}}dhe shpërndahet sipasp:X→[0,1]}\\\\to [0,1]}sikursep(x):=P[X=x] [X=x]}:H(X)=E[I\\u2061(X)]=E[−log\\u2061p(X)].', 'sq(X)=\\\\mathbb [\\\\operatorname (X)]=\\\\mathbb [-\\\\log p(X)].', 'sq}KëtuE }ështëoperatori i pritjes matematike, dhe I është përmbajtja e informacionit tëX[4]:11[5]:19–20I\\u2061(X) (X)}është në vetvete një ndryshore e rastit.Entropia mund të shkruhet shprehimisht si:H(X)=−∑x∈Xp(x)logb\\u2061p(x), (X)=-\\\\sum _}}p(x)\\\\log _p(x),}Mund të përcaktohet gjithashtu entropia e kushtëzuar e dy ndryshoreveXdheYduke marrë vlera nga bashkësitëX}}dheY}}përkatësisht, si:[6]:16H(X|Y)=−∑x,y∈X×YpX,Y(x,y)log\\u2061pX,Y(x,y)pY(y), (X|Y)=-\\\\sum _}\\\\times }}p_(x,y)\\\\log (x,y)}(y)}},}kupX,Y(x,y):=P[X=x,Y=y](x,y):=\\\\mathbb [X=x,Y=y]}dhepY(y)=P[Y=y](y)=\\\\mathbb [Y=y]}.', 'sqKjo madhësi duhet të kuptohet si rastësia e mbetur në ndryshoren e rastitXduke pasur parasysh ndryshoren e rastitY.Shembull[Redakto|Redakto nëpërmjet kodit]EntropiaH\\u2061(X) (X)}(dmth surpriza epritshme) e një rrotullimi të monedhës, e matur në bit, e grafikuar kundrejt paragjykimit të monedhësPr\\u2061(X=1) (X=1)}, ku X = 1 përfaqëson një rezultat të kokave.', 'sq[6]:14–15Këtu, entropia është më së shumti 1 bit dhe për të komunikuar rezultatin e një rrokullisjeje monedhe (2 vlera të mundshme) do të kërkojë një mesatare prej më së shumti 1 bit (saktësisht 1 bit për një monedhë të drejtë).', 'sqRezultati i një die të drejtë (6 vlera të mundshme) do të kishte regjistrin e entropisë26 bit.Merrni parasysh hedhjen e një monedhe me probabilitete të njohura, jo domosdoshmërisht të ndershme, për të dalë kokë ose pil; ky mund të modelohet si një proces Bernoulli .Entropia e rezultatit të panjohur të hedhjes tjetër të monedhës maksimizohet nëse monedha është e ndershme (d.m.th., nëse koka dhe pili kanë të dyja probabilitet të barabartë 1/2).', 'sqKjo është situata e pasigurisë maksimale pasi është më e vështirë të parashikohet rezultati i hedhjes së radhës; rezultati i çdo hedhjeje të monedhës jep një pjesë të plotë të informacionit.', 'sqKjo është për shkak seH(X)=−∑i=1np(xi)logb\\u2061p(xi)=−∑i=1212log2\\u206112=−∑i=1212⋅(−1)=1\\\\mathrm (X)&=-\\\\sum _^)\\\\log _p(x_)}\\\\\\\\&=-\\\\sum _^}\\\\log _}}\\\\\\\\&=-\\\\sum _^}\\\\cdot (-1)}=1\\\\end}}Megjithatë, nëse e dimë se monedha nuk është e drejtë, por del lart ose bisht me probabilitete p dhe q, ku p ≠ q, atëherë ka më pak pasiguri.', 'sqSa herë që hidhet, njëra anë ka më shumë gjasa të dalë lart se tjetra.', 'sqPasiguria e reduktuar matet në një entropi më të ulët: mesatarisht çdo hedhje e monedhës jep më pak se një pjesë të plotë të informacionit.', 'sqPër shembull, nëse p = 0.7, atëherëH(X)=−plog2\\u2061(p)−qlog2\\u2061(q)=−0.7log2\\u2061(0.7)−0.3log2\\u2061(0.3)≈−0.7⋅(−0.515)−0.3⋅(−1.737)=0.8816<1\\\\mathrm (X)&=-p\\\\log _(p)-q\\\\log _(q)\\\\\\\\&=-0.7\\\\log _(0.7)-0.3\\\\log _(0.3)\\\\\\\\&\\\\approx -0.7\\\\cdot (-0.515)-0.3\\\\cdot (-1.737)\\\\\\\\&=0.8816<1\\\\end}}^Pathria, R. K.; Beale, Paul (2011).Statistical Mechanics(bot.', 'sqThird).', 'sqAcademic Press.', 'sqfq.', 'sq51.ISBN978-0123821881.', 'sq}:Mungon ose është bosh parametri|language=(Ndihmë!', 'sq)^Shannon, Claude E.(korrik 1948).', 'sqA Mathematical Theory of Communication.Bell System Technical Journal.27(3): 379–423.doi:10.1002/j.1538-7305.1948.tb01338.x.', 'sq}:|hdl-access=ka nevojë për|hdl=(Ndihmë!', 'sq);Mungon ose është bosh parametri|language=(Ndihmë!', 'sq)(PDF, archived fromhere)^Shannon, Claude E.(tetor 1948).', 'sqA Mathematical Theory of Communication.Bell System Technical Journal.27(4): 623–656.doi:10.1002/j.1538-7305.1948.tb00917.x.', 'sq}:|hdl-access=ka nevojë për|hdl=(Ndihmë!', 'sq);Mungon ose është bosh parametri|language=(Ndihmë!', 'sq)(PDF, archived fromhere)^Borda, Monica (2011).Fundamentals in Information Theory and Coding.', 'sqSpringer.ISBN978-3-642-20346-6.', 'sq}:Mungon ose është bosh parametri|language=(Ndihmë!', 'sq)^Han, Te Sun; Kobayashi, Kingo (2002).Mathematics of Information and Coding.', 'sqAmerican Mathematical Society.ISBN978-0-8218-4256-0.', 'sq}:Mungon ose është bosh parametri|language=(Ndihmë!', 'sq)^abThomas M. Cover; Joy A. Thomas (1991).Elements of Information Theory.', 'sqHoboken, New Jersey: Wiley.ISBN978-0-471-24195-9.', 'sq}:Mungon ose është bosh parametri|language=(Ndihmë!', 'sq)Gabim referencash: Invalid<ref>tag; name \"cover1991\" defined multiple times with different contentMarrë nga \"https://sq.wikipedia.org/w/index.php?title=Entropia_e_informacionit&oldid=2608375\"Kategoritë:Gabime CS1: Mungon parametri i gjuhësGabime CS1: parametri i aksesimitKategori e fshehur:Pages with reference errorsKjo faqe është redaktuar për herë te fundit më 1 nëntor 2023, në orën 19:37.Të gjitha materialet që gjenden në këtë faqë janë të mbrojtura ngaCreative Commons Attribution/Share-Alike License;.', 'sqShikoTerms of Usepër më shumë informacione.Rreth të dhënave vetjakeRreth Wikipedia-sShfajësimetCode of ConductProgramuesitStatistikatDeklarata e cookiesPër celularToggle limited content width', 'bsEntropija (teorija informacija) - WikipediaIdi na sadržajGlavni meniGlavni menipremjesti na bočnu trakusakrijNavigacijaPočetna stranaIstaknuti članciPortaliKategorijeNedavne izmjeneNasumična stranicaInterakcijaPomoćIgrališteVrata zajedniceČaršijaNovostiDonacijePretragaTražiNapravi korisnički računPrijavi meLični alatiNapravi korisnički računPrijavi meStranice za odjavljene urednikedetaljnijeDoprinosiRazgovor[zatvori]Od 21. marta do 31. maja sudjelujte u projektuCEE proljeće 2024.Sadržajpremjesti na bočnu trakusakrijPočetak1Također pogledajte2Vanjski linkoviUključi/isključi sadržajEntropija (teorija informacija)45 jezikaAfrikaansالعربيةBoarischБългарскиCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Uredi vezeStranicaRazgovorbosanskiČitajUrediUredi izvorHistorijaAlatiAlatipremjesti na bočnu trakusakrijRadnjeČitajUrediUredi izvorHistorijaOpćenitoŠta vodi ovamoSrodne izmjenePostavi datotekuPosebne straniceTrajni linkInformacije o straniciCitiraj ovu stranicuSkraćeni linkPreuzmi QR kodStavka na WikipodacimaŠtampanje / izvozNapravi knjiguPreuzmi kao PDFZa štampanjeNa drugim projektimaWikimedia CommonsS Wikipedije, slobodne enciklopedijeOvaj članak ili neki od njegovih odlomaka nije dovoljno potkrijepljenizvorima(literatura, veb-sajtovi ili drugi izvori).Ako se pravilno ne potkrijepepouzdanimizvorima, sporne rečenice i navodi mogli bi biti izbrisani.', 'bsPomozite Wikipediji tako što ćete navesti validne izvore putemreferencite nakon toga možete ukloniti ovaj šablon.Ovom članku potrebna je jezička standardizacija, preuređivanje ili reorganizacija.Pogledajtekako poboljšatičlanak, kliknite na linkuredii doradite članak vodeći računa ojezičkimistilskim standardimaWikipedije.', 'bsAko niste sigurni kako bi članak trebao izgledati, pogledajte neke oddobrih članaka.U teorijiinformacije, entropija je mera neodređenosti pridružena slučajnoj promjenljivoj.', 'bsU ovom kontekstu, obično se misli na Shannonovu entropiju, koja kvantifikuje očekivanu vrijednost informacije sadržane u poruci, obično u jedinicama kao što su biti.', 'bsEkvivalentno, Shannonova entropija je mjera prosječnog informacionog sadržaja koji se propušta kada se ne zna vrijednost slučajne promjenljive.', 'bsKoncept je uveo Claude Shannon u svom čuvenom radu iz 1948. godine „Matematička teorija komunikacija“.Shannonova entropija predstavlja apsolutnu granicu najbolje moguće kompresije bez gubitaka bilo kakve komunikacije, pod izvjesnim ograničenjima: ako tretiramo poruke da su kodirane kao niz nezavisnih slučajnih promjenljivih sa istom raspodjelom, prva Shannonova teorema pokazuje da, u graničnom slučaju, srednja dužina najkraće moguće reprezentacije za kodiranje poruka u datom alfabetu je njihova entropija podijeljena sa logaritmom broja simbola u ciljnom alfabetu.Jedno bacanje fer novčića nosi entropiju od jednog bita.', 'bsDva bacanja - dva bita.', 'bsBrzina entropije za novčić je jedan bit po bacanju.', 'bsMeđutim, ako novčić nije fer, tada je neodređenost manja (ako bismo morali da se kladimo na ishod narednog pokušaja, vjerovatno ćemo se kladiti na češći rezultat), pa je i Shannonova entropija manja.', 'bsMatematički, jedno bacanje novčića (fer ili ne) predstavlja Bernoulijev eksperiment, a njegova entropija data je binarnom entropijskom funkcijom.', 'bsNiz bacanja novčića sa dvije glave imaće nultu entropiju, jer su ishodi potpuno predvidljivi.', 'bsBrzina entropije teksta na engleskom je od 1 do 1,5 bita po slovu, odnosno od 0,6 do 1,3 bita po slovu, prema procjenama baziranim na eksperimentima.Također pogledajte[uredi|uredi izvor]Kompresija podatakaVanjski linkovi[uredi|uredi izvor]Uvod u entropiju i informacijunaPrincipia Cybernetica WebEntropyinterdisciplinarni časopis o svim aspektima koncepta entropije.', 'bsSlobodan pristup.Information is not entropy, information is not uncertainty !– diskusija o korištenju izraza \"informacija\" i \"entropija\".I\\'m Confused: How Could Information Equal Entropy?– a similar discussion on the bionet.info-theory FAQ.Description of information entropy from \"Tools for Thought\" by Howard RheingoldA java applet representing Shannon\\'s Experiment to Calculate the Entropy of EnglishSlides on information gain and entropyAn Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science– a wikibook on the interpretation of the concept of entropy.Calculator for Shannon entropy estimation and interpretationNetwork Event Detection With Entropy Measures, Dr. Raimund Eimann, University of Auckland, PDF; 5993 kB – a PhD thesis demonstrating how entropy measures may be used in network anomaly detection.Preuzeto iz \"https://bs.wikipedia.org/w/index.php?title=Entropija_(teorija_informacija)&oldid=3022817\"Kategorija:InformacijaSakrivene kategorije:Članci koji trebaju izvorPotrebno preuređivanjeOva stranica je posljednji put izmijenjena na datum 16 oktobar 2019 u 00:08.Tekst je dostupan podslobodnom licencom Autorstvo-Dijeliti pod istim uvjetima; mogu se primijeniti i dodatni uvjeti.', 'bsKorištenjem ovog sajta slažete se suvjetima korištenjaipravilima o privatnosti.', 'bsWikipedia® je zaštitni znak neprofitne organizacijeWikimedia Foundation, Inc.Pravila o privatnostiO WikipedijiOdricanje odgovornostiKodeks ponašanjaRazvojni programeriStatistikaIzjava o kolačićimaMobilni prikazUključi/isključi ograničenu širinu sadržaja', 'zh熵 (信息论) - 维基百科，自由的百科全书跳转到内容主菜单主菜单移至侧栏隐藏导航首页分类索引特色内容新闻动态最近更改随机条目资助维基百科帮助帮助维基社群方针与指引互助客栈知识问答字词转换IRC即时聊天联络我们关于维基百科搜索搜索创建账号登录个人工具创建账号登录未登录编辑者的页面了解详情贡献讨论目录移至侧栏隐藏序言1简介开关简介子章节1.1熵的计算2定义3範例4熵的特性开关熵的特性子章节4.1连续性4.2对称性4.3极值性4.4可加性5进一步性质6和热力学熵的联系7逸闻8参见9参考10外部链接开关目录熵 (信息论)45种语言AfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt粵語编辑链接条目讨论不转换不转换简体繁體大陆简体香港繁體澳門繁體大马简体新加坡简体臺灣正體阅读编辑查看历史工具工具移至侧栏隐藏操作阅读编辑查看历史常规链入页面相关更改上传文件特殊页面固定链接页面信息引用本页获取短URL下载二维码维基数据项目打印/导出下载为PDF打印页面在其他项目中维基共享资源维基百科，自由的百科全书此條目需要补充更多来源。(2018年2月25日)请协助補充多方面可靠来源以改善这篇条目，无法查证的内容可能會因為异议提出而被移除。致使用者：请搜索一下条目的标题（来源搜索：\"熵 (信息论)\"—网页、新闻、书籍、学术、图像），以检查网络上是否存在该主题的更多可靠来源（判定指引）。此條目可参照英語維基百科相應條目来扩充。若您熟悉来源语言和主题，请协助参考外语维基百科扩充条目。请勿直接提交机械翻译，也不要翻译不可靠、低品质内容。依版权协议，译文需在编辑摘要注明来源，或于讨论页顶部标记}标签。此条目页的主題是信息论的熵。关于「熵」的其他意思，請見「熵 (消歧義)」。2 bit的熵。在信息论中，熵（英語：entropy）是接收的每条消息中包含的信息的平均量，又被稱為信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。（熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。）来自信源的另一个特征是样本的概率分布。这里的想法是，比较不可能发生的事情，当它发生了，会提供更多的信息。由于一些其他的原因，把信息（熵）定义为概率分布的对数的相反数是有道理的。事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的均值（即期望）就是这个分布产生的信息量的平均值（即熵）。熵的单位通常为比特，但也用Sh、nat、Hart计量，取决于定义用到对数的底。采用概率分布的对数作为信息的量度的原因是其可加性。例如，投掷一次硬币提供了1 Sh的信息，而掷m次就为m位。更一般地，你需要用log2(n)位来表示一个可以取n个值的变量。在1948年，克劳德·艾尔伍德·香农將熱力學的熵，引入到信息论，因此它又被稱為香农熵（Shannon entropy）[1][2]。简介[编辑]熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在信息论里面，熵是对不确定性的测量。但是在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。英语文本数据流的熵比较低，因为英语很容易读懂，也就是说很容易被预测。即便我们不知道下一段英语文字是什么内容，但是我们能很容易地预测，比如，字母e总是比字母z多，或者qu字母组合的可能性总是超过q与任何其它字母的组合。如果未经压缩，一段英文文本的每个字母需要8个比特来编码，但是实际上英文文本的熵大概只有4.7比特。這是由於英文的編碼包含了各式符號，如逗號、引號等。因此英文輸入法使用了8個位元來表達一共256個字母及符號。如果压缩是无损的，即通过解压缩可以百分之百地恢复初始的消息内容，那么压缩后的消息携带的信息和未压缩的原始消息是一样的多。而压缩后的消息可以通过较少的比特传递，因此压缩消息的每个比特能携带更多的信息，也就是说压缩信息的熵更加高。熵更高意味着比较难于预测压缩消息携带的信息，原因在于压缩消息里面没有冗余，即每个比特的消息携带了一个比特的信息。香农的信源编码定理揭示了，任何无损压缩技术不可能让一比特的消息携带超过一比特的信息。消息的熵乘以消息的长度决定了消息可以携带多少信息。香农的信源编码定理同时揭示了，任何无损压缩技术不可能缩短任何消息。根据鸽笼原理，如果有一些消息变短，则至少有一条消息变长。在实际使用中，由于我们通常只关注于压缩特定的某一类消息，所以这通常不是问题。例如英语文档和随机文字，数字照片和噪音，都是不同类型的。所以如果一个压缩算法会将某些不太可能出现的，或者非目标类型的消息变得更大，通常是无关紧要的。但是，在我们的日常使用中，如果去压缩已经压缩过的数据，仍会出现问题。例如，将一个已经是FLAC格式的音乐文件压缩为ZIP文件很难使它占用的空间变小。熵的计算[编辑]如果有一枚理想的硬币，其出现正面和反面的机会相等，则抛硬币事件的熵等于其能够达到的最大值。我们无法知道下一个硬币抛掷的结果是什么，因此每一次抛硬币都是不可预测的。因此，使用一枚正常硬币进行若干次抛掷，这个事件的熵是一比特，因为结果不外乎两个——正面或者反面，可以表示为0, 1编码，而且两个结果彼此之间相互独立。若进行n次独立实验，则熵为n，因为可以用长度为n的比特流表示。[3]但是如果一枚硬币的两面完全相同，那个这个系列抛硬币事件的熵等于零，因为结果能被准确预测。现实世界里，我们收集到的数据的熵介于上面两种情况之间。另一个稍微复杂的例子是假设一个随机变量X，取三种可能值x1,x2,x3x_,x_,x_\\\\end}}，概率分别为12,14,14},},}\\\\end}}，那么编码平均比特长度是：12×1+14×2+14×2=32}\\\\times 1+}\\\\times 2+}\\\\times 2=}\\\\end}}。其熵为3/2。因此熵实际是对随机变量的比特量和顺次发生概率相乘再总和的数学期望。定义[编辑]依据Boltzmann\\'s H-theorem，香农把随机变量X的熵值 Η（希腊字母Eta）定义如下，其值域为：H(X)=E[I(X)]=E[−ln\\u2061(P(X))] (X)=\\\\mathrm [\\\\mathrm (X)]=\\\\mathrm [-\\\\ln(\\\\mathrm (X))]}。其中，P为X的機率質量函數（probability mass function），E为期望函數，而I(X)是X的資訊量（又稱為資訊本體）。I(X)本身是個隨機變數。当取自有限的样本时，熵的公式可以表示為：H(X)=∑iP(xi)I(xi)=−∑iP(xi)logb\\u2061P(xi), (X)=\\\\sum _ (x_)\\\\,\\\\mathrm (x_)}=-\\\\sum _ (x_)\\\\log _\\\\mathrm (x_)},}在這裏b是對數所使用的底，通常是2，自然常數e，或是10。當b= 2，熵的單位是bit；當b= e，熵的單位是nat；而當b= 10,熵的單位是Hart。pi= 0时，对於一些i值，对应的被加数0 logb0的值将会是0，这与极限一致。limp→0+plog\\u2061p=0p\\\\log p=0}。还可以定义事件X与Y分别取xi和yj时的条件熵为H(X|Y)=−∑i,jp(xi,yj)log\\u2061p(xi,yj)p(yj) (X|Y)=-\\\\sum _p(x_,y_)\\\\log ,y_)})}}}其中p(xi,yj)为X=xi且Y=yj时的概率。这个量应当理解为你知道Y的值前提下随机变量X的随机性的量。範例[编辑]抛硬币的熵H(X)（即期望自信息），以位元度量，與之相對的是硬幣的公正度Pr(X=1).注意图的最大值取决於分布；在這裡，要傳達一個公正的拋硬幣結果至多需要1位元，但要傳達一個公正的拋骰子結果至多需要log2(6)位元。如果有一个系统S内存在多个事件S = ，每个事件的機率分布P = ，则每个事件本身的訊息（資訊本體）为：Ie=−log2\\u2061pi=-\\\\log _}}（对数以2为底，单位是位元（bit））Ie=−ln\\u2061pi=-\\\\ln }}（对数以e为底，单位是纳特/nats）如英语有26个字母，假如每个字母在文章中出现次数平均的话，每个字母的訊息量为：Ie=−log2\\u2061126=4.7=-\\\\log _=4.7}以日文五十音平假名作為相對範例，假設每個平假名日語文字在文章中出現的機率相等，每個平假名日語文字可攜帶的資訊量為：Ie=−log2\\u2061150=5.64=-\\\\log _=5.64}而汉字常用的有2500个，假如每个汉字在文章中出现次数平均的话，每个汉字的信息量为：Ie=−log2\\u206112500=11.3=-\\\\log _=11.3}实际上每个字母和每个汉字在文章中出现的次数并不平均，比方说较少见字母（如z）和罕用汉字就具有相对高的信息量。但上述计算提供了以下概念：使用书写单元越多的文字，每个单元所包含的訊息量越大。熵是整个系统的平均消息量，即：Hs=∑i=1npiIe=−∑i=1npilog2\\u2061pi=\\\\sum _^p_I_=-\\\\sum _^p_\\\\log _p_}因为和热力学中描述热力学熵的玻尔兹曼公式本质相同（仅仅单位不同，一纳特的信息量即相当于k焦耳每开尔文的热力学熵），所以也称为“熵”。如果两个系统具有同样大的消息量，如一篇用不同文字写的同一文章，由于汉字的信息量较大，中文文章应用的汉字就比英文文章使用的字母要少。所以汉字印刷的文章要比其他应用总体数量少的字母印刷的文章要短。即使一个汉字占用两个字母的空间，汉字印刷的文章也要比英文字母印刷的用纸少。熵的特性[编辑]可以用很少的标准来描述香农熵的特性，将在下面列出。任何满足这些假设的熵的定义均正比以下形式−K∑i=1npilog\\u2061(pi)^p_\\\\log(p_)}其中，K是与选择的度量单位相对应的一个正比常数。下文中，pi= Pr(X=xi)且Hn(p1,…,pn)=H(X) _(p_,\\\\ldots ,p_)=\\\\mathrm (X)}连续性[编辑]该量度应连续，概率值小幅变化只能引起熵的微小变化。对称性[编辑]符号xi重新排序后，该量度应不变。Hn(p1,p2,…)=Hn(p2,p1,…) _\\\\left(p_,p_,\\\\ldots \\\\right)=\\\\mathrm _\\\\left(p_,p_,\\\\ldots \\\\right)}等。极值性[编辑]当所有符号有同等機會出现的情况下，熵达到最大值（所有可能的事件同等概率时不确定性最高）。Hn(p1,…,pn)≤Hn(1n,…,1n)=logb\\u2061(n) _(p_,\\\\ldots ,p_)\\\\leq \\\\mathrm _\\\\left(},\\\\ldots ,}\\\\right)=\\\\log _(n)}。等概率事件的熵应随符号的数量增加。Hn(1n,…,1n⏟n)=logb\\u2061(n)<logb\\u2061(n+1)=Hn+1(1n+1,…,1n+1⏟n+1).', 'zh_\\\\underbrace },\\\\ldots ,}} _=\\\\log _(n)<\\\\log _(n+1)=\\\\mathrm _\\\\underbrace },\\\\ldots ,}} _.', 'zh}可加性[编辑]熵的量与该过程如何被划分无关。最后给出的这个函数关系刻画了一个系统与其子系统的熵的关系。如果子系统之间的相互作用是已知的，则可以通过子系统的熵来计算一个系统的熵。给定n个均匀分布元素的集合，分为k个箱（子系统），每个里面有b1, ...,bk个元素，合起来的熵应等于系统的熵与各个箱子的熵的和，每个箱子的权重为在该箱中的概率。对于正整数bi其中b1+ ... +bk=n来说，Hn(1n,…,1n)=Hk(b1n,…,bkn)+∑i=1kbinHbi(1bi,…,1bi) _\\\\left(},\\\\ldots ,}\\\\right)=\\\\mathrm _\\\\left(}},\\\\ldots ,}}\\\\right)+\\\\sum _^}}\\\\,\\\\mathrm _}\\\\left(}},\\\\ldots ,}}\\\\right)}。选取k=n，b1= ... =bn= 1，这意味着确定符号的熵为零：Η1(1) = 0。这就是说可以用n进制熵来定义n个符号的信源符号集的效率。参见信息冗余。进一步性质[编辑]香农熵满足以下性质，藉由將熵看成「在揭示随机变量X的值後，從中得到的信息量（或消除的不确定性量）」，可來幫助理解其中一些性質。增減一概率为零的事件不改变熵：Hn+1(p1,…,pn,0)=Hn(p1,…,pn) _(p_,\\\\ldots ,p_,0)=\\\\mathrm _(p_,\\\\ldots ,p_)}可用琴生不等式证明H(X)=E\\u2061[logb\\u2061(1p(X))]≤logb\\u2061(E\\u2061[1p(X)])=logb\\u2061(n) (X)=\\\\operatorname \\\\left[\\\\log _\\\\left(}\\\\right)\\\\right]\\\\leq \\\\log _\\\\left(\\\\operatorname \\\\left[}\\\\right]\\\\right)=\\\\log _(n)}具有均匀概率分布的信源符号集可以有效地达到最大熵logb(n)：所有可能的事件是等概率的时候，不确定性最大。计算 (X,Y)得到的熵或信息量（即同时计算X和Y）等于通过进行两个连续实验得到的信息：先计算Y的值，然后在你知道Y的值条件下得出X的值。写作H(X,Y)=H(X|Y)+H(Y)=H(Y|X)+H(X) (X,Y)=\\\\mathrm (X|Y)+\\\\mathrm (Y)=\\\\mathrm (Y|X)+\\\\mathrm (X)}。如果Y=f(X)，其中f是确定性的，那么Η(f(X)|X) = 0。应用前一公式Η(X,f(X))就会产生H(X)+H(f(X)|X)=H(f(X))+H(X|f(X)), (X)+\\\\mathrm (f(X)|X)=\\\\mathrm (f(X))+\\\\mathrm (X|f(X)),}所以Η(f(X)) ≤ Η(X)，因此当后者是通过确定性函数传递时，变量的熵只能降低。如果X和Y是两个独立实验，那么知道Y的值不影响我们对X值的认知（因为两者独立，所以互不影响）：H(X|Y)=H(X) (X|Y)=\\\\mathrm (X)}。两个事件同时发生的熵不大于每个事件单独发生的熵的总和，且仅当两个事件是独立的情况下相等。更具体地说，如果X和Y是同一概率空间的两个随机变量，而(X,Y)表示它们的笛卡尔积，则H(X,Y)≤H(X)+H(Y) (X,Y)\\\\leq \\\\mathrm (X)+\\\\mathrm (Y)}。在前两条熵的性质基础上，很容易用数学证明这一点。和热力学熵的联系[编辑]物理学家和化学家对一个系统自发地从初始状态向前演进过程中，遵循热力学第二定律而发生的熵的变化更感兴趣。在传统热力学中，熵被定义为对系统的宏观测定，并没有涉及概率分布，而概率分布是信息熵的核心定义。根据Jaynes（1957）的观点，热力学熵可以被视为香农信息理论的一个应用： 热力学熵被解釋成與定義系統的微態細節所需的進一步香农資訊量成正比，波茲曼常數為比例系數，其中系統與外界無交流，只靠古典熱力學的巨觀變數所描述。加熱系統會提高其热力学熵，是因為此行為增加了符合可測巨觀變數 的系統微態的數目，也使得所有系統的的完整敘述變得更長。（假想的）麦克斯韦妖可利用每個分子的状态信息，來降低热力学熵，但是羅夫·蘭道爾（英语：Rolf Landauer）（於1961年）和及其同事則證明了，让小妖精行使职责本身——即便只是了解和储存每个分子最初的香农信息——就会给系统带来热力学熵的增加，因此总的来说，系统的熵的总量没有减少。这就解决了Maxwell思想实验引发的悖论。蘭道爾原理也為現代計算機處理大量資訊時所產生的熱量給出了下限，雖然現在計算機的廢熱遠遠比這個限制高。逸闻[编辑]贝尔实验室曾流传一则可信度不高的传闻：冯诺依曼建议香农为这个概念取名为“熵”，理由是这个热力学名词别人不懂，容易被唬住。[4]参见[编辑]熵 (生态学)熵 (熱力學)熵编码麦克斯韦妖参考[编辑]^Shannon, Claude E.A Mathematical Theory of Communication.Bell System Technical Journal.', 'zhJuly 1948,27(3): 379–423.doi:10.1002/j.1538-7305.1948.tb01338.x.hdl:10338.dmlcz/101429.', 'zh(PDF, archived fromhere（页面存档备份，存于互联网档案馆）)^Shannon, Claude E.A Mathematical Theory of Communication.Bell System Technical Journal.', 'zhOctober 1948,27(4): 623–656.doi:10.1002/j.1538-7305.1948.tb00917.x.hdl:11858/00-001M-0000-002C-4317-B.', 'zh(PDF, archived fromhere（页面存档备份，存于互联网档案馆）)^Douglas Robert Stinson; Maura Paterson.', 'zh第2.4节“熵”.Cryptography Theory and Practice[密码学理论与实践] 2.^詹姆斯·格雷克.', 'zh第9章“熵及其妖”.The Information: A History, a Theory, a Flood[信息简史].', 'zh高博 (翻译), 楼伟珊 (审校), 高学栋 (审校), 李松峰 (审校) 1.人民邮电出版社.', 'zh2013: 265.ISBN978-7-115-33180-9（中文（中国大陆））.根据在贝尔实验室里流传的一个说法，是约翰·冯·诺依曼建议香农使用这个词，因为没有人懂这个词的意思，所以他与人争论时可以无往而不利。这件事虽然子虚乌有，但听起来似乎有点道理。外部链接[编辑]Hazewinkel, Michiel (编),Entropy,数学百科全书,Springer, 2001,ISBN978-1-55608-010-4规范控制数据库国际FAST各地西班牙法国BnF data德国以色列美国日本捷克取自“https://zh.wikipedia.org/w/index.php?title=熵_(信息论)&oldid=81237456”分类：\\u200b信息论信息學熵隐藏分类：\\u200b自2018年2月需补充来源的条目拒绝当选首页新条目推荐栏目的条目需要從英語維基百科翻譯的條目含有英語的條目包含FAST标识符的维基百科条目包含BNE标识符的维基百科条目包含BNF标识符的维基百科条目包含BNFdata标识符的维基百科条目包含GND标识符的维基百科条目包含J9U标识符的维基百科条目包含LCCN标识符的维基百科条目包含NDL标识符的维基百科条目包含NKC标识符的维基百科条目本页面最后修订于2024年2月18日 (星期日) 06:26。本站的全部文字在知识共享 署名-相同方式共享 4.0协议之条款下提供，附加条款亦可能应用。（请参阅使用条款）Wikipedia®和维基百科标志是维基媒体基金会的注册商标；维基™是维基媒体基金会的商标。维基媒体基金会是按美国国內稅收法501(c)(3)登记的非营利慈善机构。隐私政策关于维基百科免责声明行为准则开发者统计Cookie声明手机版视图开关有限宽度模式', \"nlEntropie (informatietheorie) - WikipediaNaar inhoud springenHoofdmenuHoofdmenunaar zijbalk verplaatsenverbergenNavigatieHoofdpaginaVind een artikelVandaagEtalageCategorieënRecente wijzigingenNieuwe artikelenWillekeurige paginaInformatieGebruikersportaalSnelcursusHulp en contactDonerenZoekenZoekenAccount aanmakenAanmeldenPersoonlijke hulpmiddelenAccount aanmakenAanmeldenPagina's voor uitgelogde redacteurenmeer lezenBijdragenOverlegInhoudnaar zijbalk verplaatsenverbergenTop1Definitie2Voorbeeld3Relatie met thermodynamica4ReferentiesInhoudsopgave tonen of verbergenEntropie (informatietheorie)45 talenAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Koppelingen bewerkenArtikelOverlegNederlandsLezenBewerkenBrontekst bewerkenGeschiedenis weergevenHulpmiddelenHulpmiddelennaar zijbalk verplaatsenverbergenHandelingenLezenBewerkenBrontekst bewerkenGeschiedenis weergevenAlgemeenLinks naar deze paginaGerelateerde wijzigingenBestand uploadenSpeciale pagina'sPermanente koppelingPaginagegevensDeze pagina citerenVerkorte URL verkrijgenQR-code downloadenWikidata-itemAfdrukken/exporterenBoek aanmakenDownloaden als PDFAfdrukversieIn andere projectenWikimedia CommonsUit Wikipedia, de vrije encyclopedieEntropieis een maat voor deonzekerheid(ofonwetendheid) bij het waarnemen van een reeks gebeurtenissen.\", 'nlNieuweinformatieontstaat als een gebeurtenis plaatsvindt waarvan vooraf onzeker was of deze daadwerkelijk zou gebeuren.', 'nlIn deinformatietheoriewordt dit inzicht verder wiskundig uitgewerkt.De verwachte (vaak gemiddelde genoemd) hoeveelheid informatie bij een nog plaats te vinden gebeurtenis of een nog uit te voeren (kans)experiment, is gedefinieerd als deverwachtingswaardevan de hoeveelheidzelfinformatiedie deze gebeurtenis zal opleveren.Bij een entropie 0 is er geen onzekerheid: men heeft volledige kennis over wat er komen gaat en deze informatie bevat dus ook geen \"nieuws\".', 'nlBij een maximale onzekerheid (bv.', 'nlbij een getoonde willekeurige symbolenreeks) is de entropie gelijk aanlog2\\u2061nn}(metnde lengte van de reeks): elke gebeurtenis is onverwacht en dus nieuw.Bedacht dient te worden dat entropie een subjectiefemergentoordeel vooronderstelt over de betekenis van de reeks gebeurtenissen: iemand die geen betekenis kan hechten aan de reeks gebeurtenissen (bv.', 'nleen analfabeet die een tekst vanntekens ziet passeren) zal een entropielog2nn}aan deze reeks toekennen.', 'nlOok kunnen twee waarnemers die verschillen in kennisniveau/onwetendheid, een verschillende entropie toekennen aan dezelfde reeks gebeurtenissen.Definitie[bewerken|brontekst bewerken]Stel datu1,…,un,\\\\ldots ,u_}de mogelijke uitkomsten zijn bij een experimentA.', 'nlDe kans van optreden van de uitkomstui}ispi}.', 'nlEen dergelijk experiment wordt beschreven door dediscrete verdelingbepaald door de kansenpi}, of ook door eendiscrete stochastische variabeleXmet dezekansverdeling.', 'nlDe uitkomstui}bevat een hoeveelheidzelfinformatieH(ui)=−log2\\u2061(pi))=-\\\\log _(p_)}.DeentropieHvan het experiment, of van de kansverdeling, of ook vanX, is de verwachte hoeveelheid informatie:H(p)=EH(X)=∑i=1npi(−log2\\u2061(pi))=−∑i=1npilog2\\u2061(pi)}(p)=}\\\\,}(X)=\\\\sum _^p_(-\\\\log _(p_))=-\\\\sum _^p_\\\\log _(p_)}bit.Als niet de binaire logaritme maar denatuurlijke logaritmegebruikt wordt, heet de eenheid waarin de entropie gemeten wordt denat; wordt debriggse logaritmegebruikt, dus metgrondtal10, dan heet de eenheid deban.Bewijsbaar is dat de entropie van een experiment met N mogelijke uitkomsten nooit meer is danlog2NN}bit, en dat deze wordt bereikt als elke uitkomst een even grote kans1/Nheeft.Omdat de definitie van entropie enigszins contra-intuïtief is (grotere entropie bij meer onzekerheid), wordt soms het begripnegentropie(negatieve entropie) gebruikt, waar een negentropie van 0 staat voor totale onzekerheid/onwetendheid en de zekerheid van informatie alleen maar toeneemt bij toenemende negentropie.', 'nlNegentropie is dan de afstand van de beschouwde kansverdeling tot die van een totaal willekeurige.Voorbeeld[bewerken|brontekst bewerken]Het alfabet bevat zes klinkers (a, e, i, o, u, y) en twintig medeklinkers.', 'nlBij een experiment schrijft iemand geheel willekeurig een letter op een papiertje, en wordt er vervolgens vastgesteld of het een klinker dan wel eenmedeklinkeris.', 'nlDe entropie van dit experiment is danH(A)=−∑i=1npi⋅log2\\u2061(pi)=−626⋅log2\\u2061(626)−(2026)⋅log2\\u2061(2026)=0,488+0,291=0,779^p_\\\\cdot \\\\log _(p_)=-}\\\\cdot \\\\log _(})-(})\\\\cdot \\\\log _(})=0488+0291=0779}bit.Dit is een voorbeeld van een experiment met twee mogelijke uitkomsten.', 'nlDe entropie is dus niet meer dan 1 bit.Relatie met thermodynamica[bewerken|brontekst bewerken]Het begripentropieis bekender in dethermodynamicadan in de informatietheorie, maar de definitie ervan in de informatietheorie heeft veel overeenkomsten.', 'nlHet werk vanBoltzmannenGibbsaanstatistische thermodynamicainspireerdeShannonom het begrip in de informatietheorie te gebruiken.Alleen detweede wet van de thermodynamica, die hier zou neerkomen op een vooronderstelde natuurlijke neiging om verschillen in \"onwetendheid\" af te vlakken, waardoor de totale entropie toe zou nemen, heeft geen equivalent in de informatietheorie.', 'nlRovelli[1]maakte bijvoorbeeld duidelijk dat het sorteren van een gemengde verzameling weliswaar een geordende verzameling oplevert (met een lagere entropie), maar het sorteren zelf (en soms ook het handhaven van de gesorteerde verzameling) zoveelenergiekost dat de totale, thermodynamische entropie toch toeneemt.Referenties[bewerken|brontekst bewerken]↑Carlo Rovelli:The thermodynamic cost of choosinghttps://arxiv.org/pdf/2309.08557.pdfOvergenomen van \"https://nl.wikipedia.org/w/index.php?title=Entropie_(informatietheorie)&oldid=67098131\"Categorie:InformatietheorieDeze pagina is voor het laatst bewerkt op 26 feb 2024 om 00:42.De tekst is beschikbaar onder de licentieCreative Commons Naamsvermelding/Gelijk delen, er kunnen aanvullende voorwaarden van toepassing zijn.', 'nlZie degebruiksvoorwaardenvoor meer informatie.Wikipedia® is een geregistreerd handelsmerk van deWikimedia Foundation, Inc., een organisatie zonder winstoogmerk.PrivacybeleidOver WikipediaDisclaimersGedragscodeOntwikkelaarsStatistiekenCookieverklaringMobiele weergaveBeperkte inhoudsbreedte inschakelen', 'viEntropy thông tin – Wikipedia tiếng ViệtBước tới nội dungTrình đơn chínhTrình đơn chínhchuyển sang thanh bênẩnĐiều hướngTrang ChínhNội dung chọn lọcBài viết ngẫu nhiênThay đổi gần đâyPhản hồi lỗiĐóng gópTương tácHướng dẫnGiới thiệu WikipediaCộng đồngThảo luận chungGiúp sử dụngLiên lạcTải lên tập tinTìm kiếmTìm kiếmTạo tài khoảnĐăng nhậpCông cụ cá nhânTạo tài khoảnĐăng nhậpTrang dành cho người dùng chưa đăng nhậptìm hiểu thêmĐóng gópThảo luận cho địa chỉ IP nàyNội dungchuyển sang thanh bênẩnĐầu1Định nghĩaHiện/ẩn mục Định nghĩa1.1Ngẫu nhiên rời rạc1.2Ngẫu nhiên liên tục2Ví dụ3Liên hệ với cơ học thống kê4Tham khảo5Liên kết ngoàiĐóng mở mục lụcEntropy thông tin45 ngôn ngữAfrikaansالعربيةBahasa IndonesiaBoarischBosanskiБългарскиCatalàČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalego한국어ItalianoעבריתLietuviųMagyarNederlands日本語Олык марийਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийShqipSimple EnglishSlovenčinaSlovenščinaکوردیСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردو粵語中文Sửa liên kếtBài viếtThảo luậnTiếng ViệtĐọcSửa đổiSửa mã nguồnXem lịch sửCông cụCông cụchuyển sang thanh bênẩnTác vụĐọcSửa đổiSửa mã nguồnXem lịch sửChungCác liên kết đến đâyThay đổi liên quanTrang đặc biệtLiên kết thường trựcThông tin trangTrích dẫn trang nàyLấy URL ngắn gọnTải mã QRKhoản mục WikidataIn và xuấtTạo một quyển sáchTải dưới dạng PDFBản để in raTại dự án khácWikimedia CommonsBách khoa toàn thư mở WikipediaEntropy thông tinlà một khái niệm mở rộng củaentropytrongnhiệt động lực họcvàcơ học thống kêsang cholý thuyết thông tin.Entropy thông tin mô tả mức độhỗn loạntrong mộttín hiệulấy từ một sự kiệnngẫu nhiên.', 'viNói cách khác, entropy cũng chỉ ra có bao nhiêuthông tintrong tín hiệu, với thông tin là các phần không hỗn loạn ngẫu nhiên của tín hiệu.Ví dụ, nhìn vào một dòng chữtiếng Việt, được mã hóa bởi cácchữ cái, khoảng cách, vàdấu câu, tổng quát là cácký tự.', 'viDòng chữ có ý nghĩa sẽ không hiện ra một cách hoàn toàn hỗn loạn ngẫu nhiên; ví dụ nhưtần sốxuất hiện của chữ cáixsẽ không giống với tần số xuất hiện của chữ cái phổ biến hơn làt.', 'viĐồng thời, nếu dòng chữ vẫn đang được viết hay đang được truyền tải, khó có thể đoán trước được ký tự tiếp theo sẽ là gì, do đó nó có mức độ ngẫu nhiên nhất định.', 'viEntropy thông tin là một thang đo mức độ ngẫu nhiên này.Khái niệm này lần đầu giới thiệu bởiClaude E. Shannontrong bài báo \"A Mathematical Theory of CommunicationLưu trữ1998-01-31 tạiWayback Machine\", năm1948.', 'viTrước đóvon Neumannđã dùng đến công thức có entropy vào năm1927.Định nghĩa[sửa|sửa mã nguồn]Claude E. Shannonđã xây dựng định nghĩa về entropy để thoả mãn các giả định sau:Entropy phảitỷ lệ thuậnliên tụcvới cácxác suấtxuất hiện của các phần tử ngẫu nhiên trong tín hiệu.', 'viThay đổi nhỏ trong xác suất phải dẫn đến thay đổi nhỏ trong entropy.Nếu các phần tử ngẫu nhiên đều có xác suất xuất hiện bằng nhau, việc tăng số lượng phần tử ngẫu nhiên phải làm tăng entropy.Có thể tạo các chuỗi tín hiệu theo nhiều bước, và entropy tổng cộng phải bằng tổng có trọng số của entropy của từng bước.Shannon cũng chỉ ra rằng bất cứ định nghĩa nào của entropy, cho một tín hiệu có thể nhận các giá trị rời rạc, thoả mãn các giả định của ông thì đều có dạng:−K∑i=1np(i)log\\u2061p(i).^p(i)\\\\log p(i).\\\\,\\\\!', 'vi}vớiKlà một hằng số, chỉ phụ thuộc vàođơn vị đo.nlà tổng số các giá trị có thể nhận của tín hiệu.ilà giá trị rời rạc thứi.p(i) là xác suất xuất hiện của giá trịi.Ngẫu nhiên rời rạc[sửa|sửa mã nguồn]Entropy của mộtphép thử Bernoulliđược vẽ như mộthàm sốtheoxác suấtthành công, thường gọi làhàm entropy nhị phân.Nếu một sự kiệnngẫu nhiên rời rạcx, có thể nhận các giá trị là 1..n, thì entropy của nó là:H(x)=∑i=1np(i)log2\\u2061(1p(i))=−∑i=1np(i)log2\\u2061p(i).^p(i)\\\\log _\\\\left(}\\\\right)=-\\\\sum _^p(i)\\\\log _p(i).\\\\,\\\\!', 'vi}vớip(i) là xác suất xảy ra của giá trịi.', 'viNhư vậy, entropy củaxcũng làgiá trị kì vọngcủa cácđộ ngạc nhiêncủa các giá trị màxcó thể nhận.Entropy thông tin trong trường hợp phần tử tín hiệu ngẫu nhiên rời rạc còn được gọi làentropy Shannon.Ngẫu nhiên liên tục[sửa|sửa mã nguồn]Nếuxlàsố thựcngẫu nhiên liên tục, thì định nghĩa entropy có thể được biểu diễn là:h[f]=−∫−∞∞f(x)log\\u2061f(x)dx,^f(x)\\\\log f(x)\\\\,dx,\\\\quad }vớiflàhàm mật độ xác suất.', 'viĐịnh nghĩa này thường được gọi làentropy Boltzmannhayentropy liên tục, hayentropy vi phân.Có thể chứng minh rằngentropy Boltzmannkhông phải là giới hạn củaentropy Shannonkhin→ ∞ và do đó không phải là độ đo mức độ hỗn loạn của thông tin.Ví dụ[sửa|sửa mã nguồn]Một dòng chữ luôn chỉ có các ký tự \"a\" sẽ có entropy bằng 0, vì ký tự tiếp theo sẽ luôn là \"a\".', 'viMột dòng chữ chỉ có hai ký tự 0 và 1 ngẫu nhiên hoàn toàn sẽ có entropy là 1bitcho mỗi ký tự.Một dòng chữtiếng Anhthông thường có entropy khoảng 1,1 đến 1,6 bit cho mỗi ký tự.Thuật toán nén PPMcó thể tạo ra tỷ lệ nén 1,5 bit cho mỗi ký tự.', 'viTrên thực tế, tỷ lệ nén của các thuật toán nén thông dụng có thể được dùng làm ước lượng cho entropy của dữ liệu.Entropy của dòng văn bản thuần thường được định nghĩa dựa trênmô hình Markov.', 'viNếu các ký tự tiếp theo hoàn toàn độc lập với các ký tự trước đó, entropy nhị phân sẽ là:H(S)=−∑pilog2\\u2061pi,})=-\\\\sum p_\\\\log _p_,\\\\,\\\\!', 'vi}vớipilàxác suấtcủai.Liên hệ với cơ học thống kê[sửa|sửa mã nguồn]Định nghĩa entropy của Shannon có liên hệ chặt chẽ với định nghĩaentropytrongcơ học thống kê.', 'viChính các công trình củaLudwig BoltzmannhayWillard Gibbstrongcơ học thống kêđã kích thích việc sử dụng từentropytronglý thuyết thông tin.', 'viTheoEdwin Thompson Jaynes(1957), thực tếcơ học thống kêvànhiệt động lực họccó thể coi là ứng dụng củalý thuyết thông tin: entropy trong nhiệt động lực học có thể cọi là độ đo của thông tin vi mô (mô tả các trạng thái vi mô của từng phần tử trong hệ vật lý) mà chưa được mô tả hết bởi các thông số vĩ mô của hệ nhiệt động lực học.Ví dụ về tương quan giữa entropy nhiệt động lực học và entropy thông tin còn được thể hiện ởcon quỷ Maxwell.', 'viQuỷ Maxwell có thể tạo ra được khi nó làm giảm entropy nhiệt động lực học nhưng làm tăng entropy thông tin và cả hệ vẫn tuân thủđịnh luật hai nhiệt động lực họcvới tổng entropy không đổi và quá trình hoạt động của quỷ làthuận nghịch.Tham khảo[sửa|sửa mã nguồn]Liên kết ngoài[sửa|sửa mã nguồn]Description of information entropy from \"Tools for Thought\" by Howard RheingoldLưu trữ2011-05-15 tạiWayback MachineLấy từ “https://vi.wikipedia.org/w/index.php?title=Entropy_thông_tin&oldid=68165029”Thể loại:Entropy thông tinEntropyLý thuyết thông tinSố ngẫu nhiênLý thuyết thống kêThể loại ẩn:Bản mẫu webarchive dùng liên kết waybackTrang này được sửa đổi lần cuối vào ngày 17 tháng 2 năm 2022, 01:41.Văn bản được phát hành theoGiấy phép Creative Commons Ghi công–Chia sẻ tương tự; có thể áp dụng điều khoản bổ sung.', 'viVới việc sử dụng trang web này, bạn chấp nhậnĐiều khoản Sử dụngvàQuy định quyền riêng tư.', 'viWikipedia® là thương hiệu đã đăng ký củaWikimedia Foundation, Inc., một tổ chức phi lợi nhuận.Quy định quyền riêng tưGiới thiệu WikipediaLời phủ nhậnBộ Quy tắc Ứng xử ChungLập trình viênThống kêTuyên bố về cookiePhiên bản di độngChuyển đổi chiều rộng nội dung giới hạn', 'afEntropie (Inligtingsteorie) - WikipediaGaan na inhoudHoofkieslysHoofkieslysskuif na kantbalkversteekNavigasieTuisbladGebruikersportaalGeselshoekieOnlangse wysigingsLukrake bladsyHulpSandputSkenkingsSoekSoekSkep gebruikerMeld aanPersoonlike gereedskapSkep gebruikerMeld aanBladsye vir uitgemelde redakteursleer meerBydraesBesprekingInhoudskuif na kantbalkversteekInleiding1Sien ook2VerwysingsWissel die inhoudsopgaweEntropie (Inligtingsteorie)45 taleالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Wysig skakelsBladsyBesprekingAfrikaansLeesWysigWysig bronWys geskiedenisGereedskapGereedskapskuif na kantbalkversteekActionsLeesWysigWysig bronWys geskiedenisAlgemeenSkakels hierheenVerwante veranderingsSpesiale bladsyePermanente skakelBladinligtingHaal dié blad aanKry verkorte URLDownload QR codeWikidata-itemDruk/uitvoerSkep boekLaai af as PDFDrukbare weergaweAnder projekteWikimedia Commonsin Wikipedia, die vrye ensiklopedieInligtingsteorieEntropieDifferensiële entropieVoorwaardelike entropieGesamentlike entropieWedersydse inligtingVoorwaardelike wedersydse inligtingRelatiewe entropieEntropie koersBeperking van digtheid van diskrete punteAsimptotiese ekwipartisie-eienskapTempo-vervorming teorieShannon se bronkoderingstellingKanaal kapasiteitRuiserige-kanaal kodering stellingShannon–Hartley stellinglbwInInligtingsteorie, is dieentropievan \\'n ewekansige veranderlike die gemiddelde vlak van \"inligting\", \"verrassing\" of \"onsekerheid\" ingebore aan die veranderlike se moontlike uitkomste.', \"afGegewe 'n diskrete ewekansige veranderlikeX, met moontlike uitkomstex1,...,xn,...,x_}, wat met waarskynlikheidP(x1),...,P(xn), (x_),...,\\\\mathrm (x_),}plaasvind, die entropie vanXword formeel gedefinieer as:H(X)=−∑i=1nP(xi)log\\u2061P(xi) (X)=-\\\\sum _^ (x_)\\\\log \\\\mathrm (x_)}}waarΣdui die som bo die veranderlike se moontlike waardes aan.\", 'afDie keuse van basis virlog, dielogaritme, verskil vir verskillende toepassings.', 'afBasis 2 gee die eenheid vanbis(of \"shannons\"), terwyl basisegee \"natuurlike eenhede\"nat, en basis 10 gee eenhede van \"dits\", \"bans\", of \"hartleys\".', \"af'n Ekwivalente definisie van entropie is dieverwagte waardevan dieselfinligtingvan 'n veranderlike.\", 'af[1]Twee bisse entropie: In die geval van twee billike muntgooie, die inligtingsentropie in bisse is die basis-2-logaritme van die aantal moontlike uitkomste; met twee munte is daar vier moontlike uitkomste, en twee bisse van entropie.In die algemeen is inligtingsentropie die gemiddelde hoeveelheid inligting wat deur \\'n gebeurtenis oorgedra word, wanneer alle moontlike uitkomste in ag geneem word.Die konsep van inligtingsentropie is deurClaude Shannonbekendgestel in sy 1948-artikel \"A Mathematical Theory of Communication\",[2][3]en word ook na verwys asShannon-entropie.Sien ook[wysig|wysig bron]Entropie (termodinamika)Verwysings[wysig|wysig bron]↑Pathria, R. K.; Beale, Paul (2011).Statistical Mechanics(Third uitg.).', 'afAcademic Press.', 'afp. 51.ISBN978-0123821881.↑Shannon, Claude E.(Julie 1948).', 'afA Mathematical Theory of Communication.Bell System Technical Journal.27(3): 379–423.doi:10.1002/j.1538-7305.1948.tb01338.x.hdl:10338.dmlcz/101429.', 'af(PDF, archived fromhere)↑Shannon, Claude E.(Oktober 1948).', 'afA Mathematical Theory of Communication.Bell System Technical Journal.27(4): 623–656.doi:10.1002/j.1538-7305.1948.tb00917.x.hdl:11858/00-001M-0000-002C-4317-B.', 'af(PDF, archived fromhere)Hierdie artikel is ’nsaadjie.', 'afVoel vry om Wikipedia te help deur dituit te brei.NormdataBNE:XX535116BNF:cb11985913j(data)GND:4743861-7LCCN:sh85044152NDL:01191172NKC:ph425914Ontsluit van \"https://af.wikipedia.org/w/index.php?title=Entropie_(Inligtingsteorie)&oldid=2493078\"Kategorie:InligtingsteorieVersteekte kategorieë:SaadjiesArtikels met BNE-identifiseerdersArtikels met BNF-identifiseerdersArtikels met GND-identifiseerdersArtikels met LCCN-identifiseerdersArtikels met NDL-identifiseerdersArtikels met NKC-identifiseerdersNormdata met 6 elementeDie bladsy is laas op 14 April 2022 om 12:42 bygewerk.Die teks is beskikbaar onder die lisensieCreative Commons Erkenning-Insgelyks Deel.', 'afAanvullende voorwaardes kan moontlik ook van toepassing wees.', 'afSien dieAlgemene Voorwaardesvir meer inligting.PrivaatheidsbeleidInligting oor WikipediaVrywaringCode of ConductOntwikkelaarsStatistiekeKoekieverklaringSelfoonweergaweWissel beperkte inhoudwydte', 'enEntropy (information theory) - WikipediaJump to contentMain menuMain menumove to sidebarhideNavigationMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonateContributeHelpLearn to editCommunity portalRecent changesUpload fileSearchSearchCreate accountLog inPersonal toolsCreate accountLog inPages for logged out editorslearn moreContributionsTalkContentsmove to sidebarhide(Top)1IntroductionToggle Introduction subsection1.1Example2DefinitionToggle Definition subsection2.1Measure theory3Example4CharacterizationToggle Characterization subsection4.1Alternative characterization4.1.1Discussion4.2Alternative characterization via additivity and subadditivity4.2.1Discussion5Further properties6AspectsToggle Aspects subsection6.1Relationship to thermodynamic entropy6.2Data compression6.3Entropy as a measure of diversity6.4Entropy of a sequence6.5Limitations of entropy in cryptography6.6Data as a Markov process7Efficiency (normalized entropy)8Entropy for continuous random variablesToggle Entropy for continuous random variables subsection8.1Differential entropy8.2Limiting density of discrete points8.3Relative entropy9Use in number theory10Use in combinatoricsToggle Use in combinatorics subsection10.1Loomis–Whitney inequality10.2Approximation to binomial coefficient11Use in machine learning12See also13References14Further readingToggle Further reading subsection14.1Textbooks on information theory15External linksToggle the table of contentsEntropy (information theory)45 languagesAfrikaansالعربيةБългарскиBoarischBosanskiCatalàČeštinaCymraegDanskDeutschΕλληνικάEspañolEuskaraفارسیFrançaisGalego한국어Bahasa IndonesiaItalianoעבריתLietuviųMagyarNederlands日本語Олык марийਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийShqipSimple EnglishSlovenčinaSlovenščinaکوردیСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt粵語中文Edit linksArticleTalkEnglishReadEditView historyToolsToolsmove to sidebarhideActionsReadEditView historyGeneralWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata itemPrint/exportDownload as PDFPrintable versionIn other projectsWikimedia CommonsFrom Wikipedia, the free encyclopediaExpected amount of information needed to specify the output of a stochastic data sourceFor other uses, seeEntropy (disambiguation).This articleneeds additional citations forverification.Please helpimprove this articlebyadding citations to reliable sources.', 'enUnsourced material may be challenged and removed.Find sources:\"Entropy\" information theory–news·newspapers·books·scholar·JSTOR(February 2019)(Learn how and when to remove this template message)Information theoryEntropyDifferential entropyConditional entropyJoint entropyMutual informationDirected informationConditional mutual informationRelative entropyEntropy rateLimiting density of discrete pointsAsymptotic equipartition propertyRate–distortion theoryShannon\\'s source coding theoremChannel capacityNoisy-channel coding theoremShannon–Hartley theoremvteIninformation theory, theentropyof arandom variableis the average level of \"information\", \"surprise\", or \"uncertainty\" inherent to the variable\\'s possible outcomes.', \"enGiven a discrete random variableX, which takes values in the alphabetX}}and is distributed according top:X→[0,1]}\\\\to [0,1]}, the entropy isH(X):=−∑x∈Xp(x)log\\u2061p(x), (X):=-\\\\sum _}}p(x)\\\\log p(x),}whereΣdenotes the sum over the variable's possible values.\", 'enThe choice of base forlog, thelogarithm, varies for different applications.', 'enBase 2 gives the unit ofbits(or \"shannons\"), while baseegives \"natural units\"nat, and base 10 gives units of \"dits\", \"bans\", or \"hartleys\".', 'enAn equivalent definition of entropy is theexpected valueof theself-informationof a variable.', 'en[1]Two bits of entropy: In the case of two fair coin tosses, the information entropy in bits is the base-2 logarithm of the number of possible outcomes\\u200d— with two coins there are four possible outcomes, and two bits of entropy.', 'enGenerally, information entropy is the average amount of information conveyed by an event, when considering all possible outcomes.The concept of information entropy was introduced byClaude Shannonin his 1948 paper \"A Mathematical Theory of Communication\",[2][3]and is also referred to asShannon entropy.', \"enShannon's theory defines adata communicationsystem composed of three elements: a source of data, acommunication channel, and a receiver.\", 'enThe \"fundamental problem of communication\" – as expressed by Shannon – is for the receiver to be able to identify what data was generated by the source, based on the signal it receives through the channel.', 'en[2][3]Shannon considered various ways to encode, compress, and transmit messages from a data source, and proved in his famoussource coding theoremthat the entropy represents an absolute mathematical limit on how well data from the source can belosslesslycompressed onto a perfectly noiseless channel.', 'enShannon strengthened this result considerably for noisy channels in hisnoisy-channel coding theorem.Entropy in information theory is directly analogous to theentropyinstatistical thermodynamics.', \"enThe analogy results when the values of the random variable designate energies of microstates, so Gibbs formula for the entropy is formally identical to Shannon's formula.\", 'enEntropy has relevance to other areas of mathematics such ascombinatoricsandmachine learning.', 'enThe definition can be derived from a set ofaxiomsestablishing that entropy should be a measure of how informative the average outcome of a variable is.', 'enFor a continuous random variable,differential entropyis analogous to entropy.', 'enThe definitionE[−log\\u2061p(X)] [-\\\\log p(X)]}generalizes the above.Introduction[edit]The core idea of information theory is that the \"informational value\" of a communicated message depends on the degree to which the content of the message is surprising.', 'enIf a highly likely event occurs, the message carries very little information.', 'enOn the other hand, if a highly unlikely event occurs, the message is much more informative.', 'enFor instance, the knowledge that some particular numberwill notbe the winning number of a lottery provides very little information, because any particular chosen number will almost certainly not win.', 'enHowever, knowledge that a particular numberwillwin a lottery has high informational value because it communicates the outcome of a very low probability event.Theinformation content,also called thesurprisalorself-information,of an eventEis a function which increases as the probabilityp(E)of an event decreases.', 'enWhenp(E)is close to 1, the surprisal of the event is low, but ifp(E)is close to 0, the surprisal of the event is high.', 'enThis relationship is described by the functionlog\\u2061(1p(E)),}\\\\right),}wherelogis thelogarithm, which gives 0 surprise when the probability of the event is 1.', 'en[4]In fact,logis the only function that satisfies а specific set of conditions defined in section§ Characterization.Hence, we can define the information, or surprisal, of an eventEbyI(E)=−log2\\u2061(p(E)),(p(E)),}or equivalently,I(E)=log2\\u2061(1p(E)).\\\\left(}\\\\right).', 'en}Entropy measures the expected (i.e., average) amount of information conveyed by identifying the outcome of a random trial.', 'en[5]: 67This implies that rolling a die has higher entropy than tossing a coin because each outcome of a die toss has smaller probability (p=1/6) than each outcome of a coin toss (p=1/2).Consider a coin with probabilitypof landing on heads and probability1 −pof landing on tails.', 'enThe maximum surprise is whenp= 1/2, for which one outcome is not expected over the other.', 'enIn this case a coin flip has an entropy of onebit.', 'en(Similarly, onetritwith equiprobable values containslog2\\u206133}(about 1.58496) bits of information because it can have one of three values.)', 'enThe minimum surprise is whenp= 0orp= 1, when the event outcome is known ahead of time, and the entropy is zero bits.', 'enWhen the entropy is zero bits, this is sometimes referred to as unity, where there is no uncertainty at all – no freedom of choice – noinformation.', 'enOther values ofpgive entropies between zero and one bits.Example[edit]Information theory is useful to calculate the smallest amount of information required to convey a message, as indata compression.', \"enFor example, consider the transmission of sequences comprising the 4 characters 'A', 'B', 'C', and 'D' over a binary channel.\", 'enIf all 4 letters are equally likely (25%), one can not do better than using two bits to encode each letter.', \"en'A' might code as '00', 'B' as '01', 'C' as '10', and 'D' as '11'.\", \"enHowever, if the probabilities of each letter are unequal, say 'A' occurs with 70% probability, 'B' with 26%, and 'C' and 'D' with 2% each, one could assign variable length codes.\", \"enIn this case, 'A' would be coded as '0', 'B' as '10', 'C' as '110', and 'D' as '111'.\", 'enWith this representation, 70% of the time only one bit needs to be sent, 26% of the time two bits, and only 4% of the time 3 bits.', \"enOn average, fewer than 2 bits are required since the entropy is lower (owing to the high prevalence of 'A' followed by 'B' – together 96% of characters).\", 'enThe calculation of the sum of probability-weighted log probabilities measures and captures this effect.', 'enEnglish text, treated as a string of characters, has fairly low entropy; i.e.', 'enit is fairly predictable.', \"enWe can be fairly certain that, for example, 'e' will be far more common than 'z', that the combination 'qu' will be much more common than any other combination with a 'q' in it, and that the combination 'th' will be more common than 'z', 'q', or 'qu'.\", 'enAfter the first few letters one can often guess the rest of the word.', 'enEnglish text has between 0.6 and 1.3 bits of entropy per character of the message.', \"en[6]: 234Definition[edit]Named afterBoltzmann's Η-theorem, Shannon defined the entropyΗ(Greek capital lettereta) of adiscrete random variableX, which takes values in the alphabetX}}and is distributed according top:X→[0,1]}\\\\to [0,1]}such thatp(x):=P[X=x] [X=x]}:H(X)=E[I\\u2061(X)]=E[−log\\u2061p(X)].\", 'en(X)=\\\\mathbb [\\\\operatorname (X)]=\\\\mathbb [-\\\\log p(X)].', 'en}HereE }is theexpected value operator, andIis theinformation contentofX.', 'en[7]: 11[8]: 19–20I\\u2061(X) (X)}is itself a random variable.The entropy can explicitly be written as:H(X)=−∑x∈Xp(x)logb\\u2061p(x), (X)=-\\\\sum _}}p(x)\\\\log _p(x),}wherebis thebaseof thelogarithmused.', \"enCommon values ofbare 2,Euler's numbere, and 10, and the corresponding units of entropy are thebitsforb= 2,natsforb=e, andbansforb= 10.\", 'en[9]In the case ofp(x)=0for somex∈X}}, the value of the corresponding summand0 logb(0)is taken to be0, which is consistent with thelimit:[10]: 13limp→0+plog\\u2061(p)=0.}p\\\\log(p)=0.', 'en}One may also define theconditional entropyof two variablesXandYtaking values from setsX}}andY}}respectively, as:[10]: 16H(X|Y)=−∑x,y∈X×YpX,Y(x,y)log\\u2061pX,Y(x,y)pY(y), (X|Y)=-\\\\sum _}\\\\times }}p_(x,y)\\\\log (x,y)}(y)}},}wherepX,Y(x,y):=P[X=x,Y=y](x,y):=\\\\mathbb [X=x,Y=y]}andpY(y)=P[Y=y](y)=\\\\mathbb [Y=y]}.', 'enThis quantity should be understood as the remaining randomness in the random variableXgiven the random variableY.Measure theory[edit]Entropy can be formally defined in the language ofmeasure theoryas follows:[11]Let(X,Σ,μ)be aprobability space.', 'enLetA∈Σbe anevent.', 'enThesurprisalofAisσμ(A)=−ln\\u2061μ(A).', 'en(A)=-\\\\ln \\\\mu (A).', 'en}Theexpectedsurprisal ofAishμ(A)=μ(A)σμ(A).', 'en(A)=\\\\mu (A)\\\\sigma _(A).', 'en}Aμ-almostpartitionis aset familyP⊆P(X)}(X)}such thatμ(∪\\u2061P)=1 P)=1}andμ(A∩B)=0for all distinctA,B∈P.', 'en(This is a relaxation of the usual conditions for a partition.)', 'enThe entropy ofPisHμ(P)=∑A∈Phμ(A).', 'en_(P)=\\\\sum _h_(A).', 'en}LetMbe asigma-algebraonX.', 'enThe entropy ofMisHμ(M)=supP⊆MHμ(P).', 'en_(M)=\\\\sup _\\\\mathrm _(P).', 'en}Finally, the entropy of the probability space isHμ(Σ) _(\\\\Sigma )}, that is, the entropy with respect toμof the sigma-algebra ofallmeasurable subsets ofX.Example[edit]EntropyΗ(X)(i.e.', 'entheexpectedsurprisal) of a coin flip, measured in bits, graphed versus the bias of the coinPr(X= 1), whereX= 1represents a result of heads.', 'en[10]: 14–15Here, the entropy is at most 1 bit, and to communicate the outcome of a coin flip (2 possible values) will require an average of at most 1 bit (exactly 1 bit for a fair coin).', 'enThe result of a fair die (6 possible values) would have entropy log26 bits.Main articles:Binary entropy functionandBernoulli processConsider tossing a coin with known, not necessarily fair, probabilities of coming up heads or tails; this can be modelled as aBernoulli process.The entropy of the unknown result of the next toss of the coin is maximized if the coin is fair (that is, if heads and tails both have equal probability 1/2).', 'enThis is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss; the result of each toss of the coin delivers one full bit of information.', 'enThis is becauseH(X)=−∑i=1np(xi)logb\\u2061p(xi)=−∑i=1212log2\\u206112=−∑i=1212⋅(−1)=1.\\\\mathrm (X)&=-\\\\sum _^)\\\\log _p(x_)}\\\\\\\\&=-\\\\sum _^}\\\\log _}}\\\\\\\\&=-\\\\sum _^}\\\\cdot (-1)}=1.\\\\end}}However, if we know the coin is not fair, but comes up heads or tails with probabilitiespandq, wherep≠q, then there is less uncertainty.', 'enEvery time it is tossed, one side is more likely to come up than the other.', 'enThe reduced uncertainty is quantified in a lower entropy: on average each toss of the coin delivers less than one full bit of information.', 'enFor example, ifp= 0.7, thenH(X)=−plog2\\u2061(p)−qlog2\\u2061(q)=−0.7log2\\u2061(0.7)−0.3log2\\u2061(0.3)≈−0.7⋅(−0.515)−0.3⋅(−1.737)=0.8816<1.\\\\mathrm (X)&=-p\\\\log _(p)-q\\\\log _(q)\\\\\\\\&=-0.7\\\\log _(0.7)-0.3\\\\log _(0.3)\\\\\\\\&\\\\approx -0.7\\\\cdot (-0.515)-0.3\\\\cdot (-1.737)\\\\\\\\&=0.8816<1.\\\\end}}Uniform probability yields maximum uncertainty and therefore maximum entropy.', 'enEntropy, then, can only decrease from the value associated with uniform probability.', 'enThe extreme case is that of a double-headed coin that never comes up tails, or a double-tailed coin that never results in a head.', 'enThen there is no uncertainty.', 'enThe entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain.', 'en[10]: 14–15Characterization[edit]To understand the meaning of−Σpilog(pi), first define an information functionIin terms of an eventiwith probabilitypi.', \"enThe amount of information acquired due to the observation of eventifollows from Shannon's solution of the fundamental properties ofinformation:[12]I(p)ismonotonically decreasinginp: an increase in the probability of an event decreases the information from an observed event, and vice versa.I(1) = 0: events that always occur do not communicate information.I(p1·p2) = I(p1) + I(p2): the information learned fromindependent eventsis the sum of the information learned from each event.Given two independent events, if the first event can yield one ofnequiprobableoutcomes and another has one ofmequiprobable outcomes then there aremnequiprobable outcomes of the joint event.\", 'enThis means that iflog2(n)bits are needed to encode the first value andlog2(m)to encode the second, one needslog2(mn) = log2(m) + log2(n)to encode both.Shannon discovered that a suitable choice ofI }is given by:[13]I\\u2061(p)=log\\u2061(1p)=−log\\u2061(p).', 'en(p)=\\\\log \\\\left(}\\\\right)=-\\\\log(p).', 'en}In fact, the only possible values ofI }areI\\u2061(u)=klog\\u2061u (u)=k\\\\log u}fork<0.', 'enAdditionally, choosing a value forkis equivalent to choosing a valuex>1fork=−1/log\\u2061x, so thatxcorresponds to thebase for the logarithm.', \"enThus, entropy ischaracterizedby the above four properties.ProofLetI }be the information function which one assumes to be twice continuously differentiable, one has:I\\u2061(p1p2)=I\\u2061(p1)+I\\u2061(p2)Starting from property 3p2I′\\u2061(p1p2)=I′\\u2061(p1)taking the derivative w.r.tp1I′\\u2061(p1p2)+p1p2I″\\u2061(p1p2)=0taking the derivative w.r.tp2I′\\u2061(u)+uI″\\u2061(u)=0introducingu=p1p2(u↦uI′\\u2061(u))′=0&\\\\operatorname (p_p_)&=\\\\ &\\\\operatorname (p_)+\\\\operatorname (p_)&&\\\\quad }\\\\\\\\&p_\\\\operatorname '(p_p_)&=\\\\ &\\\\operatorname '(p_)&&\\\\quad }\\\\ p_\\\\\\\\&\\\\operatorname '(p_p_)+p_p_\\\\operatorname ''(p_p_)&=\\\\ &0&&\\\\quad }\\\\ p_\\\\\\\\&\\\\operatorname '(u)+u\\\\operatorname ''(u)&=\\\\ &0&&\\\\quad }\\\\,u=p_p_\\\\\\\\&(u\\\\mapsto u\\\\operatorname '(u))'&=\\\\ &0\\\\end}}Thisdifferential equationleads to the solutionI\\u2061(u)=klog\\u2061u+c (u)=k\\\\log u+c}for somek,c∈R }.\", 'enProperty 2 givesc=0.', 'enProperty 1 and 2 give thatI\\u2061(p)≥0 (p)\\\\geq 0}for allp∈[0,1], so thatk<0.The differentunits of information(bitsfor thebinary logarithmlog2,natsfor thenatural logarithmln,bansfor thedecimal logarithmlog10and so on) areconstant multiplesof each other.', 'enFor instance, in case of a fair coin toss, heads provideslog2(2) = 1bit of information, which is approximately 0.693 nats or 0.301 decimal digits.', 'enBecause of additivity,ntosses providenbits of information, which is approximately0.693nnats or0.301ndecimal digits.Themeaningof the events observed (the meaning ofmessages) does not matter in the definition of entropy.', 'enEntropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlyingprobability distribution, not the meaning of the events themselves.Alternative characterization[edit]Another characterization of entropy uses the following properties.', 'enWe denotepi= Pr(X=xi)andΗn(p1, ...,pn) = Η(X).Continuity:Hshould becontinuous, so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount.Symmetry:Hshould be unchanged if the outcomesxiare re-ordered.', 'enThat is,Hn(p1,p2,…pn)=Hn(pi1,pi2,…,pin) _\\\\left(p_,p_,\\\\ldots p_\\\\right)=\\\\mathrm _\\\\left(p_},p_},\\\\ldots ,p_}\\\\right)}for anypermutation,...,i_\\\\}}of}.Maximum:Hn _}should be maximal if all the outcomes are equally likely i.e.Hn(p1,…,pn)≤Hn(1n,…,1n) _(p_,\\\\ldots ,p_)\\\\leq \\\\mathrm _\\\\left(},\\\\ldots ,}\\\\right)}.Increasing number of outcomes: for equiprobable events, the entropy should increase with the number of outcomes i.e.Hn(1n,…,1n⏟n)<Hn+1(1n+1,…,1n+1⏟n+1).', 'en_\\\\underbrace },\\\\ldots ,}} _<\\\\mathrm _\\\\underbrace },\\\\ldots ,}} _.', 'en}Additivity: given an ensemble ofnuniformly distributed elements that are partitioned intokboxes (sub-systems) withb1, ...,bkelements each, the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes, each weighted with the probability of being in that particular box.Discussion[edit]The rule of additivity has the following consequences: forpositive integersbiwhereb1+ ... +bk=n,Hn(1n,…,1n)=Hk(b1n,…,bkn)+∑i=1kbinHbi(1bi,…,1bi).', 'en_\\\\left(},\\\\ldots ,}\\\\right)=\\\\mathrm _\\\\left(}},\\\\ldots ,}}\\\\right)+\\\\sum _^}}\\\\,\\\\mathrm _}\\\\left(}},\\\\ldots ,}}\\\\right).', 'en}Choosingk=n,b1= ... =bn= 1this implies that the entropy of a certain outcome is zero:Η1(1) = 0.', 'enThis implies that the efficiency of a source alphabet withnsymbols can be defined simply as being equal to itsn-ary entropy.', 'enSee alsoRedundancy (information theory).The characterization here imposes an additive property with respect to apartition of a set.', 'enMeanwhile, theconditional probabilityis defined in terms of a multiplicative property,P(A∣B)⋅P(B)=P(A∩B).', 'enObserve that a logarithm mediates between these two operations.', 'enTheconditional entropyand related quantities inherit simple relation, in turn.', 'enThe measure theoretic definition in the previous section defined the entropy as a sum over expected surprisalsμ(A)⋅ln\\u2061μ(A)for an extremal partition.', 'enHere the logarithm is ad hoc and the entropy is not a measure in itself.', 'enAt least in the information theory of a binary string,log2}lends itself to practical interpretations.Motivated by such relations, a plethora of related and competing quantities have been defined.', 'enFor example,David Ellerman\\'s analysis of a \"logic of partitions\" defines a competing measure in structuresdualto that of subsets of a universal set.', 'en[14]Information is quantified as \"dits\" (distinctions), a measure on partitions.', \"enDits can be converted intoShannon's bits, to get the formulas for conditional entropy, and so on.Alternative characterization via additivity and subadditivity[edit]Another succinct axiomatic characterization of Shannon entropy was given byAczél, Forte and Ng,[15]via the following properties:Subadditivity:H(X,Y)≤H(X)+H(Y) (X,Y)\\\\leq \\\\mathrm (X)+\\\\mathrm (Y)}for jointly distributed random variablesX,Y.Additivity:H(X,Y)=H(X)+H(Y) (X,Y)=\\\\mathrm (X)+\\\\mathrm (Y)}when the random variablesX,Yare independent.Expansibility:Hn+1(p1,…,pn,0)=Hn(p1,…,pn) _(p_,\\\\ldots ,p_,0)=\\\\mathrm _(p_,\\\\ldots ,p_)}, i.e., adding an outcome with probability zero does not change the entropy.Symmetry:Hn(p1,…,pn) _(p_,\\\\ldots ,p_)}is invariant under permutation ofp1,…,pn,\\\\ldots ,p_}.Small for small probabilities:limq→0+H2(1−q,q)=0}\\\\mathrm _(1-q,q)=0}.Discussion[edit]It was shown that any functionH }satisfying the above properties must be a constant multiple of Shannon entropy, with a non-negative constant.\", 'en[15]Compared to the previously mentioned characterizations of entropy, this characterization focuses on the properties of entropy as a function of random variables (subadditivity and additivity), rather than the properties of entropy as a function of the probability vectorp1,…,pn,\\\\ldots ,p_}.It is worth noting that if we drop the \"small for small probabilities\" property, thenH }must be a non-negative linear combination of the Shannon entropy and theHartley entropy.', 'en[15]Further properties[edit]The Shannon entropy satisfies the following properties, for some of which it is useful to interpret entropy as the expected amount of information learned (or uncertainty eliminated) by revealing the value of a random variableX:Adding or removing an event with probability zero does not contribute to the entropy:Hn+1(p1,…,pn,0)=Hn(p1,…,pn) _(p_,\\\\ldots ,p_,0)=\\\\mathrm _(p_,\\\\ldots ,p_)}.The maximal entropy of an event withndifferent outcomes islogb(n): it is attained by the uniform probability distribution.', 'enThat is, uncertainty is maximal when all possible events are equiprobable:H(p1,…,pn)≤logb\\u2061n (p_,\\\\dots ,p_)\\\\leq \\\\log _n}.', 'en[10]: 29The entropy or the amount of information revealed by evaluating(X,Y)(that is, evaluatingXandYsimultaneously) is equal to the information revealed by conducting two consecutive experiments: first evaluating the value ofY, then revealing the value ofXgiven that you know the value ofY.', 'enThis may be written as:[10]: 16H(X,Y)=H(X|Y)+H(Y)=H(Y|X)+H(X).', 'en(X,Y)=\\\\mathrm (X|Y)+\\\\mathrm (Y)=\\\\mathrm (Y|X)+\\\\mathrm (X).', 'en}IfY=f(X)wherefis a function, thenH(f(X)|X)=0 (f(X)|X)=0}.', \"enApplying the previous formula toH(X,f(X)) (X,f(X))}yieldsH(X)+H(f(X)|X)=H(f(X))+H(X|f(X)), (X)+\\\\mathrm (f(X)|X)=\\\\mathrm (f(X))+\\\\mathrm (X|f(X)),}soH(f(X))≤H(X) (f(X))\\\\leq \\\\mathrm (X)}, the entropy of a variable can only decrease when the latter is passed through a function.IfXandYare two independent random variables, then knowing the value ofYdoesn't influence our knowledge of the value ofX(since the two don't influence each other by independence):H(X|Y)=H(X).\", 'en(X|Y)=\\\\mathrm (X).', 'en}More generally, for any random variablesXandY, we haveH(X|Y)≤H(X) (X|Y)\\\\leq \\\\mathrm (X)}.', 'en[10]: 29The entropy of two simultaneous events is no more than the sum of the entropies of each individual event i.e.,H(X,Y)≤H(X)+H(Y) (X,Y)\\\\leq \\\\mathrm (X)+\\\\mathrm (Y)}, with equality if and only if the two events are independent.', 'en[10]: 28The entropyH(p) (p)}isconcavein the probability mass functionp, i.e.', 'en[10]: 30H(λp1+(1−λ)p2)≥λH(p1)+(1−λ)H(p2) (\\\\lambda p_+(1-\\\\lambda )p_)\\\\geq \\\\lambda \\\\mathrm (p_)+(1-\\\\lambda )\\\\mathrm (p_)}for all probability mass functionsp1,p2,p_}and0≤λ≤1.', \"en[10]: 32Accordingly, thenegative entropy(negentropy) function is convex, and itsconvex conjugateisLogSumExp.Aspects[edit]Relationship to thermodynamic entropy[edit]Main article:Entropy in thermodynamics and information theoryThe inspiration for adopting the wordentropyin information theory came from the close resemblance between Shannon's formula and very similar known formulae fromstatistical mechanics.Instatistical thermodynamicsthe most general formula for the thermodynamicentropySof athermodynamic systemis theGibbs entropyS=−kB∑piln\\u2061pi,}\\\\sum p_\\\\ln p_\\\\,,}wherekBis theBoltzmann constant, andpiis the probability of amicrostate.\", 'enTheGibbs entropywas defined byJ.', 'enWillard Gibbsin 1878 after earlier work byBoltzmann(1872).', 'en[16]The Gibbs entropy translates over almost unchanged into the world ofquantum physicsto give thevon Neumann entropyintroduced byJohn von Neumannin 1927:S=−kBTr(ρln\\u2061ρ),}\\\\,}(\\\\rho \\\\ln \\\\rho )\\\\,,}where ρ is thedensity matrixof the quantum mechanical system and Tr is thetrace.', 'en[17]At an everyday practical level, the links between information entropy and thermodynamic entropy are not evident.', 'enPhysicists and chemists are apt to be more interested inchangesin entropy as a system spontaneously evolves away from its initial conditions, in accordance with thesecond law of thermodynamics, rather than an unchanging probability distribution.', 'enAs the minuteness of theBoltzmann constantkBindicates, the changes inS/kBfor even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything indata compressionorsignal processing.', 'enIn classical thermodynamics, entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution, which is central to the definition of information entropy.The connection between thermodynamics and what is now known as information theory was first made byLudwig Boltzmannand expressed by hisfamous equation:S=kBln\\u2061W,}\\\\ln W,}whereSis the thermodynamic entropy of a particular macrostate (defined by thermodynamic parameters such as temperature, volume, energy, etc.', 'en),Wis the number of microstates (various combinations of particles in various energy states) that can yield the given macrostate, andkBis theBoltzmann constant.', 'en[18]It is assumed that each microstate is equally likely, so that the probability of a given microstate ispi= 1/W.', \"enWhen these probabilities are substituted into the above expression for the Gibbs entropy (or equivalentlykBtimes the Shannon entropy), Boltzmann's equation results.\", 'enIn information theoretic terms, the information entropy of a system is the amount of \"missing\" information needed to determine a microstate, given the macrostate.In the view ofJaynes(1957),[19]thermodynamic entropy, as explained bystatistical mechanics, should be seen as anapplicationof Shannon\\'s information theory: the thermodynamic entropy is interpreted as being proportional to the amount of further Shannon information needed to define the detailed microscopic state of the system, that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics, with the constant of proportionality being just theBoltzmann constant.', 'enAdding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables, making any complete state description longer.', \"en(See article:maximum entropy thermodynamics).Maxwell's demoncan (hypothetically) reduce the thermodynamic entropy of a system by using information about the states of individual molecules; but, asLandauer(from 1961) and co-workers[20]have shown, to function the demon himself must increase thermodynamic entropy in the process, by at least the amount of Shannon information he proposes to first acquire and store; and so the total thermodynamic entropy does not decrease (which resolves the paradox).Landauer's principleimposes a lower bound on the amount of heat a computer must generate to process a given amount of information, though modern computers are far less efficient.Data compression[edit]Main articles:Shannon's source coding theoremandData compressionShannon's definition of entropy, when applied to an information source, can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits.\", \"enShannon's entropy measures the information contained in a message as opposed to the portion of the message that is determined (or predictable).\", 'enExamples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs, triplets etc.', 'enThe minimum channel capacity can be realized in theory by using thetypical setor in practice usingHuffman,Lempel–Zivorarithmetic coding.', 'en(See alsoKolmogorov complexity.)', 'enIn practice, compression algorithms deliberately include some judicious redundancy in the form ofchecksumsto protect against errors.', 'enTheentropy rateof a data source is the average number of bits per symbol needed to encode it.', \"enShannon's experiments with human predictors show an information rate between 0.6 and 1.3 bits per character in English;[21]thePPM compression algorithmcan achieve a compression ratio of 1.5 bits per character in English text.If acompressionscheme is lossless – one in which you can always recover the entire original message by decompression – then a compressed message has the same quantity of information as the original but communicated in fewer characters.\", 'enIt has more information (higher entropy) per character.', \"enA compressed message has lessredundancy.Shannon's source coding theoremstates a lossless compression scheme cannot compress messages, on average, to havemorethan one bit of information per bit of message, but that any valuelessthan one bit of information per bit of message can be attained by employing a suitable coding scheme.\", 'enThe entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains.', \"enShannon's theorem also implies that no lossless compression scheme can shortenallmessages.\", 'enIf some messages come out shorter, at least one must come out longer due to thepigeonhole principle.', \"enIn practical use, this is generally not a problem, because one is usually only interested in compressing certain types of messages, such as a document in English, as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger.A 2011 study inScienceestimates the world's technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007, therefore estimating the entropy of the technologically available sources.\", 'en[22]: 60–65All figures in entropically compressedexabytesType of Information19862007Storage2.6295Broadcast4321900Telecommunications0.28165The authors estimate humankind technological capacity to store information (fully entropically compressed) in 1986 and again in 2007.', 'enThey break the information into three categories—to store information on a medium, to receive information through one-waybroadcastnetworks, or to exchange information through two-waytelecommunicationnetworks.', 'en[22]Entropy as a measure of diversity[edit]Main article:Diversity indexEntropy is one of several ways to measure biodiversity, and is applied in the form of theShannon index.', 'en[23]A diversity index is a quantitative statistical measure of how many different types exist in a dataset, such as species in a community, accounting for ecologicalrichness,evenness, anddominance.', 'enSpecifically, Shannon entropy is the logarithm of1D, thetrue diversityindex with parameter equal to 1.', 'enThe Shannon index is related to the proportional abundances of types.Entropy of a sequence[edit]There are a number of entropy-related concepts that mathematically quantify information content of a sequence or message:theself-informationof an individual message or symbol taken from a given probability distribution (message or sequence see as an individual event),thejoint entropyof the symbols forming the message or sequence (seen as a set of events),theentropy rateof astochastic process(message or sequence is seen as a succession of events).', 'en(The \"rate of self-information\" can also be defined for a particular sequence of messages or symbols generated by a given stochastic process: this will always be equal to the entropy rate in the case of astationary process.)', 'enOtherquantities of informationare also used to compare or relate different sources of information.It is important not to confuse the above concepts.', 'enOften it is only clear from context which one is meant.', 'enFor example, when someone says that the \"entropy\" of the English language is about 1 bit per character, they are actually modeling the English language as a stochastic process and talking about its entropyrate.', 'enShannon himself used the term in this way.If very large blocks are used, the estimate of per-character entropy rate may become artificially low because the probability distribution of the sequence is not known exactly; it is only an estimate.', 'enIf one considers the text of every book ever published as a sequence, with each symbol being the text of a complete book, and if there areNpublished books, and each book is only published once, the estimate of the probability of each book is1/N, and the entropy (in bits) is−log2(1/N) = log2(N).', 'enAs a practical code, this corresponds to assigning each book aunique identifierand using it in place of the text of the book whenever one wants to refer to the book.', 'enThis is enormously useful for talking about books, but it is not so useful for characterizing the information content of an individual book, or of language in general: it is not possible to reconstruct the book from its identifier without knowing the probability distribution, that is, the complete text of all the books.', 'enThe key idea is that the complexity of the probabilistic model must be considered.Kolmogorov complexityis a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model; it considers the shortestprogramfor auniversal computerthat outputs the sequence.', 'enA code that achieves the entropy rate of a sequence for a given model, plus the codebook (i.e.', 'enthe probabilistic model), is one such program, but it may not be the shortest.The Fibonacci sequence is 1, 1, 2, 3, 5, 8, 13, .... treating the sequence as a message and each number as a symbol, there are almost as many symbols as there are characters in the message, giving an entropy of approximatelylog2(n).', 'enThe first 128 symbols of the Fibonacci sequence has an entropy of approximately 7 bits/symbol, but the sequence can be expressed using a formula [F(n) = F(n−1) + F(n−2)forn= 3, 4, 5, ...,F(1) =1,F(2) = 1] and this formula has a much lower entropy and applies to any length of the Fibonacci sequence.Limitations of entropy in cryptography[edit]Incryptanalysis, entropy is often roughly used as a measure of the unpredictability of a cryptographic key, though its realuncertaintyis unmeasurable.', 'enFor example, a 128-bit key that is uniformly and randomly generated has 128 bits of entropy.', 'enIt also takes (on average)2127}guesses to break by brute force.', 'enEntropy fails to capture the number of guesses required if the possible keys are not chosen uniformly.', 'en[24][25]Instead, a measure calledguessworkcan be used to measure the effort required for a brute force attack.', 'en[26]Other problems may arise from non-uniform distributions used in cryptography.', 'enFor example, a 1,000,000-digit binaryone-time padusing exclusive or.', 'enIf the pad has 1,000,000 bits of entropy, it is perfect.', 'enIf the pad has 999,999 bits of entropy, evenly distributed (each individual bit of the pad having 0.999999 bits of entropy) it may provide good security.', 'enBut if the pad has 999,999 bits of entropy, where the first bit is fixed and the remaining 999,999 bits are perfectly random, the first bit of the ciphertext will not be encrypted at all.Data as a Markov process[edit]A common way to define entropy for text is based on theMarkov modelof text.', 'enFor an order-0 source (each character is selected independent of the last characters), the binary entropy is:H(S)=−∑pilog\\u2061pi, (})=-\\\\sum p_\\\\log p_,}wherepiis the probability ofi.', 'enFor a first-orderMarkov source(one in which the probability of selecting a character is dependent only on the immediately preceding character), theentropy rateis:H(S)=−∑ipi∑jpi(j)log\\u2061pi(j), (})=-\\\\sum _p_\\\\sum _\\\\ p_(j)\\\\log p_(j),}[citation needed]whereiis astate(certain preceding characters) andpi(j)(j)}is the probability ofjgivenias the previous character.For a second order Markov source, the entropy rate isH(S)=−∑ipi∑jpi(j)∑kpi,j(k)log\\u2061pi,j(k).', 'en(})=-\\\\sum _p_\\\\sum _p_(j)\\\\sum _p_(k)\\\\ \\\\log \\\\ p_(k).', 'en}Efficiency (normalized entropy)[edit]A source alphabet with non-uniform distribution will have less entropy than if those symbols had uniform distribution (i.e.', 'enthe \"optimized alphabet\").', 'enThis deficiency in entropy can be expressed as a ratio called efficiency:[27]η(X)=HHmax=−∑i=1np(xi)logb\\u2061(p(xi))logb\\u2061(n).', 'en}}=-\\\\sum _^)\\\\log _(p(x_))}(n)}}.', 'en}Applying the basic properties of the logarithm, this quantity can also be expressed as:η(X)=−∑i=1np(xi)logb\\u2061(p(xi))logb\\u2061(n)=∑i=1nlogb\\u2061(p(xi)−p(xi))logb\\u2061(n)=∑i=1nlogn\\u2061(p(xi)−p(xi))=logn\\u2061(∏i=1np(xi)−p(xi)).^)\\\\log _(p(x_))}(n)}}=\\\\sum _^(p(x_)^)})}(n)}}=\\\\sum _^\\\\log _(p(x_)^)})=\\\\log _(\\\\prod _^p(x_)^)}).', 'en}Efficiency has utility in quantifying the effective use of acommunication channel.', 'enThis formulation is also referred to as the normalized entropy, as the entropy is divided by the maximum entropylogb\\u2061(n)(n)}}.', 'enFurthermore, the efficiency is indifferent to choice of (positive) baseb, as indicated by the insensitivity within the final logarithm above thereto.Entropy for continuous random variables[edit]Differential entropy[edit]Main article:Differential entropyThe Shannon entropy is restricted to random variables taking discrete values.', 'enThe corresponding formula for a continuous random variable withprobability density functionf(x)with finite or infinite supportX }on the real line is defined by analogy, using the above form of the entropy as an expectation:[10]: 224H(X)=E[−log\\u2061f(X)]=−∫Xf(x)log\\u2061f(x)dx.', 'en(X)=\\\\mathbb [-\\\\log f(X)]=-\\\\int _ }f(x)\\\\log f(x)\\\\,\\\\mathrm x.', 'en}This is the differential entropy (or continuous entropy).', 'enA precursor of the continuous entropyh[f]is the expression for the functionalΗin theH-theoremofBoltzmann.Although the analogy between both functions is suggestive, the following question must be set: is the differential entropy a valid extension of the Shannon discrete entropy?', 'enDifferential entropy lacks a number of properties that the Shannon discrete entropy has – it can even be negative – and corrections have been suggested, notablylimiting density of discrete points.To answer this question, a connection must be established between the two functions:In order to obtain a generally finite measure as thebin sizegoes to zero.', 'enIn the discrete case, the bin size is the (implicit) width of each of then(finite or infinite) bins whose probabilities are denoted bypn.', 'enAs the continuous domain is generalized, the width must be made explicit.To do this, start with a continuous functionfdiscretized into bins of sizeΔ.', 'enBy the mean-value theorem there exists a valuexiin each bin such thatf(xi)Δ=∫iΔ(i+1)Δf(x)dx)\\\\Delta =\\\\int _^f(x)\\\\,dx}the integral of the functionfcan be approximated (in the Riemannian sense) by∫−∞∞f(x)dx=limΔ→0∑i=−∞∞f(xi)Δ,^f(x)\\\\,dx=\\\\lim _\\\\sum _^f(x_)\\\\Delta ,}where this limit and \"bin size goes to zero\" are equivalent.We will denoteHΔ:=−∑i=−∞∞f(xi)Δlog\\u2061(f(xi)Δ) ^:=-\\\\sum _^f(x_)\\\\Delta \\\\log \\\\left(f(x_)\\\\Delta \\\\right)}and expanding the logarithm, we haveHΔ=−∑i=−∞∞f(xi)Δlog\\u2061(f(xi))−∑i=−∞∞f(xi)Δlog\\u2061(Δ).', 'en^=-\\\\sum _^f(x_)\\\\Delta \\\\log(f(x_))-\\\\sum _^f(x_)\\\\Delta \\\\log(\\\\Delta ).', 'en}AsΔ → 0, we have∑i=−∞∞f(xi)Δ→∫−∞∞f(x)dx=1∑i=−∞∞f(xi)Δlog\\u2061(f(xi))→∫−∞∞f(x)log\\u2061f(x)dx.\\\\sum _^f(x_)\\\\Delta &\\\\to \\\\int _^f(x)\\\\,dx=1\\\\\\\\\\\\sum _^f(x_)\\\\Delta \\\\log(f(x_))&\\\\to \\\\int _^f(x)\\\\log f(x)\\\\,dx.\\\\end}}Note;log(Δ) → −∞asΔ → 0, requires a special definition of the differential or continuous entropy:h[f]=limΔ→0(HΔ+log\\u2061Δ)=−∫−∞∞f(x)log\\u2061f(x)dx,\\\\left(\\\\mathrm ^+\\\\log \\\\Delta \\\\right)=-\\\\int _^f(x)\\\\log f(x)\\\\,dx,}which is, as said before, referred to as the differential entropy.', 'enThis means that the differential entropyis nota limit of the Shannon entropy forn→ ∞.', 'enRather, it differs from the limit of the Shannon entropy by an infinite offset (see also the article oninformation dimension).Limiting density of discrete points[edit]Main article:Limiting density of discrete pointsIt turns out as a result that, unlike the Shannon entropy, the differential entropy isnotin general a good measure of uncertainty or information.', 'enFor example, the differential entropy can be negative; also it is not invariant under continuous co-ordinate transformations.', 'enThis problem may be illustrated by a change of units whenxis a dimensioned variable.f(x)will then have the units of1/x.', 'enThe argument of the logarithm must be dimensionless, otherwise it is improper, so that the differential entropy as given above will be improper.', 'enIfΔis some \"standard\" value ofx(i.e.', 'enbin size) and therefore has the same units, then a modified differential entropy may be written in proper form as:H=∫−∞∞f(x)log\\u2061(f(x)Δ)dx, =\\\\int _^f(x)\\\\log(f(x)\\\\,\\\\Delta )\\\\,dx,}and the result will be the same for any choice of units forx.', 'enIn fact, the limit of discrete entropy asN→∞would also include a term oflog\\u2061(N), which would in general be infinite.', 'enThis is expected: continuous variables would typically have infinite entropy when discretized.', 'enThelimiting density of discrete pointsis really a measure of how much easier a distribution is to describe than a distribution that is uniform over its quantization scheme.Relative entropy[edit]Main article:Generalized relative entropyAnother useful measure of entropy that works equally well in the discrete and the continuous case is therelative entropyof a distribution.', 'enIt is defined as theKullback–Leibler divergencefrom the distribution to a reference measuremas follows.', 'enAssume that a probability distributionpisabsolutely continuouswith respect to a measurem, i.e.', 'enis of the formp(dx) =f(x)m(dx)for some non-negativem-integrable functionfwithm-integral 1, then the relative entropy can be defined asDKL(p‖m)=∫log\\u2061(f(x))p(dx)=∫f(x)log\\u2061(f(x))m(dx).', 'en}(p\\\\|m)=\\\\int \\\\log(f(x))p(dx)=\\\\int f(x)\\\\log(f(x))m(dx).', 'en}In this form the relative entropy generalizes (up to change in sign) both the discrete entropy, where the measuremis thecounting measure, and the differential entropy, where the measuremis theLebesgue measure.', 'enIf the measuremis itself a probability distribution, the relative entropy is non-negative, and zero ifp=mas measures.', 'enIt is defined for any measure space, hence coordinate independent and invariant under co-ordinate reparameterizations if one properly takes into account the transformation of the measurem.', 'enThe relative entropy, and (implicitly) entropy and differential entropy, do depend on the \"reference\" measurem.Use in number theory[edit]Terence Taoused entropy to make a useful connection trying to solve theErdős discrepancy problem.', 'en[28]Intuitively the idea behind the proof was if there is low information in terms of the Shannon entropy between consecutive random variables (here the random variable is defined using theLiouville function(which is a useful mathematical function for studying distribution of primes)XH=λ(n+H).', 'enAnd in an interval [n, n+H] the sum over that interval could become arbitrary large.', \"enFor example, a sequence of +1's (which are values ofXH'could take) have trivially low entropy and their sum would become big.\", 'enBut the key insight was showing a reduction in entropy by non negligible amounts as one expands H leading inturn to unbounded growth of a mathematical object over this random variable is equivalent to showing the unbounded growth per theErdős discrepancy problem.The proof is quite involved and it bought together breakthroughs not just in novel use of Shannon Entropy, but also its used theLiouville functionalong withaverages of modulated multiplicative functionsin short intervals.', 'enProving it also broke the\"parity barrier\"for this specific problem.While the use of Shannon Entropy in the proof is novel it is likely to open new research in this direction.Use in combinatorics[edit]Entropy has become a useful quantity incombinatorics.Loomis–Whitney inequality[edit]A simple example of this is an alternative proof of theLoomis–Whitney inequality: for every subsetA⊆Zd, we have|A|d−1≤∏i=1d|Pi(A)|\\\\leq \\\\prod _^|P_(A)|}wherePiis theorthogonal projectionin theith coordinate:Pi(A)=.', 'en(A)=\\\\,\\\\ldots ,x_,x_,\\\\ldots ,x_):(x_,\\\\ldots ,x_)\\\\in A\\\\}.', \"en}The proof follows as a simple corollary ofShearer's inequality: ifX1, ...,Xdare random variables andS1, ...,Snare subsets of such that every integer between 1 anddlies in exactlyrof these subsets, thenH[(X1,…,Xd)]≤1r∑i=1nH[(Xj)j∈Si] [(X_,\\\\ldots ,X_)]\\\\leq }\\\\sum _^\\\\mathrm [(X_)_}]}where(Xj)j∈Si)_}}is the Cartesian product of random variablesXjwith indexesjinSi(so the dimension of this vector is equal to the size ofSi).We sketch how Loomis–Whitney follows from this: Indeed, letXbe a uniformly distributed random variable with values inAand so that each point inAoccurs with equal probability.\", 'enThen (by the further properties of entropy mentioned above)Η(X) = log|A|, where|A|denotes the cardinality ofA.', 'enLetSi= .', 'enThe range of(Xj)j∈Si)_}}is contained inPi(A)and henceH[(Xj)j∈Si]≤log\\u2061|Pi(A)| [(X_)_}]\\\\leq \\\\log |P_(A)|}.', \"enNow use this to bound the right side of Shearer's inequality and exponentiate the opposite sides of the resulting inequality you obtain.Approximation to binomial coefficient[edit]For integers0 <k<nletq=k/n.\", 'enThen2nH(q)n+1≤(nk)≤2nH(q), (q)}}}\\\\leq }\\\\leq 2^ (q)},}whereH(q)=−qlog2\\u2061(q)−(1−q)log2\\u2061(1−q).', 'en(q)=-q\\\\log _(q)-(1-q)\\\\log _(1-q).', 'en}[29]: 43Proof (sketch)Note that(nk)qqn(1−q)n−nq}q^(1-q)^}is one term of the expression∑i=0n(ni)qi(1−q)n−i=(q+(1−q))n=1.^}q^(1-q)^=(q+(1-q))^=1.', 'en}Rearranging gives the upper bound.', 'enFor the lower bound one first shows, using some algebra, that it is the largest term in the summation.', 'enBut then,(nk)qqn(1−q)n−nq≥1n+1}q^(1-q)^\\\\geq }}since there aren+ 1terms in the summation.', \"enRearranging gives the lower bound.A nice interpretation of this is that the number of binary strings of lengthnwith exactlykmany 1's is approximately2nH(k/n) (k/n)}}.\", 'en[30]Use in machine learning[edit]Machine learningtechniques arise largely from statistics and also information theory.', 'enIn general, entropy is a measure of uncertainty and the objective of machine learning is to minimize uncertainty.Decision tree learningalgorithms use relative entropy to determine the decision rules that govern the data at each node.', 'en[31]Theinformation gain in decision treesIG(Y,X), which is equal to the difference between the entropy ofYand the conditional entropy ofYgivenX, quantifies the expected information, or the reduction in entropy, from additionally knowing the value of an attributeX.', 'enThe information gain is used to identify which attributes of the dataset provide the most information and should be used to split the nodes of the tree optimally.Bayesian inferencemodels often apply theprinciple of maximum entropyto obtainprior probabilitydistributions.', 'en[32]The idea is that the distribution that best represents the current state of knowledge of a system is the one with the largest entropy, and is therefore suitable to be the prior.Classification in machine learningperformed bylogistic regressionorartificial neural networksoften employs a standard loss function, calledcross-entropyloss, that minimizes the average cross entropy between ground truth and predicted distributions.', 'en[33]In general, cross entropy is a measure of the differences between two datasets similar to the KL divergence (also known as relative entropy).See also[edit]Mathematics portalApproximate entropy(ApEn)Entropy (thermodynamics)Cross entropy– is a measure of the average number of bits needed to identify an event from a set of possibilities between two probability distributionsEntropy (arrow of time)Entropy encoding– a coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols.Entropy estimationEntropy power inequalityFisher informationGraph entropyHamming distanceHistory of entropyHistory of information theoryInformation fluctuation complexityInformation geometryKolmogorov–Sinai entropyindynamical systemsLevenshtein distanceMutual informationPerplexityQualitative variation– other measures ofstatistical dispersionfornominal distributionsQuantum relative entropy– a measure of distinguishability between two quantum states.Rényi entropy– a generalization of Shannon entropy; it is one of a family of functionals for quantifying the diversity, uncertainty or randomness of a system.RandomnessSample entropy(SampEn)Shannon indexTheil indexTypoglycemiaReferences[edit]^Pathria, R. K.; Beale, Paul (2011).Statistical Mechanics(Third ed.).', 'enAcademic Press.', 'enp. 51.ISBN978-0123821881.^abShannon, Claude E.(July 1948).', 'enA Mathematical Theory of Communication.Bell System Technical Journal.27(3): 379–423.doi:10.1002/j.1538-7305.1948.tb01338.x.hdl:10338.dmlcz/101429.', 'en(PDF, archived fromhere)^abShannon, Claude E.(October 1948).', 'enA Mathematical Theory of Communication.Bell System Technical Journal.27(4): 623–656.doi:10.1002/j.1538-7305.1948.tb00917.x.hdl:11858/00-001M-0000-002C-4317-B.', 'en(PDF, archived fromhere)^\"Entropy (for data science) Clearly Explained!!!', 'en– viaYouTube.^MacKay, David J.C.(2003).Information Theory, Inference, and Learning Algorithms.\\ten\\nCambridge University Press.ISBN0-521-64298-1.^Schneier, B:Applied Cryptography, Second edition, John Wiley and Sons.^Borda, Monica (2011).Fundamentals in Information Theory and Coding.\\ten\\nSpringer.ISBN978-3-642-20346-6.^Han, Te Sun; Kobayashi, Kingo (2002).Mathematics of Information and Coding.\\ten\\nAmerican Mathematical Society.ISBN978-0-8218-4256-0.^Schneider, T.D,Information theory primer with an appendix on logarithms[permanent dead link], National Cancer Institute, 14 April 2007.^abcdefghijkThomas M. Cover; Joy A. Thomas (1991).Elements of Information Theory.\\ten\\nHoboken, New Jersey: Wiley.ISBN978-0-471-24195-9.^Entropyat thenLab^Carter, Tom (March 2014).An introduction to information theory and entropy(PDF).\\ten\\nSanta Fe.\\ten\\nRetrieved4 August2017.\\ten\\n}: CS1 maint: location missing publisher (link)^Chakrabarti, C. G., and Indranil Chakrabarty.\\ten\\nShannon entropy: axiomatic characterization and application.', 'enInternational Journal of Mathematics and Mathematical Sciences2005.\\ten\\n17 (2005): 2847-2854url^Ellerman, David (October 2017).\\ten\\nLogical Information Theory: New Logical Foundations for Information Theory\"(PDF).Logic Journal of the IGPL.25(5): 806–835.doi:10.1093/jigpal/jzx022.', 'enRetrieved2 November2022.^abcAczél, J.; Forte, B.; Ng, C. T. (1974).', \"enWhy the Shannon and Hartley entropies are 'natural'.Advances in Applied Probability.6(1): 131–146.doi:10.2307/1426210.JSTOR1426210.S2CID204177762.^Compare: Boltzmann, Ludwig (1896, 1898).\", 'enVorlesungen über Gastheorie : 2 Volumes – Leipzig 1895/98 UB: O 5262-6.', 'enEnglish version: Lectures on gas theory.', 'enTranslated by Stephen G. Brush (1964) Berkeley: University of California Press; (1995) New York: DoverISBN0-486-68455-5^Życzkowski, Karol (2006).Geometry of Quantum States: An Introduction to Quantum Entanglement.', 'enCambridge University Press.', 'enp. 301.^Sharp, Kim; Matschinsky, Franz (2015).', 'enTranslation of Ludwig Boltzmann\\'s Paper On the Relationship between the Second Fundamental Theorem of the Mechanical Theory of Heat and Probability Calculations Regarding the Conditions for Thermal Equilibrium\"\".Entropy.17: 1971–2009.doi:10.3390/e17041971.^Jaynes, E. T. (15 May 1957).', 'enInformation Theory and Statistical Mechanics.Physical Review.106(4): 620–630.Bibcode:1957PhRv..106..620J.doi:10.1103/PhysRev.106.620.S2CID17870175.^Landauer, R. (July 1961).', 'enIrreversibility and Heat Generation in the Computing Process.IBM Journal of Research and Development.5(3): 183–191.doi:10.1147/rd.53.0183.ISSN0018-8646.^Mark Nelson (24 August 2006).', 'enThe Hutter Prize.', 'enArchived fromthe originalon 1 March 2018.', 'enRetrieved27 November2008.^ab\"The World\\'s Technological Capacity to Store, Communicate, and Compute Information\", Martin Hilbert and Priscila López (2011),Science, 332(6025); free access to the article through here: martinhilbert.net/WorldInfoCapacity.html^Spellerberg, Ian F.; Fedor, Peter J.', 'en(2003).', \"enA tribute to Claude Shannon (1916–2001) and a plea for more rigorous use of species richness, species diversity and the 'Shannon–Wiener' Index.Global Ecology and Biogeography.12(3): 177–179.Bibcode:2003GloEB..12..177S.doi:10.1046/j.1466-822X.2003.00015.x.ISSN1466-8238.S2CID85935463.^Massey, James (1994).\", 'enGuessing and Entropy(PDF).Proc.', 'enIEEE International Symposium on Information Theory.', 'enRetrieved31 December2013.^Malone, David; Sullivan, Wayne (2005).', 'enGuesswork is not a Substitute for Entropy(PDF).Proceedings of the Information Technology & Telecommunications Conference.', 'enRetrieved31 December2013.^Pliam, John (1999).', 'enSelected Areas in Cryptography.International Workshop on Selected Areas in Cryptography.', 'enLecture Notes in Computer Science.', 'enVol.', 'en1758. pp.', 'en62–77.doi:10.1007/3-540-46513-8_5.ISBN978-3-540-67185-5.^Indices of Qualitative Variation.', 'enAR Wilcox - 1967https://www.osti.gov/servlets/purl/4167340^Tao, Terence (28 February 2016).', 'enThe Erdős discrepancy problem.Discrete Analysis.arXiv:1509.05363v6.doi:10.19086/da.609.S2CID59361755.^Aoki, New Approaches to Macroeconomic Modeling.^Probability and Computing, M. Mitzenmacher and E. Upfal, Cambridge University Press^Batra, Mridula; Agrawal, Rashmi (2018).', 'enComparative Analysis of Decision Tree Algorithms.', 'enIn Panigrahi, Bijaya Ketan; Hoda, M. N.; Sharma, Vinod; Goel, Shivendra (eds.', 'en).Nature Inspired Computing.', 'enAdvances in Intelligent Systems and Computing.', 'enVol.', 'en652.', 'enSingapore: Springer.', 'enpp.', 'en31–36.doi:10.1007/978-981-10-6747-1_4.ISBN978-981-10-6747-1.^Jaynes, Edwin T. (September 1968).', 'enPrior Probabilities.IEEE Transactions on Systems Science and Cybernetics.4(3): 227–241.doi:10.1109/TSSC.1968.300117.ISSN2168-2887.^Rubinstein, Reuven Y.; Kroese, Dirk P. (9 March 2013).The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning.', \"enSpringer Science & Business Media.ISBN978-1-4757-4321-0.This article incorporates material from Shannon's entropy onPlanetMath, which is licensed under theCreative Commons Attribution/Share-Alike License.Further reading[edit]Textbooks on information theory[edit]Cover, T.M.,Thomas, J.A.\", 'en(2006),Elements of Information Theory – 2nd Ed., Wiley-Interscience,ISBN978-0-471-24195-9MacKay, D.J.C.', 'en(2003),Information Theory, Inference and Learning Algorithms, Cambridge University Press,ISBN978-0-521-64298-9Arndt, C. (2004),Information Measures: Information and its Description in Science and Engineering, Springer,ISBN978-3-540-40855-0Gray, R. M. (2011),Entropy and Information Theory, Springer.Martin, Nathaniel F.G.; England, James W. (2011).Mathematical Theory of Entropy.', 'enCambridge University Press.ISBN978-0-521-17738-2.Shannon, C.E.,Weaver, W.(1949)The Mathematical Theory of Communication, Univ of Illinois Press.ISBN0-252-72548-4Stone, J. V. (2014), Chapter 1 ofInformation Theory: A Tutorial Introduction, University of Sheffield, England.ISBN978-0956372857.External links[edit]Wikibooks has a book on the topic of:An Intuitive Guide to the Concept of Entropy Arising in Various Sectors of ScienceLibrary resourcesaboutEntropy (information theory)Online booksResources in your libraryResources in other libraries\"Entropy\",Encyclopedia of Mathematics,EMS Press, 2001 [1994]\"Entropy\"atRosetta Code—repository of implementations of Shannon entropy in different programming languages.Entropyan interdisciplinary journal on all aspects of the entropy concept.', 'enOpen access.vteData compressionmethodsLosslessEntropy typeAdaptive codingArithmeticAsymmetric numeral systemsGolombHuffmanAdaptiveCanonicalModifiedRangeShannonShannon–FanoShannon–Fano–EliasTunstallUnaryUniversalExp-GolombFibonacciGammaLevenshteinDictionary typeByte pair encodingLempel–Ziv842LZ4LZJBLZOLZRWLZSSLZWLZWLSnappyOther typesBWTCTWCMDeltaIncrementalDMCDPCMGrammarRe-PairSequiturLDCTMTFPAQPPMRLEHybridLZ77 + HuffmanDeflateLZXLZSLZ77 + ANSLZFSELZ77 + Huffman + ANSZstandardLZ77 + Huffman + contextBrotliLZSS + HuffmanLHA/LZHLZ77 + RangeLZMALZHAMbzip2(RLE + BWT + MTF + Huffman)LossyTransform typeDiscrete cosine transformDCTMDCTDSTFFTWaveletDaubechiesDWTSPIHTPredictive typeDPCMADPCMLPCACELPCELPLARLSPWLPCMotionCompensationEstimationVectorPsychoacousticAudioConceptsBit rateABRCBRVBRCompandingConvolutionDynamic rangeLatencyNyquist–Shannon theoremSamplingSilence compressionSound qualitySpeech codingSub-band codingCodecpartsA-lawμ-lawDPCMADPCMDMFTFFTLPCACELPCELPLARLSPWLPCMDCTPsychoacoustic modelImageConceptsChroma subsamplingCoding tree unitColor spaceCompression artifactImage resolutionMacroblockPixelPSNRQuantizationStandard test imageTexture compressionMethodsChain codeDCTDeflateFractalKLTLPRLEWaveletDaubechiesDWTEZWSPIHTVideoConceptsBit rateABRCBRVBRDisplay resolutionFrameFrame rateFrame typesInterlaceVideo characteristicsVideo qualityCodecpartsDCTDPCMDeblocking filterLapped transformMotionCompensationEstimationVectorWaveletDaubechiesDWTTheoryCompressed data structuresCompressed suffix arrayFM-indexEntropyInformation theoryTimelineKolmogorov complexityPrefix codeQuantizationRate–distortionRedundancySymmetrySmallest grammar problemCommunityHutter PrizeGlobal Data Compression Competitionencode.suPeopleMatt MahoneyMark AdlerCompression formatsCompression software(codecs)Authority control databasesInternationalFASTNationalSpainFranceBnF dataGermanyIsraelUnited StatesJapanCzech RepublicRetrieved from \"https://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&oldid=1218357702\"Categories:Data compressionEntropy and informationInformation theoryStatistical randomnessComplex systems theoryHidden categories:All articles with dead external linksArticles with dead external links from August 2023Articles with permanently dead external linksCS1 maint: location missing publisherArticles with short descriptionShort description is different from WikidataArticles needing additional references from February 2019All articles needing additional referencesUse dmy dates from October 2023All articles with unsourced statementsArticles with unsourced statements from April 2013Wikipedia articles incorporating text from PlanetMathArticles with FAST identifiersArticles with BNE identifiersArticles with BNF identifiersArticles with BNFdata identifiersArticles with GND identifiersArticles with J9U identifiersArticles with LCCN identifiersArticles with NDL identifiersArticles with NKC identifiersThis page was last edited on 11 April 2024, at 07:14(UTC).Text is available under theCreative Commons Attribution-ShareAlike License 4.0; additional terms may apply.', 'enBy using this site, you agree to theTerms of UseandPrivacy Policy.', 'enWikipedia® is a registered trademark of theWikimedia Foundation, Inc., a non-profit organization.Privacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile viewToggle limited content width', 'ja情報量 - Wikipediaコンテンツにスキップメインメニューメインメニューサイドバーに移動非表示案内メインページコミュニティ・ポータル最近の出来事新しいページ最近の更新おまかせ表示練習用ページアップロード (ウィキメディア・コモンズ)ヘルプヘルプ井戸端お知らせバグの報告寄付ウィキペディアに関するお問い合わせ検索検索アカウント作成ログイン個人用ツールアカウント作成ログインログアウトした編集者のページもっと詳しく投稿記録トーク目次サイドバーに移動非表示ページ先頭1自己情報量（自己エントロピー）と平均情報量（エントロピー）2自己情報量自己情報量サブセクションを切り替えます2.1直観的意味2.2情報量の加法性2.3導出3平均情報量（エントロピー）平均情報量（エントロピー）サブセクションを切り替えます3.1エントロピーの基本的性質3.2コイン投げの例4連続系のエントロピー5Renyiエントロピー6歴史7単位8脚注9参考文献10関連項目11外部リンク目次の表示・非表示を切り替え情報量45の言語版AfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語リンクを編集ページノート日本語閲覧編集履歴表示ツールツールサイドバーに移動非表示操作閲覧編集履歴表示全般リンク元関連ページの更新状況ファイルをアップロード特別ページこの版への固定リンクページ情報このページを引用短縮URLを取得するQRコードをダウンロードウィキデータ項目印刷/書き出しブックの新規作成PDF 形式でダウンロード印刷用バージョン他のプロジェクトコモンズ出典: フリー百科事典『ウィキペディア（Wikipedia）』この項目では、情報量（エントロピー）の概念の情報理論的側面について説明しています。熱力学的側面については「エントロピー」をご覧ください。「データ量」とは異なります。情報理論情報量情報量微分エントロピー条件付きエントロピー交差エントロピー結合エントロピー相互情報量カルバック・ライブラー情報量エントロピーレート通信路情報源符号化定理通信路容量通信路符号化定理シャノン＝ハートレーの定理単位シャノンナットハートレーその他漸近等分割性（英語版）レート歪み理論（英語版）カテゴリ表話編歴情報量（じょうほうりょう）やエントロピー（英:entropy）は、情報理論の概念で、あるできごと（事象）が起きた際、それがどれほど起こりにくいかを表す尺度である。ありふれたできごと（たとえば「風の音」）が起こったことを知ってもそれはたいした「情報」にはならないが、逆に珍しいできごと（たとえば「曲の演奏」）が起これば、それはより多くの「情報」を含んでいると考えられる。情報量はそのできごとが本質的にどの程度の情報を持つかの尺度であるとみなすこともできる。なおここでいう「情報」とは、あくまでそのできごとの起こりにくさ（確率）だけによって決まる数学的な量でしかなく、個人・社会における有用性とは無関係である。たとえば「自分が宝くじに当たった」と「見知らぬAさんが宝くじに当たった」は、前者の方が有用な情報に見えるが、両者の情報量は全く同じである（宝くじが当たる確率は所与条件一定のもとでは誰でも同じであるため）。自己情報量（自己エントロピー）と平均情報量（エントロピー）[編集]それぞれのできごとの情報量だけでなく、それらのできごとの情報量の平均値も情報量と呼ぶ。両者を区別する場合には、前者を自己情報量（自己エントロピーとも）、後者を平均情報量（エントロピーとも）と呼ぶ。自己情報量[編集]事象Eが起こる確率をP(E)とするとき、事象Eが起こったことを知らされたとき受け取る自己情報量I(E)は、以下で定義される：I(E)=log\\u20611P(E)=−log\\u2061P(E)}=-\\\\log P(E)}確率は0≤P(E)≤1なので自己情報量I(E)は非負である。また対数の単調増加性により、起こりにくい事象（＝生起確率が低い事象）の情報量ほど値が大きい。対数の底として何を選んでも情報量の値が定数倍変わるだけなので本質的な差はない。慣習的に底に2を選ぶことが多い。底が2の場合、1/2n}の確率で起こる事象の情報量はnである。直観的意味[編集]整数uに対し、uの対数logm\\u2061uu}はm進法でのuの桁数にほぼ等しい値を表す。したがって、確率1/uで起こる事象の情報量は、ほぼuの桁数になる。情報量の加法性[編集]情報量は加法性を持つ。すなわち独立な事象AとBに対し、事象「AもBも起こる」の情報量は、Aの情報量とBの情報量の和である。これは以下で証明される。I(A,B)=−log\\u2061P(A,B)=−log\\u2061(P(A)⋅P(B))=−(log\\u2061P(A)+log\\u2061P(B))=I(A)+I(B)例えば、52枚のトランプから無作為に1枚を取り出すという試行を考える。「取り出したカードはハートの4である」という事象の情報量は、前述の定義からlog 52であると分かる。ここで、「取り出したカードのスートはハートである」という事象と「取り出したカードの数字は4である」という事象の二つを考えると、前者の情報量はlog 4、後者はlog 13である。この両者の和はlog 4 + log 13 = log (4×13) = log 52となり、「取り出したカードはハートの4である」という事象の情報量と等しい。これは「独立した情報の和が、全体の情報量と一致する」という直感的要請に合致する。導出[編集]情報量に対する直感的要請には「発生確率が低いほど大きく（単調減少性）」「確率に関して連続的に変化し（連続性）」「独立同時事象の情報量が周辺事象の情報量和に等しい（加法性）」の三条件が挙げられる。この3条件を満たす関数はコーシーの函数方程式を利用することでClog\\u2061pと一意に求まる。よって情報量の定義は上記の3条件から一意に導出できる。典型的には対数の底を2としてp=1/2で1となるようにCを設定（C=-1）する。平均情報量（エントロピー）[編集](Ω,F,P)},P)}を確率空間とする。全事象Ωの分割Ai}が与えられたとき[2]、各事象Ai∈Ω\\\\in \\\\Omega }の自己情報量I(Ai))}で定義した値H(P)=∑Ai∈ΩP(Ai)I(Ai)=−∑Ai∈ΩP(Ai)log\\u2061P(Ai)\\\\in \\\\Omega }P(A_)\\\\ I(A_)=-\\\\sum _\\\\in \\\\Omega }P(A_)\\\\log P(A_)}を確率測度PのエントロピーH(P)と呼ぶ（平均情報量、シャノン情報量、情報論のエントロピーとも）。ただし、ここでP(Ai)=0)=0}のときは、P(Ai)log\\u2061P(Ai)=0)\\\\log P(A_)=0}とみなす。これはlimp→0+plog\\u2061p=0=0}であることによる。また、離散型確率変数Xが確率分布Pに従う場合には、XのエントロピーH(X)を自己情報量Iの期待値によって定義する。すなわち、H(X)=EP[I(X)]=−∑x∈XfX(x)log\\u2061fX(x) _[I(X)]=-\\\\sum _f_(x)\\\\log f_(x)}である[3]。ここでfXはXの確率質量関数である[4]。0≦I(⋅)より、エントロピーは常に非負である。確率変数XとYの組(X,Y)も確率変数とみなせる。この確率変数の値の発生確率すなわち同時確率をPX,Y(X,Y)(X,Y)}とすると、(X,Y)のエントロピーH(X,Y)はH(X,Y)=EPX,Y[I(X,Y)]=−∑(x,y)∈(X,Y)PX,Y(x,y)log\\u2061PX,Y(x,y) _}[I(X,Y)]=-\\\\sum _P_(x,y)\\\\log P_(x,y)}になる。これを結合エントロピーと呼ぶ。(X,Y)が互いに独立な確率変数である場合には、H(X,Y)はH(X)+H(Y)に一致する。すなわち、全体の情報量H(X,Y)は、それぞれの確率変数の情報量の和である。しかし、XとYが互いに独立ではない場合は、H(X,Y)とH(X)+H(Y)は一致せず、前者より後者の方が大きい値になる。両者の情報量の差を相互情報量と呼び、I(X,Y)=H(X)+H(Y)−H(X,Y)で表す。相互情報量は常に非負の値になる。事象Bが生じているという条件下における事象Aの条件付き情報量を−log\\u2061Pr(A∣B)によって定める。確率変数Xが与えられたとき、事象「X=x」の条件付き情報量−log\\u2061Pr(X=x∣B)のxに関する加重平均を条件付きエントロピーと言い、H(X∣B)=EPX∣B[I(X∣B)]=−∑x∈XPr(X=x∣B)log\\u2061Pr(X=x∣B) _}[I(X\\\\mid B)]=-\\\\sum _\\\\Pr(X=x\\\\mid B)\\\\log \\\\Pr(X=x\\\\mid B)}で表す。さらに確率変数Yが与えられたとき、事象「Y=y」が生じているという条件下における条件付きエントロピーH(X∣Y=y)のyに関する加重平均H(X∣Y)=∑y∈YPr(Y=y)H(X∣Y=y)=−∑x∈X,y∈YPr(X=x,Y=y)log\\u2061Pr(X=x∣Y=y)\\\\Pr(Y=y)H(X\\\\mid Y=y)=-\\\\sum _\\\\Pr(X=x,Y=y)\\\\log }も、やはり条件付きエントロピーと呼ぶ。エントロピーの基本的性質[編集]情報量は確率だけによって決まる。情報量は非負の値または無限大を取る。nビットのビット列の空間（情報源）から（一様ランダムとは限らない方法で）ランダムにビット列を選んだときのエントロピーは、n以下になる。エントロピーがnになる必要十分条件は、ビット列が一様ランダムに選ばれることである。確率変数XとYが独立である必要十分条件は、H(X)+H(Y)=H(X,Y)が成立することである。コイン投げの例[編集]あるコインを投げたときに表が出る確率をp、裏が出る確率を1−pとする。このコインを投げたときに得られる平均情報量（エントロピー）は、H(X)=−plog\\u2061p−(1−p)log\\u2061(1−p)-(1-p)\\\\log }である。この関数f(p)=−plog\\u2061p−(1−p)log\\u2061(1−p)-(1-p)\\\\log }をエントロピー関数と呼ぶ。図を見ると分かるように、p=0とp=1ではHはゼロである。つまり、コインを投げる前から裏または表が出ることが確実に分かっているときに得られる平均情報量は、ゼロである。Hが最大になるのはp=1/2のときであり、一般にすべての事象（できごと）が等確率になるときにエントロピーが最大になる。連続系のエントロピー[編集]実数値を取る確率変数Xの確率密度関数をp(x)とするとき、Xのエントロピーをh(X)=−∫−∞∞p(x)log\\u2061p(x)dx^p(x)\\\\log p(x)dx}によって定義する。Xが有限集合に値を取る確率変数である場合には、Xのシャノン情報量H(X)も定義できる。Xがn通りの値を取るとき、H(X)とh(X)は、h(X)=H(Un)−H(X))-H(X)}を満たす。ただし、ここでUn}はn元集合上の一様分布とする（すなわちH(Un)=log\\u2061n)=\\\\log n}）。Renyiエントロピー[編集]Ωを、台が有限集合である確率空間とする。PをΩ上の確率分布とし、αを非負の実数とする。α≠1のとき、PのdegeeαのRenyiエントロピーをHα(P)=log\\u2061(∑A∈ΩP(A)α)1−α(P)=P(A)^)}}}によって定義する。 また、α=1,∞の場合には、RenyiエントロピーをH_(P)&=\\\\lim _&H_(P)\\\\\\\\H_(P)&=\\\\lim _&H_(P)\\\\end}\\\\right.', 'ja}によって定義する。単にRenyiエントロピーと言った場合はH2(P)(P)}を意味することも多い。さらに、確率変数Xが確率分布Pに従うとき、Hα(X)(X)}をHα(X)=Hα(P)(X)=H_(P)}によって定義する。Renyiエントロピーは以下の性質を満たす：H0(P)=log\\u2061#Ω(P)=\\\\log \\\\#\\\\Omega }が成立する。H1(P)(P)}はシャノン情報量H(P)=−∑A∈ΩP(A)log\\u2061P(A)P(A)\\\\log P(A)}と一致する。αが2以上の整数の場合には、Hα(P)=11−αlog\\u2061Pr(X1=⋯=Xα)(P)=}\\\\log \\\\Pr(X_=\\\\cdots =X_)}が成立する。ここで、X1,…,Xα,\\\\ldots ,X_}は確率分布Pに従う独立同一分布であって、Pr(X1=⋯=Xα)=\\\\cdots =X_)}はx1,…,xα,\\\\ldots ,x_}をそれぞれX1,…,Xα,\\\\ldots ,X_}に従って選んだときにx1=⋯=xα=\\\\cdots =x_}が成立する確率とする。H∞(P)=minA∈Ω(P)=\\\\min _\\\\}が成立する。このH∞(P)(P)}をminエントロピーともいう。歴史[編集]「エントロピー」の概念は1865年にルドルフ・クラウジウスがギリシャ語の「変換」を意味する言葉を語源として、熱力学における気体のある状態量として導入した。これは統計力学では微視的な状態数の対数に比例する量として表される。1929年にはレオ・シラードが、気体についての情報を観測者が獲得することと統計力学におけるエントロピーとの間に直接の関係があることを示し、現在 1 ビット（1 シャノン）と呼ぶ量が統計力学でkln 2 に対応するという関係を導いていた[5]。現在の情報理論におけるエントロピーの直接の導入は1948年のクロード・シャノンによるもので、その論文『通信の数学的理論』でエントロピーの概念を情報理論に応用した[6]。シャノン自身は熱統計力学でこの概念と関連する概念がすでに使われていることを知らずにこの定義に到達したが、その名称を考えていたとき同僚フォン・ノイマンが、熱統計力学のエントロピーに似ていることから示唆したもので、フォン・ノイマンは「統計エントロピーが何なのかを理解してる人は少ないから、議論になったら有利であろう」と語ったとされる[7][8]。しかしシャノンはフォン・ノイマンとの会話は認めつつその影響を否定している[9]。なお、シャノン以前にもラルフ・ハートレーが1928年に、集合Aに対してlog\\u2061#Aという量を考察している（“#A”はAの元数）。log\\u2061#AはA上の一様分布のエントロピーに一致する。現在では、log\\u2061#AをAのハートレー・エントロピーと呼ぶ[10]。単位[編集]情報量は本来無次元の量である。しかし、対数の底として何を用いたかによって値が異なるので，単位を付けて区別している。前述のように、情報量は確率の逆数の桁数の期待値なので、単位も桁数のそれを流用する。この為、対数の底として2、e、10を選んだときの情報量の単位は、それぞれビット(bit)、ナット(nat)、ディット(dit)である。また、今のところ主流ではないものの、1997年に日本工業規格JIS X 0016:1997（これは国際規格ISO/IEC 2382-16:1996と一致している）は、これらの量を表す単位を別に定めている。対数の底と単位底通常の単位JISおよびISOが定めた単位備考2ビット (bit)シャノン(shannon)lb, 二進対数e=2.718…ナット (nat)ナット(nat)ln, 自然対数10ディット (dit)ハートレー(hartley)lg, 常用対数単位「シャノン」、「ハートレー」の名称は、それぞれ情報量の概念を提案したクロード・シャノン、ラルフ・ハートレーにちなむ。脚注[編集][脚注の使い方]^Gray, Robert M. (2013-03-14) (英語).Entropy and Information Theory.', 'jaSpringer Science & Business Media.ISBN978-1-4757-3982-4.https://books.google.com/books?id=ZoTSBwAAQBAJ&pg=PA23&q=entropy+as+a+function+of+the+partition^この分割は離散型確率変数の確率質量関数から誘導されることもある[1]。^Cover, Thomas M.; Thomas, Joy A.', 'ja(2012-11-28) (英語).Elements of Information Theory.', 'jaJohn Wiley & Sons.ISBN978-1-118-58577-1.https://books.google.com/books?id=VWq5GG6ycxMC&pg=PA14^fX(x)をPX(x)=P(X=x)=P()(x)=P(X=x)=P(\\\\)}と書くこともある。^Szilard, L. (1929)\"Über die Entropieverminderung in einem Thermodynamischen System bei Eingriffen Intelligenter Wesen\",Zeitschrift für Physik53:840–856^Cover & Thomas 2006,Historical Notes.^『ファインマン計算機科学』 p. 96 ファインマンによる脚注*8で、「言い伝えによれば」と断りのうえでこの説を紹介している。^韓太舜、小林欣吾『情報と符号の数理』^CLAUDE E. SHANNON: An Interview Conducted by Robert Price, 28 July 1982^なお、JIS X 0016:1997 で定義される選択情報量（decision content）も同じ定義である。「互いに排反な事象から成る有限集合中の事象の数の対数。」参考文献[編集]Shannon entropy calculator(English)A Mathematical Theory of CommunicationShannon 1948 (English)Cover, Thomas M.; Thomas, Joy A.', 'ja(2006).Elements of information theory(Second ed.).', 'jaJohn Wiley & Sons.ISBN978-0-471-24195-9.MR2239987.https://books.google.com/books?id=VWq5GG6ycxMC関連項目[編集]標本化定理（シャノンの定理）データ量の比較エントロピーマクスウェルの悪魔ハフマン符号コルモゴロフ複雑性ランダウアーの原理交差エントロピー結合エントロピー量子エントロピー外部リンク[編集]情報量-脳科学辞典『情報量の意味と対数関数を使う理由』 -高校数学の美しい物語“JISX0016:1997 情報処理用語（情報理論）”.kikakurui.com.2023年10月28日閲覧。典拠管理データベース全般FAST国立図書館スペインフランスBnF dataドイツイスラエルアメリカ日本チェコ表話編歴確率論確率の歴史アンドレイ・コルモゴロフトーマス・ベイズアンドレイ・マルコフジョゼフ・L・ドゥーブ伊藤清確率の定義客観確率統計的確率古典的確率公理的確率主観確率ベイズ確率確率の拡張外確率負の確率基礎概念モデル試行結果事象標本空間確率測度確率空間確率変数確率変数の収束確率分布離散確率分布連続確率分布同時分布周辺分布条件付き確率分布独立同分布関数確率質量関数確率密度関数累積分布関数特性関数用語独立期待値モーメント条件付き確率条件付き期待値確率の解釈ベルトランの逆説3囚人問題モンティ・ホール問題サンクトペテルブルクのパラドックス合接の誤謬ギャンブラーの誤謬問題壺問題クーポンコレクター問題法則・定理ベイズの定理大数の法則中心極限定理コルモゴロフの0-1法則デ・フィネッティの定理ウィーナー＝ヒンチンの定理測度論確率測度の拡張カラテオドリの拡張定理E.ホップの拡張定理コルモゴロフの拡張定理ヴィタリの収束定理ルベーグの優収束定理ラプラス原理スコロホッドの表現定理確率微分方程式伊藤の補題確率過程独立増分過程定常過程マルチンゲールマルコフ過程マルコフ性マルコフ連鎖マルコフ決定過程部分観測マルコフ決定過程マルコフ再生過程ウィーナー過程ブラウン運動幾何ブラウン運動非整数ブラウン運動ベルヌーイ過程ガウス過程自己相似過程経験過程中華料理店過程オルンシュタイン＝ウーレンベック過程情報量最大エントロピー原理交差エントロピー結合エントロピーカルバック・ライブラー情報量相互情報量応用数理ファイナンスブラック–ショールズ方程式確率的ボラティリティモデル系統学ベイズ法カテゴリ表話編歴データ圧縮方式可逆エントロピー符号一進法算術Asymmetric numeral systems（英語版）ゴロムハフマン適応型（英語版）正準（英語版）MHレンジシャノンシャノン・ファノシャノン・ファノ・イライアス（英語版）タンストール（英語版）ユニバーサル（英語版）指数ゴロム（英語版）フィボナッチ（英語版）ガンマレーベンシュタイン（英語版）辞書式（英語版）BPEDeflateLempel-ZivLZ77LZ78LZFSELZHLZJB（英語版）LZMALZOLZRW（英語版）LZS（英語版）LZSSLZWLZWL（英語版）LZXLZ4ROLZ（英語版）統計型（英語版）BrotliSnappyZstandardその他BWTCTW（英語版）DeltaDMC（英語版）MTFPAQPPMRLE音声理論ビットレート平均(ABR)固定(CBR)可変(VBR)コンパンディング畳み込みダイナミックレンジレイテンシ（英語版）標本化定理標本化音質音声符号化サブバンド符号化変換符号化知覚符号化コーデックA-lawμ-lawACELPADPCMCELPDPCMフーリエ変換LPCLARLSPMDCT音響心理学WLPC画像理論クロマサブサンプリング符号化ツリーユニット（英語版）色空間圧縮アーティファクト解像度マクロブロックピクセルPSNR量子化（英語版）標準テストイメージ（英語版）手法チェインコード（英語版）DCTEZW（英語版）フラクタルKLT（英語版）ピラミッド（英語版）RLESPIHT（英語版）ウェーブレット映像理論ビットレート平均(ABR)固定(CBR)可変(VBR)画面解像度フレームフレームレートインターレース映像品質（英語版）コーデック（英語版）重複変換（英語版）DCTデブロッキングフィルタ（英語版）フレーム間予測理論情報量複雑性非可逆量子化レート歪み（英語版）冗長性情報理論の年表（英語版）「https://ja.wikipedia.org/w/index.php?title=情報量&oldid=99971877」から取得カテゴリ:情報理論エントロピー数学に関する記事隠しカテゴリ:FAST識別子が指定されている記事BNE識別子が指定されている記事BNF識別子が指定されている記事BNFdata識別子が指定されている記事GND識別子が指定されている記事J9U識別子が指定されている記事LCCN識別子が指定されている記事NDL識別子が指定されている記事NKC識別子が指定されている記事日本語版記事がリダイレクトの仮リンクを含む記事最終更新 2024年4月10日 (水) 16:32 （日時は個人設定で未設定ならばUTC）。テキストはクリエイティブ・コモンズ 表示-継承ライセンスのもとで利用できます。追加の条件が適用される場合があります。詳細については利用規約を参照してください。プライバシー・ポリシーウィキペディアについて免責事項行動規範開発者統計Cookieに関する声明モバイルビュー本文の横幅制限を有効化／無効化', 'skEntropia (teória informácií) – WikipédiaPreskočiť na obsahHlavné menuHlavné menupresunúť do postranného paneluskryťNavigáciaHlavná stránkaPortál komunityKaviareňPosledné úpravyNáhodná stránkaPomocníkPrispieťHľadaťHľadaťVytvoriť účetPrihlásiť saOsobné nástrojeVytvoriť účetPrihlásiť saStránky pre odhlásených redaktorovzistiť viacPríspevkyDiskusia[nezobrazovať]Entropia (teória informácií)45 jazykovAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Upraviť odkazyStránkaDiskusiaslovenčinaČítaťUpraviťUpraviť zdrojZobraziť históriuNástrojeNástrojepresunúť do postranného paneluskryťAkceČítaťUpraviťUpraviť zdrojZobraziť históriuVšeobecnéOdkazy na túto stránkuSúvisiace úpravyNahrať súborŠpeciálne stránkyTrvalý odkazInformácie o stránkeCitovať túto stránkuZískať skrátené URLStiahnuť QR kódPoložka WikidataTlačiť/exportovaťVytvoriť knihuStiahnuť ako PDFVerzia pre tlačV iných projektochWikimedia Commonsz Wikipédie, slobodnej encyklopédieEntropiaje vteórii informáciímiera neurčitosti t. j. protiklad k pojmuinformácia.', 'skUrčuje množstvo nejasnosti a na druhej strane informácie obsiahnuté v správe.Pojem zaviedolClaude E. Shannonv roku 1948 vo svojom dieleA Mathematical Theory of Communication.Zdroj: „https://sk.wikipedia.org/w/index.php?title=Entropia_(teória_informácií)&oldid=5366892“Kategória:Teória informácieDátum a čas poslednej úpravy tejto stránky: 16. marec 2013, 02:24.Text je dostupný za podmienokCreative Commons Attribution/Share-Alike License 4.0 Unported; prípadne za ďalších podmienok.', 'skPodrobnejšie informácie nájdete na stránkePodmienky použitia.Ochrana osobných údajovO WikipédiaZrieknutie sa zodpovednostiKodex chováníVývojáriŠtatistikyVyhlásenie o cookiesMobilné zobrazeniePřepnout omezenou šířku obsahu', 'trEntropi (bilgi teorisi) - Vikipediİçeriğe atlaAna menüAna menükenar çubuğuna taşıgizleGezintiAnasayfaHakkımızdaİçindekilerRastgele maddeSeçkin içerikYakınımdakilerKatılımBağış yapınDeneme tahtasıKöy çeşmesiSon değişikliklerDosya yükleTopluluk portaliWikimedia dükkânıYardımAraAraHesap oluşturOturum açKişisel araçlarHesap oluşturOturum açÇıkış yapmış editörler için sayfalardaha fazla bilgiKatkılarMesaj[kapat]Ödüllü Vikibahar 2024 Yarışması başladı!', \"trKatılmak içinVikibahar 2024sayfasını ziyaret edin.İçindekilerkenar çubuğuna taşıgizleGiriş1Tanımİçindekiler tablosunu değiştirEntropi (bilgi teorisi)45 dilAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยУкраїнськаاردوTiếng Việt中文粵語Bağlantıları değiştirMaddeTartışmaTürkçeOkuDeğiştirKaynağı değiştirGeçmişi görAraçlarAraçlarkenar çubuğuna taşıgizleEylemlerOkuDeğiştirKaynağı değiştirGeçmişi görGenelSayfaya bağlantılarİlgili değişikliklerÖzel sayfalarKalıcı bağlantıSayfa bilgisiBu sayfayı kaynak gösterKısaltılmış URL'yi alKarekodu indirVikiveri ögesiYazdır/dışa aktarBir kitap oluşturPDF olarak indirBasılmaya uygun görünümDiğer projelerdeWikimedia CommonsVikipedi, özgür ansiklopediBilgi teorisi'ndeentropibir iletinin içerdiği bilgi miktarıdır.Tanım[değiştir|kaynağı değiştir]Ayrık birrassal değişkenX'in entropisiH(X):=−∑xPr[X=x]lg\\u2061Pr[X=x] (X):=-\\\\sum _\\\\Pr[X=x]\\\\lg \\\\Pr[X=x]}olarak tanımlanır.Burada birxiçin Pr[X=x]=0 ise,Pr[X=x]lg\\u2061Pr[X=x]=0varsayılır.\", 'trBu varsayımlimp→0+plg\\u2061p=0}p\\\\lg p=0}limiti ile tutarlıdır.Otorite kontrolüBNE:XX535116BNF:cb11985913j(data)GND:4743861-7LCCN:sh85044152NDL:01191172NKC:ph425914NLI:987007550784405171\"https://tr.wikipedia.org/w/index.php?title=Entropi_(bilgi_teorisi)&oldid=24860992\" sayfasından alınmıştırKategori:Bilgi teorisiİstatistik teorisiRastlantısallıkEntropiGizli kategoriler:BNE tanımlayıcısı olan Vikipedi maddeleriBNF tanımlayıcısı olan Vikipedi maddeleriGND tanımlayıcısı olan Vikipedi maddeleriLCCN tanımlayıcısı olan Vikipedi maddeleriNDL tanımlayıcısı olan Vikipedi maddeleriNKC tanımlayıcısı olan Vikipedi maddeleriNLI tanımlayıcısı olan Vikipedi maddeleriSayfa en son 15.37, 15 Şubat 2021 tarihinde değiştirildi.MetinCreative Commons Atıf-BenzerPaylaşım Lisansıaltındadır ve ek koşullar uygulanabilir.', 'trBu siteyi kullanarakKullanım ŞartlarınıveGizlilik Politikasınıkabul etmiş olursunuz.Vikipedi® (ve Wikipedia®) kâr amacı gütmeyen kuruluş olanWikimedia Foundation, Inc.tescilli markasıdır.Gizlilik politikasıVikipedi hakkındaSorumluluk reddiDavranış KurallarıGeliştiricilerİstatistiklerÇerez politikasıMobil görünümSınırlı içerik genişliğini değiştir', 'ltEntropija (informacijos teorija) – VikipedijaPereiti prie turinioPagrindinis meniuPagrindinis meniumove to sidebarpaslėptiNaršymasPagrindinis puslapisBendruomenės puslapisForumasNaujausi keitimaiAtsitiktinis straipsnisPagalbaParamaPaieškaPaieškaSukurti paskyrąPrisijungtiAsmeniniai įrankiaiSukurti paskyrąPrisijungtiPages for logged out editorssužinoti daugiauIndėlisŠio IP aptarimų puslapis[paslėpti]Šiossavaitės iniciatyva:Europos paveldo ženklas.', 'ltKviečiame prisidėti!Kviečiame dalyvauti balsavimedėl turinio vertimo įrankio.Turinysmove to sidebarpaslėptiPradžia1Formulė2ŠaltiniaiToggle the table of contentsEntropija (informacijos teorija)45 kalbųAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어Олык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Keisti nuorodasStraipsnisAptarimaslietuviųSkaitytiKeistiKeisti vikitekstąIstorijaĮrankiaiĮrankiaimove to sidebarpaslėptiActionsSkaitytiKeistiKeisti vikitekstąIstorijaBendraSusiję straipsniaiSusiję keitimaiSpecialieji puslapiaiNuolatinė nuorodaPuslapio informacijaCituoti straipsnįGauti sutrumpintą URL nuorodąAtsisiųsti QR kodąVikiduomenys įrašasSpausdinti/eksportuotiKurti knygąParsisiųsti kaip PDFVersija spausdinimuiKituose projektuoseVikitekaStraipsnis iš Vikipedijos, laisvosios enciklopedijos.Entropijayrainformacijos teorijojenaudojamas dydis, kuris apibūdina vidutinį informacijos kiekį, kurį teikia vienas pranešimas (kai perdavimo kanale nėra trukdžių).Informacinės entropijos sąvoką 1948 m. pristatėKlodas Šenonassavo darbe „Matematinė komunikacijos teorija“ (A Mathematical Theory of Communication),[1][2]kuri dar vadinamaŠenono entropija.Formulė[redaguoti|redaguoti vikitekstą]H=−∑i=1npiloga\\u2061pi^p_\\\\log _p_\\\\,\\\\!', 'lt}Šaltiniai[redaguoti|redaguoti vikitekstą]↑Shannon, Claude E.(July 1948).', 'lt„A Mathematical Theory of Communication“.Bell System Technical Journa].27(3): 379–423.doi:10.1002/j.1538-7305.1948.tb01338.x.hdl:10338.dmlcz/101429.', 'lt(PDF, archived fromhere)↑Shannon, Claude E.(October 1948).', 'lt„A Mathematical Theory of Communication“.Bell System Technical Journal.27(4): 623–656.doi:10.1002/j.1538-7305.1948.tb00917.x.hdl:11858/00-001M-0000-002C-4317-B.', 'lt(PDF, archived fromhere)Rodomas puslapis \"https://lt.wikipedia.org/w/index.php?title=Entropija_(informacijos_teorija)&oldid=7180126\"Kategorija:Informacijos teorijaŠis puslapis paskutinį kartą keistas 5 vasario 2024 00:49.Tekstas pateikiamas pagalCreative Commons Attribution/Share-Alike Licenciją; gali būti taikomos papildomos sąlygos.', 'ltDetaliau –Terms of Use.Privatumo politikaApie VikipedijąJokių garantijųCode of ConductKūrėjaiStatistikaSlapukų politikaMobili peržiūraToggle limited content width', \"cyEntropi gwybodaeth - WicipediaNeidio i'r cynnwysPrif ddewislenPrif ddewislensymud i'r bar ochrcuddioPanel llywioHafanPorth y GymunedY CaffiMaterion cyfoesNewidiadau diweddarErthygl ar hapCymorthRhoiChwilioChwilioCreu cyfrifMewngofnodiOffer personolCreu cyfrifMewngofnodiTudalennau ar gyfer golygyddion allgofnodedigdysgu mwyCyfraniadauSgwrsCynnwyssymud i'r bar ochrcuddioY dechrau1DiffiniadToglo is-adran Diffiniad1.1Entropi a cynnwys gwybodaeth1.2Cywasgu data1.3Cyfyngiadau ar y dehongliad o entropi fel cynnwys gwybodaeth2Dolenni allanolToglo'r tabl cynnwysEntropi gwybodaeth45 iaithAfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Golygu dolenniErthyglSgwrsCymraegDarllenGolyguGolygu codGweld hanesBlwch offerBlwch offersymud i'r bar ochrcuddioGweithredoeddDarllenGolyguGolygu codGweld hanesCyffredinolBeth sy'n cysylltu ymaNewidiadau perthnasolTudalennau arbennigDolen barhaolGwybodaeth am y dudalenCyfeirio at y dudalen honCael URL byrDownload QR codeEitem WicidataArgraffu / allforioLlunio llyfrLawrlwytho fel PDFFersiwn argraffuMewn prosiectau eraillComin WikimediaOddi ar WicipediaMewndamcaniaeth gwybodaeth, mesur o'r ansicrwydd a gysylltir âhapnewidynywentropi gwybodaeth.Gellir dehongli'r entropi hwn fel yr hyd neges cyfartalog lleiaf posib (mewndidau) sy'n cyfleu allbwn yr hapnewidyn.\", 'cyCynrychiola hyn derfan mathemategol ar y cywasgiad di-golled gorau posib o ddata: lleiafswm y nifer o ddidau a ellir eu danfon i gyfathrebu neges ydyw.', 'cyYn gyfwerth, gellir ei ystyried yn fesur o\\'r wybodaeth gyfartalog mae\\'r derbynnydd yn ei golli wrth beidio gwybod gwerth yr hapnewidyn.Cyflwynwyd y cysyniad ganClaude E. Shannonyn ei bapur\"A Mathematical Theory of Communication\"a gyhoeddwyd ym1948.Diffiniad[golygu|golygu cod]H(X)=E\\u2061(I(X))=∑i=1np(xi)log2\\u2061(1/p(xi))=−∑i=1np(xi)log2\\u2061p(xi)H(X)=\\\\operatorname (I(X))&=&\\\\displaystyle ^p(x_)\\\\log _\\\\left(1/p(x_)\\\\right)}\\\\\\\\&=&-\\\\displaystyle ^p(x_)\\\\log _p(x_)}\\\\qquad \\\\end}}yw entropi gwybodaethhapnewidyn arwahanolXa all gymryd y gwerthoedd lle maiI(X) yw cynnwys gwybodaethX, sydd yn hapnewidyn ei hun; ap(xi) = P(X=xi) ywffwythiant dwysedd tebygolrwyddX.', \"cy[gellir ymestyn y diffiniad hwm i hapnewidynnau di-dor]Entropi a cynnwys gwybodaeth[golygu|golygu cod]gw.Theorem codio ffynhonnell ShannonCywasgu data[golygu|golygu cod]Mae entropi yn arffinio perfformiad cywasgiad di-golled o ddata, ac fe ellir cyflawni'r cywasgiad gorau posib trwy ddefnyddio'rset nodweddiadol(neu'n ymarferol,codio Huffman,Lempel-Zivneucodio rhifyddol.Cyfyngiadau ar y dehongliad o entropi fel cynnwys gwybodaeth[golygu|golygu cod]Er fod entropi yn cael ei ddehongli'n aml fel mesur o gynnwys gwybodaeth ffynhonnell ddata, nid yw'r cynnwys gwybodaeth hwn yn absolìwt: mae'n dibynnu'n llwyr ar y model tebygoliaethol.\", \"cyMae entropi o 0 gan ffynhonnell sy'n cynhyrchu un symbol yn unig, ond mae'r diffiniad o symbol yn dibynnu ar yr wyddor dan sylw.\", \"cyYstyriwch ffynhonnell sy'n cynhyrchu'r dilyniant ABABABABAB... Os tybiwn fod llythrennau unigol ynannibynnol, mae cyfradd entropi o un did i bob llythyren.\", \"cyOnd os tybiwn maedeugraffiauyw'r symbolau yn y model, yna 0 yw'r cyfradd entropi.\", 'cyFodd bynnag, os defnyddiwn blociau mawr, yna fe allem gael amcymgyfrifiad annaturiol o isel o\\'r gyfradd entropi.Dolenni allanol[golygu|golygu cod](en)http://planetmath.org/?op=getobj&from=objects&id=968(en)Disgrifiad o entropi gwybodaeth o\"Tools for Thought\"gan Howard RheingoldArchifwyd2011-05-15 yn yPeiriant Wayback.Mae\\'r erthygl hon yn cynnwys term neu dermau sydd efallai wedi eu bathu\\'n newydd sbon:set nodweddiadolo\\'r Saesneg \"typical set\".', 'cyGallwch helpu trwy safoni\\'r termau.Wedi dod o \"https://cy.wikipedia.org/w/index.php?title=Entropi_gwybodaeth&oldid=11817366\"Categorïau:Damcaniaeth gwybodaethGategori cudd:Webarchive template wayback linksErthyglau sy\\'n cynnwys termau wedi eu bathuGolygwyd y dudalen hon ddiwethaf ar 18 Mehefin 2023, am 22:11.Mae\\'r testun ar gael o danCreative Commons Attribution-ShareAlike License; gall telerau ychwanegol fod yn berthnasol.', 'cyGweler yTelerau Gwasanaetham fanylion.Polisi preifatrwyddYnglŷn â WicipediaGwadiadauCod YmddygiadDatblygwyrYstadegauDatganiad cwcisGolwg symudolToglo lled cynnwys cyfyngedig', 'barEntropie (Informationstheorie) – Boarische WikipediaZum Inhalt springenHauptmenüHauptmenüIn die Seitenleiste verschiebenvasteggaNavigaziónHoamseitenZuafoisartiklThemenPortalLezde EndarungaNeie ArtikeGmoaAutornKaffeeStammdischMir fehlt ein WortAutornPortalQualitätssicherungEtz spendn!SuachSuachNutzakonto oolengOmejdnMeih WerkzeigNutzakonto oolengOmejdnSeiten für abgemeldete BenutzerWeitere InformationenBeitrégDischkriaseitn vo dera IPSchon gewusst?Bairischist älter als Hochdeutsch!→ Während die deutsche Schriftsprache im 15./16.', 'barJh.', 'barentstanden ist, sind die ältestenaltbairischenTexte aus dem 8.', 'barJahrhundert.InhaltsverzeichnisIn die Seitenleiste verschiebenvasteggaAnfang1RefarenzInhaltsverzeichnis umschaltenEntropie (Informationstheorie)45 SproochenAfrikaansالعربيةБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語한국어LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語Links beorwatnArtikldischkrianBoarischLeesenWerkelnAm Gwëntext werkelnGschicht åhschaunSunstigsWerkzeigIn die Seitenleiste verschiebenvasteggaAktionenLeesenWerkelnAm Gwëntext werkelnGschicht åhschaunOigmoahLinks auf de SeitnValinkts priafmSpezialseitenPermanénter LinkSeiteninformaziónSeitn zitianGekürzte URL abrufenQR-Code runterladenWikidata-Itemdrugga/exportianBuach erstënOis PDF owerlodenSeitn ausdruckaIn anderen ProjektenWikimedia CommonsAus WikipediaDer Artikl is im DialektObaboarischgschriem worn.Entropieis a Moß fian mittlanInformationsghoidoda aa Informationsdichtn vo oan Zeichnsystems.', 'barDa Begriff in daInformationstheorieis in Analogie zuaEntropiein daThermodynamikundStatistischn Mechanikentstandn.', 'barBeide Begriffe hom Gmoasamkeitn, de wo ma owa nua dann eindeitig erkennt, wenn Kenntnisse in beidn Fochgebietn vuahandn san.As informationstheoretische Vaständnis vom BegriffEntropiegähd afClaude E. Shannonzruck und existiat seit etwa 1948.', 'barIn dem Joar hod da Shannon a fundamentale OrbatA Mathematical Theory of Communicationvaeffentlicht und hod damit de moderneInformationstheorieprägt.De BezeichnungHfia de Entropie kimmt aus ana Verwechslung mitm uaspringli gwejdn großnEta(H }) fias Kunstwort εντροπία.', 'bar[1]Refarenz[Werkeln|Am Gwëntext werkeln]↑Károly Simonyi:Kulturgeschichte der Physik.', 'barIn: Urania-Verlag, Leipzig 1990,ISBN 3-332-00254-6, S. 372.Voh „https://bar.wikipedia.org/w/index.php?title=Entropie_(Informationstheorie)&oldid=716212“Kategoariner:Artikel auf WestmittelbairischKybernetikInformationstheorieDe Seitn is zletzt am 22.', 'barJuni 2019 um 06:00 gändert worn.AbruafstatistikDea Text is unta da Lizenz„Creative Commons Attribution/Share-Alike“vafigbor; zuasätzliche Bedingunga kennan owendbor sei.', 'barOazlheitn san in deNutzungsbedingungabschriebm.DatenschutzIwer WikipediaHoftungsausschlusVerhaltenskodexEntwicklaStatistikenStellungsnåm zua dé CookiesMobile OsichtUmschalten der eingeschränkten Breite des Inhalts', 'ko정보 엔트로피 - 위키백과, 우리 모두의 백과사전본문으로 이동주 메뉴주 메뉴사이드바로 이동숨기기둘러보기대문최근 바뀜요즘 화제임의의 문서로기부사용자 모임사랑방사용자 모임관리 요청편집 안내소개도움말정책과 지침질문방검색검색계정 만들기로그인개인 도구계정 만들기로그인로그아웃한 편집자를 위한 문서더 알아보기기여토론목차사이드바로 이동숨기기처음 위치1의미2정의정의 하위섹션 토글하기2.1조건부 엔트로피3예예 하위섹션 토글하기3.1균등분포4열역학적 엔트로피와의 관계5다양한 분야에서 활용된 엔트로피 개념6참고 문헌7같이 보기8외부 링크목차 토글정보 엔트로피45개 언어AfrikaansالعربيةBoarischБългарскиBosanskiCatalàکوردیČeštinaCymraegDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתMagyarBahasa IndonesiaItaliano日本語LietuviųОлык марийNederlandsਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSundaSvenskaไทยTürkçeУкраїнськаاردوTiếng Việt中文粵語링크 편집문서토론한국어읽기편집역사 보기도구도구사이드바로 이동숨기기동작읽기편집역사 보기일반여기를 가리키는 문서가리키는 글의 최근 바뀜파일 올리기특수 문서 목록고유 링크문서 정보이 문서 인용하기축약된 URL 얻기QR 코드 다운로드위키데이터 항목인쇄/내보내기책 만들기PDF로 다운로드인쇄용 판다른 프로젝트위키미디어 공용위키백과, 우리 모두의 백과사전.2 섀넌의 엔트로피: 2 개의 공정한 동전을 던질 때 정보 엔트로피는 발생 가능한 모든 결과의 개수에 밑이 2 인 로그를 취한 것과 같다.', 'ko2 개의 동전을 던지면 4 가지 결과가 발생할 수 있고, 엔트로피는 2 비트가 된다.', 'ko일반적으로 정보 엔트로피는 모든 발생가능한 결과의 평균적인 정보가 된다.정보이론에서 시스템은 송신자, 채널, 수신자를 이용하여 모형화한다.', 'ko송신자는 채널을 통해 전달되는 메시지를 만들어낸다.', 'ko채널은 특정한 방식을 통해 메시지를 변경한다.', 'ko수신자는 어떤 메시지가 보내진 것인지 추론하고자 한다.', 'ko이 맥락에서정보 엔트로피(또는섀넌 엔트로피)는 각 메시지에 포함된 정보의기댓값(평균)이다.', \"ko'메시지'는 어떤 흐름의 정보에 대해서도 모형화할 수 있다.기술적인 관점에서 보면 정보는 발생 가능한 사건이나 메시지의 확률분포의 음의 로그로 정의할 수 있다.\", 'ko각 사건의 정보량은 그 기댓값, 또는 평균이 섀넌 엔트로피인 확률변수를 형성한다.', 'ko엔트로피의 단위는 정의에 사용된 로그의 밑이 무엇인지에 따라섀넌(shannon), 내트(nat) 또는 하틀리(hartley)를 사용한다.', 'ko단, 섀넌의 경우 보통 비트(bit)로 표현한다.확률분포의 로그는 엔트로피의 단위로 사용하기에 매우 유용한데 이는 독립적인 소스(source)들에 대해 그 값을 더할 수 있기 때문이다.', 'ko예를 들어 동전을 1개 던지면 엔트로피는 1 섀넌이고,m개의 동전을 던질 때는m섀넌이다.n이 2의 거듭제곱일 때, 일반적으로n개의 값 중 하나를 취하는 변수를 표현하기 위해서는log2(n)비트가 필요하다.', 'ko모든 값의 발생 확률이 동일하면, (섀넌으로 표현된) 엔트로피는 비트의 개수와 동일하게 된다.', 'ko비트의 개수와 섀넌이 동일한 경우는 모든 결과의 발생 확률이 동일한 경우로 한정된다.', 'ko만약 하나의 사건이 다른 사건보다 발생할 확률이 높다면 그 사건에 대한 관측이 제공할 수 있는 정보는 적다.', 'ko반대로 희귀한 사건을 관측하면 더 많은 정보를 얻을 수 있다.', 'ko확률이 낮은 사건에 대한 관측은 덜 발생할 것이므로 순 효과는 불균등하게 분포한 자료로부터 얻어진log2(n)보다 작은 엔트로피가 된다.', 'ko하나의 사건이 확실하게 일어나는 경우라면 엔트로피는 0 이 된다.', 'ko섀넌 엔트로피는 소스(source)의 확률분포가 알려져 있을 때 이 모든 고려사항을 수치화한다.', 'ko관측된 사건들의 의미(메시지의 의미)는 엔트로피를 정의할 때 중요하지 않다.', 'ko엔트로피는 특정한 사건이 일어날 확률만을 고려함으로써 사건의 배후에 존재하는 확률분포에 대한 정보를 캡슐화할뿐 사건 자체의 의미는 포함하지 않는다.일반적으로 엔트로피는 무질서도 또는 불확실성을 가리킨다.', 'ko섀넌 엔트로피의 개념은클로드 섀넌이 자신의 1948년 논문 \"수학적 통신 이론\"에서 도입하였다.', \"ko[1]섀넌 엔트로피는 정보 소스(source)를무손실인코딩 또는압축할 때 가능한 최상의 평균 길이의 절대적 한계치를 제공해준다.레니 엔트로피는 섀넌 엔트로피를 일반화한 것이다.의미[편집]어떤 결과값의 발생 가능도가 작아질수록 그 정보량은 커지고, 더 자주 발생할수록 그 정보량은 작아진다.요약 : 확률이 낮을수록, 어떤 정보일지는 불확실하게 되고, 우리는 이때 '정보가 많다', '엔트로피가 높다'고 표현한다.정보 이론의 기본은 어떤 사람이 정보를 더 많이 알수록 새롭게 알 수 있는 정보는 적어진다는 것이다.\", 'ko어떤 사건의 확률이 매우 높다고 가정하자.', 'ko우리는 그 사건이 발생해도 별로 놀라지 않는다.', 'ko즉, 이 사건은 적은 정보를 제공한다.', 'ko반대로, 만약 사건이 불확실하다면, 그 사건이 일어났을 때 훨씬 유용한 정보를 제공한다.', 'ko그러므로, 정보량(information content)은 확률에 반비례한다.', 'ko이제 만약 더 많은 사건이 일어난다면, 엔트로피는 실제로 한 사건이 일어났을 때, 얻을 것으로 기대되는 평균 정보량을 측정한다.', 'ko주사위 던지기와 동전 던지기를 생각해보자.', 'ko주사위 던지기에서 일어나는 한 사건의 확률은 동전 던지기에서 일어나는 한 사건의 확률보다 작다.', \"ko즉, 여기서 엔트로피는 주사위 던지기가 동전 던지기보다 크다고 할 수 있다.그러므로 엔트로피는 '어떤 상태에서의 불확실성', 또는 이와 동등한 의미로 '평균 정보량'을 의미한다.\", 'ko이 용어를 직관적으로 이해하기 위해 정치에서의 선거를 생각해보자.', 'ko보통, 그러한 선거는 우리가 선거 결과를 모르기 때문에 실시한다.', \"ko즉, 선거 결과는 상대적으로 '불확실'하다.\", \"ko그리고 실제로 선거를 시행하고 선거 결과를 얻는 것은 우리에게 '새로운 정보'를 제공한다.\", 'ko이 말은 선거 결과에 대한 엔트로피가 크다는 말과 같다.', 'ko이제, 첫 번째 선거가 실시되고 난 후, 두 번째 선거가 실시되었다고 생각해보자.', 'ko첫 번째 선거 결과를 이미 알기 때문에, 우리는 두 번째 선거 결과를 예측할 수 있고 두 번째 선거 결과의 엔트로피가 첫 번째 것보다 작다.문자 배열로 간주되는 영어 텍스트의 경우, 꽤 낮은 엔트로피를 가지고 있다.', 'ko즉, 예상하기 쉽다는 의미이다.', \"ko예를 들어 우리가 다음 문자로 무엇이 올지 정확히는 몰라도, 'e'가 'z'보다는 많이 쓰이고 'qu'라는 조합은 'q'와 다른 문자의 조합보다는 훨씬 많다는 것을 알고 있다.\", 'ko처음 문자 몇 개를 알려주면 나머지 문자를 추측하여 글자를 알아맞히는 일은 어렵지 않다.', 'ko영어 문장은 연구 방법에 따라 다르지만 일반적으로 메시지의 글자당 0.6에서 1.3비트의 엔트로피를 갖는다고 알려져 있다.무손실 압축은 압축된 메시지를 복원함으로써 원래 메시지를 온전히 복원할 수 있음을 의미한다.', 'ko무손실 압축된 메시지는 원본 메시지와 정보량은 같지만 더 적은 문자를 가진다.', 'ko많은 정보량은 높은 엔트로피를 가지고 있다는 말과 같다.', 'ko중복성이 거의 없다는 뜻이다.', 'ko개략적으로 말해서, 섀년의 소스 코딩 정리는 무손실 압축이 문자 1비트가 1비트 이상의 정보를 포함하도록 할 수는 없지만, 어떤 코딩 방법에 의해 문자 1비트당 1비트 이하만큼의 정보를 포함하도록 할 수 있다고 말한다.', 'ko비트당 메시지의 엔트로피와 메시지 길이의 곱은 전체 메시지가 얼마나 많은 정보를 담고 있는지를 알려준다.직관적인 이해를 위해, 우리가 ABCD 4가지 문자로 이루어져있는 메시지를 전달한다고 생각해보자.', \"ko이 메시지는 'ABADDCAB'이다.\", 'ko정보 이론은 우리에게 메시지를 전달할 가장 작은 정보량을 계산하는 방법을 제공한다.', 'ko만약 4개 글자가 모두 확률이 동일하다면 (25%), A는 00, B는 01, C는 10, D는 11과 같이 2비트로 (바이너리에서) 인코딩하는 수 밖에 방법이 없다.', 'ko이제 A가 70%, B가 26%, C와 D가 2%의 확률로 발생한다고 가정하자.', 'ko우리는 각각의 문자에 가변 길이 코드를 부여할 수 있다.', 'ko우리가 순차적인 2비트의 1 (11)을 받지 않았다면, 1을 받는 것은 다음 비트까지 조사하라는 것을 의미한다.', 'ko이 경우에 A는 0 (1비트), B는 10 (2비트), C와 D는 각각 110, 111 (3비트)로 인코딩 될 수 있다.', 'ko70%의 경우, 우리는 1비트만 전송하며 26%의 경우 2비트를, 나머지 4%의 경우에만 3비트를 전송한다.', 'ko평균적으로, 2비트 이하의 정보를 전송하기 때문에 엔트로피는 감소한다.', 'ko(A와 B를 합치면 96%라는 높은 출현빈도가 나오기 때문이다)섀넌의 이론은 어떤 무손실 압축 방식도 모든 메시지를 줄일 수는 없다는 것도 의미한다.', 'ko비둘기집 원리에 의해서, 어떤 메시지가 짧게 압축되기 위해서 최소한 다른 하나는 길어져야 한다.', 'ko평소에 우리가 횡설수설하는 문서보다는 문법에 따라 쓰인 문장을 압축하는 것처럼, 특정한 형식의 메시지만을 압축하는 것에 집중하기 때문에 압축 알고리즘이 어떤 메시지를 늘리는가는 별로 문제가 되지 않는다.', 'ko그러나 이 문제는 이미 압축된 데이터를 다시 압축하려고 할 때 드러난다.', 'ko실제로, 우리는 FLAC, MP3, WebM, AAC, PNG, JPEG 같은 압축된 데이터들을 다시 ZIP 형식으로 압축하면 원본 데이터보다 용량이 조금 더 늘어나 있는 것을 목격할 수 있다.한편, 정보 엔트로피가 커지는것은 역시 변수(불확실성)가 증가하는 것을 의미하므로, 변수를 제어함으로써 불확실성이 줄어드는 것은 결국 정보 획득을 의미하게 된다.그러므로 획득을 증가시켜 불확실성을 감소시키는 것은 변수가 줄어드는 것으로 볼 수 있는데 이것은 결과적으로 엔트로피의 크기를 감소시키는정보 이득과 관계있다.', 'ko[2]정의[편집]확률변수X:P→E가 분포f:E→R }를 따른다고 하자.', 'ko그렇다면X의정보 엔트로피H(X)는 다음과 같다.H(X)=−E\\u2061(ln\\u2061f)=−∫Ef(x)ln\\u2061(f(x))dx (\\\\ln f)=-\\\\int _f(x)\\\\ln(f(x))\\\\,dx}만약표본 공간E가 이산공간E=,\\\\ldots ,x_\\\\}}이라면,르베그 적분은 합이 되며, 따라서 정보 엔트로피는 다음과 같다.H(X)=−∑ifiln\\u2061fif_\\\\ln f_}간혹, 위 정의에서자연로그대신 이진로그log2}를 사용하는 경우가 있다.', 'ko이 경우 정보 엔트로피의 단위는비트이고, 자연로그의 경우에는 단위내트(nat)를 사용한다.조건부 엔트로피[편집]두 확률변수(X,Y):P→EX×EY\\\\times E_}가 주어졌고, 그 확률 분포f:EX×EY→R\\\\times E_\\\\to \\\\mathbb }가 주어졌다고 하자.', 'ko그렇다면,Y가 주어졌을 때X의조건부 엔트로피(영어:conditional entropy)는 다음과 같다.H(X|Y∈S)=E\\u2061(ln\\u2061∫f(x′,y)dx′f(x,y))=∫EX×EYf(x,y)ln\\u2061∫f(x′,y)dx′f(x,y)dxdy \\\\left(\\\\ln }\\\\right)=\\\\int _\\\\times E_}f(x,y)\\\\ln }\\\\,dx\\\\,dy}조건부 엔트로피는 항상 음이 아니며, Y값을 알고 있을 때 X값의 무작위한 정도의 양으로 해석될 수 있다.', 'ko예를 들어, 6면을 가진 주사위의 엔트로피H(주사위)를 구하는데, 그 주사위가 1,2,3만 나오도록 조작되어있다는 사실을 알고 있다면, 이것의 엔트로피는H(주사위 값이 1 또는 2 또는 3)와 같게 된다.예[편집]동전 던지기에서 결과값의 엔트로피 H(X)를 나타낸 그래프.', 'koX축이 동전의 공정한 정도 (Pr(X=1))를 나타내고 Y축이 대응되는 엔트로피의 크기를 나타낸다.', 'ko여기서는 공정한 동전 (Pr(X=1)=0.5)을 사용한 동전 던지기 결과값을 전송할 때 가장 큰 엔트로피인 1비트가 필요함을 확인할 수 있다.앞에서 언급한 동전던지기 사례를 다시 생각해 보자.', 'ko만약 우리가 동전의 특정 면이 나올 확률을 알고 있다고 가정해 보자.', 'ko(반드시 앞, 뒷면이 나올 확률이 같을 필요는 없다.', 'ko)동전 던지기를 시행했을 때 결과값의 엔트로피는 공정한 동전일 때 가장 높게 나온다.', 'ko(앞, 뒷면이 나올 확률이 각각 1/2로 같을 경우이다.)', 'ko이러한 경우가 불확실성을 가장 극대화 시키고 결과값을 예상하기가 가장 어렵다는 것을 의미한다.', 'ko이때의 동전던지기 결과값은 1비트에 해당하는 정보를 가지게 된다.그러나 만약 우리가 이 동전이 공정하지 않다면, 즉 앞면이 나올 확률이 p, 뒷면이 나올 확률을 q로 이미 알고 있다면 불확실성은 더 떨어질 것이다.', 'ko이는 동전을 던질 때마다 특정한 면이 나올 확률이 더 높기 때문이다.', 'ko이때의 불확실성의 감소는 엔트로피의 감소로 정량화될 수 있다.', 'ko공정하지 않은 동전 던지기 결과값의 엔트로피는 1비트의 정보 보다 적다고 해석할 수 있다.이와 같은 경우 중 가장 극단적인 사례는 양면을 가지고 있으나 절대로 뒷면이 나오지 않는 동전을 사용할 경우이다.', 'ko이때에는 불확실성이 전혀 없으므로 (항상 앞면이 나오므로) 엔트로피는 0이다.', 'ko즉, 이러한 동전 던지기의 시행결과는 아무런 정보도 전달하지 않는다.균등분포[편집]정보 엔트로피의 정의를 이해하기 위해서,이산균등분포의 엔트로피를 계산해 보자.표본 공간이 총n개의 서로 다른 값들로 이루어진다면,확률 질량 함수는pi=1/n=1/n}이고, 따라서 엔트로피는S=−∑i=1npiln\\u2061pi=ln\\u2061n^p_\\\\ln p_=\\\\ln n}이다.로그 함수는 독립적인 불확실성에가산성을 제공하는데 사용된다.', 'ko예를 들어, 크기m의 이산 표본 공간과 크기n의 이산 표본 공간에서, 서로 독립이며 균등분포를 따르는 두 확률변수를 동시에 측정할 경우, 그 총 엔트로피는ln\\u2061(mn)=ln\\u2061m+ln\\u2061n이 된다.', \"ko즉, 서로 독립인 두 확률변수의 엔트로피는 각 확률변수의 엔트로피의 합과 같다.열역학적 엔트로피와의 관계[편집]이 부분의 본문은엔트로피입니다.정보이론에서 '엔트로피'라는 단어를 사용하게 된 이유는 섀넌의 공식이 열역학적 엔트로피의 공식과 상당 부분 비슷하기 때문이다.\", 'ko[3]열역학적엔트로피S로서 가장 많이 사용되는 통계열역학에서는 기브스의 엔트로피를 다음과 같이 정의한다.S=−kB∑piln\\u2061pi\\\\sum p_\\\\ln p_\\\\,}여기서kB는볼츠만 상수를, 그리고 pi는 미시적인 상태의 확률을 의미한다.', 'ko기브스엔트로피는 1872년볼츠만의 연구업적을 뒤이어조사이어 윌러드 기브스에 의하여 1878년에 정의되었다.', 'ko기브스 엔트로피는 또한 변화가 거의 없이양자물리학에서의 노만 엔트로피로도 변형되는데 이는 1927년존 폰 노이만에 의해 정의 소개되었으며 다음과 같이 정의된다S=−kBTr(ρln\\u2061ρ)\\\\,\\\\,}(\\\\rho \\\\ln \\\\rho )\\\\,}여기서 ρ로는양자역학시스템에서의밀도행렬을 나타내며 Tr은 대각합을 나타낸다.일상생활의 실용적인 수준에서는정보이론의 엔트로피와열역학적엔트로피의 관계가 그리 깊지는 않다.', \"ko물리학자나 화학자는 자발적으로 초기상태에서 멀어지는 시스템에서의 엔트로피 '변화'에 더욱 관심이 있다.\", 'ko이는열역학 제2법칙에 부합하는 내용으로, 불변하는확률분포에 집중하는 정보엔트로피와는 포커싱이 사뭇 다르다.그러나 여러 학문분야에 걸쳐 종합적인 분석을 해보면,열역학적 엔트로피와 정보 엔트로피 사이에서는 연결고리가 만들어질 수 있다.', \"ko1957년 제인스(영어:Jaynes)의 의견에 의하면, 열역학은 섀넌의 정보 이론의 '응용'으로 간주될 수 있다.\", 'ko[4]열역학에서의 엔트로피는 시스템의 더욱 자세한 미시적인 상태를 정의하기 위해 더 필요한 섀넌 정보 양의 추정으로 해석될 수 있다.', 'ko이는 고전열역학의 미시변수란 개념으로는 설명될 수 없는 것이었다.', 'ko예를 들면 시스템에 열을 가하는 것은 열역학적 엔트로피를 증가하는 것이다.', 'ko왜냐하면 이것은 미세 상태의 가능한 가짓수를 증가시키기 때문이다.제임스 클러크 맥스웰은 이론적으로 시스템의 열역학적 엔트로피를 개별적인 분자 상태에 대한 정보를 사용함으로써 감소시킬 수 있다고 주장하였다.', 'ko이는맥스웰의 도깨비란 이름으로 널리 알려져 있다.', 'ko그러나 란다우어(Landauer)와 그의 동료들은 총 엔트로피는 줄지 않는다는 것을 보이며 이 역설을 해결하였다.다양한 분야에서 활용된 엔트로피 개념[편집]아이작 아시모프의 단편 소설 《마지막 질문》.헨리 애덤스의 책 《엔트로피와 생명》.', 'ko19세기 미국 역사가 헨리 애덤스는 기계의 힘인 동력과 힘의 쇠퇴인엔트로피개념을 포함한 정교한 역사 이론을 주창하였다.', 'ko애덤스는 인간 사회가 진보하는 것이 아니라 어쩔 수 없이 쇠퇴하는 것을 자연 상태에서 엔트로피가 커지는 것에 결합하여 설명하였다.참고 문헌[편집]위키미디어 공용에 관련된미디어 분류가 있습니다.정보 엔트로피↑Shannon, Claude E.(July–October 1948).', 'ko“A Mathematical Theory of Communication”.', 'ko《Bell System Technical Journal》27(3): 379–423.doi:10.1002/j.1538-7305.1948.tb01338.x.', 'ko(PDF, archived fromhere)↑엔트로피 & 정보 이득,Entropy & Information GainArchived2017년 8월 19일 -웨이백 머신↑Jaynes, E.T.', 'ko(1957년 5월).“Information Theory and Statistical Mechanics”(PDF).', \"ko《Physical Review》 (영어)106(4): 620–630.Bibcode:1957PhRv..106..620J.doi:10.1103/PhysRev.106.620.더 이상 지원되지 않는 변수를 사용함 (도움말)↑Realated Paper: Vesselin I. Dimitrov, 'On Shannon-Jaynes Entropy and Fisher Information'.같이 보기[편집]정보이론엔트로피 부호화해밍 거리외부 링크[편집]Entropyan interdisciplinary journal on all aspect of the entropy concept.\", 'koOpen access.Information is not entropy, information is not uncertainty !- a discussion of the use of the terms \"information\" and \"entropy\".I\\'m Confused: How Could Information Equal Entropy?- a similar discussion on the bionet.info-theory FAQ.Description of information entropy from \"Tools for Thought\" by Howard RheingoldA java applet representing Shannon\\'s Experiment to Calculate the Entropy of EnglishSlides on information gain and entropyAn Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science- a wikibook on the interpretation of the concept of entropy.vte압축방식이론엔트로피복잡도손실 압축부호율 변형 이론골롬 부호화양자화 (정보 이론)턴스톨 부호화무손실 압축엔트로피 부호화허프만 부호화산술 부호화범위 부호화범용 부호샤논-파노 부호화턴스톨 부호화주요 알고리즘반복 길이 부호화사전 기반 부호화DEFLATE버로우즈-휠러 변환오디오 압축이론컴팬딩합성곱표본화표본화 정리오디오 코덱 기술DPCM푸리에 변환음성 코덱 기술LPCWLPCCELPACELPA-lawμ-lawADPCMMDCT기타비트레이트(가변고정)이미지 압축용어색 공간화소크로마 서브샘플링해상도매크로블록이미지 코덱 기술프랙탈 압축웨이블릿 변환이산 코사인 변환카루넨-뢰브 변환기타표준 시험 영상PSNR양자화 (정보 이론)영상 압축용어비디오프레임 레이트비월 주사 방식순차 주사 방식영상 코덱 기술움직임 보상쿼터 픽셀디블로킹 필터기타율-왜곡 이론비트레이트(가변고정)형식에 대해서는압축 형식을, 코덱에 대해서는압축 소프트웨어를 참고하십시오.전거 통제국제FAST국가스페인프랑스BnF 데이터독일이스라엘미국일본체코원본 주소 \"https://ko.wikipedia.org/w/index.php?title=정보_엔트로피&oldid=35257068\"분류:정보 엔트로피정보 이론통계 이론엔트로피통계적 무작위성숨은 분류:인용 오류 - 오래된 변수를 사용함CS1 - 영어 인용 (en)위키데이터 속성 P373을 사용하는 문서위키데이터 속성 P227을 사용하는 문서위키데이터 속성 P244를 사용하는 문서위키데이터 속성 P268을 사용하는 문서위키데이터 속성 P349를 사용하는 문서위키데이터 속성 P691을 사용하는 문서위키데이터 속성 P950을 사용하는 문서위키데이터 속성 P2163을 사용하는 문서위키데이터 속성 P8189를 사용하는 문서영어 표기를 포함한 문서FAST 식별자를 포함한 위키백과 문서BNE 식별자를 포함한 위키백과 문서BNF 식별자를 포함한 위키백과 문서BNFdata 식별자를 포함한 위키백과 문서GND 식별자를 포함한 위키백과 문서J9U 식별자를 포함한 위키백과 문서LCCN 식별자를 포함한 위키백과 문서NDL 식별자를 포함한 위키백과 문서NKC 식별자를 포함한 위키백과 문서이 문서는 2023년 7월 17일 (월) 12:46에 마지막으로 편집되었습니다.모든 문서는크리에이티브 커먼즈 저작자표시-동일조건변경허락 4.0에 따라 사용할 수 있으며, 추가적인 조건이 적용될 수 있습니다.', 'ko자세한 내용은이용 약관을 참고하십시오.Wikipedia®는 미국 및 다른 국가에 등록되어 있는Wikimedia Foundation, Inc.소유의 등록 상표입니다.개인정보처리방침위키백과 소개면책 조항행동 강령개발자통계쿠키 정책모바일 보기내용 폭 제한 전환', 'sr-cyrlЕнтропија (теорија информација) — ВикипедијаПређи на садржајГлавни мениГлавни менипомери на странусакријНавигацијаГлавна странаСадржајСкорашње изменеСлучајна страницаАктуелностиКонтактДонацијеИнтеракцијаПомоћНаучите да уређујетеТргРадионицаОгласна таблаОтпреми датотекуПретрагаПретражиОтвори налогПријави меЛичне алаткеОтвори налогПријави меСтранице за одјављене уредникедетаљнијеДоприносиРазговор[одбаци]Придружите сеАкцији писања биографија 4.Придружите сеАкцији сређивања спискова заштићених природних добара Србије.Придружите сеАкцији писања чланака о женама.У току јетакмичење ЦЕЕ пролеће 2024.Отворена јерегистрација за конференцију Викилајв 2024.Садржајпомери на странусакријПочетак1ДефиницијаToggle Дефиниција subsection1.1Елерманова дефиниција2Интерпретација3Види још4Референце5Литература6Спољашње везеПрикажи/сакриј садржајЕнтропија (теорија информација)45 језикаAfrikaansالعربيةBahasa IndonesiaБългарскиBoarischBosanskiCatalàCymraegČeštinaDanskDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGalegoעבריתItaliano日本語한국어LietuviųMagyarNederlandsОлык марийਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийShqipSimple EnglishSlovenčinaSlovenščinaکوردیSundaSvenskaไทยTiếng ViệtTürkçeУкраїнськаاردو中文粵語Уреди везеЧланакРазговорЋир./lat.Ћир./lat.ЋирилицаLatinicaЧитајУредиУреди изворИсторијаАлаткеАлаткепомери на странусакријРадњеЧитајУредиУреди изворИсторијаОпштеШта води овамоПовезане изменеОтпреми датотекуПосебне страницеТрајна везаПодаци о странициЦитирај страницуКратки URLПреузми QR кодСтавка на ВикиподацимаШтампање/извозПреузми у PDF-уОдштампајНа другим пројектимаВикиоставаС Википедије, слободне енциклопедијеДва бита ентропије: У случају два бацања новчића, информациона ентропија у битима је логаритам основе-2 броја могућих исхода; са два новчића постоје четири могућа исхода, и два бита ентропије.', 'sr-cyrlГенерално, информациона ентропија је просечна количина информације пренесена догађајем, имајући у виду све могуће исходе.У теорији информације,ентропијаје мера неодређености придруженаслучајној променљивој.', 'sr-cyrlУ овом контексту, обично се мисли на Шенонову ентропију, која квантификује очекивану вредност информације садржане у поруци, обично у јединицама као што су бити.', 'sr-cyrlЕквивалентно, Шенонова ентропија је мера просечног информационог садржаја који се пропушта када се не зна вредност случајне променљиве.', 'sr-cyrlКонцепт је увеоКлод Шенону свом чувеном раду из1948.', 'sr-cyrlгодине „Математичка теорија комуникација“.', 'sr-cyrl[1]Шенонова ентропија представља апсолутну границу најбоље могуће компресије без губитака било какве комуникације, под извесним ограничењима: ако третирамо поруке да су кодоване као низ независних случајних променљивих са истом расподелом,прва Шенонова теоремапоказује да, у граничном случају, средња дужина најкраће могуће репрезентације за кодовање порука у датом алфабету је њихова ентропија подељена са логаритмом броја симбола у циљном алфабету.Једно бацање фер новчића носи ентропију од једногбита.', 'sr-cyrlДва бацања - два бита.', 'sr-cyrlБрзина ентропије за новчић је један бит по бацању.', 'sr-cyrlМеђутим, ако новчић није фер, тада је неодређеност мања (ако бисмо морали да се кладимо на исход наредног покушаја, вероватно ћемо се кладити на чешћи резултат), па је и Шенонова ентропија мања.', 'sr-cyrlМатематички, једно бацање новчића (фер или не) представљаБернулијевексперимент, а његова ентропија дата је бинарном ентропијском функцијом.', 'sr-cyrlНиз бацања новчића са две главе имаће нулту ентропију, јер су исходи потпуно предвидљиви.', 'sr-cyrlБрзина ентропије текста на енглеском је од 1 до 1,5 бита по слову, односно од 0,6 до 1,3 бита по слову, према проценама базираним на експериментима.', 'sr-cyrl[2][3]Мера информационе ентропије асоциране са сваком могућом вредношћу података је негативнилогаритамфункције вероватноће масевредности:S=−∑iPilog\\u2061PiS=-\\\\sum _P_\\\\log }.', 'sr-cyrl[4]Када извор података има вредност ниже вероватноће (тј., кад се јави догађај мање вероватноће), догађај носи више „информација”, него кад извор података има вредност веће вероватноће.', 'sr-cyrlКоличина информације пресене сваким догађајем на овај начин постајеслучајна променљивачија очекивана вредност је информациона ентропија.', 'sr-cyrlГенерално,ентропијасе односи на неред или несигурност, и дефиниција ентропије која се користи у информационој теорији је директно аналогнадефиницијикориштеној устатистичкој термодинамици.Основни модел системапреноса податакасе састоји од три елемента, извора података,комуникацијског канала, и примаоца, и према Шанону „фундаментални проблем комуникације” је за примаоца да може да идентификује који подаци су генерисани у извору на бази сигнала који су примљени кроз канал.', 'sr-cyrl[5]\\u200d:379–423 и 623–656Ентропија пружа апсолутни лимит најкраће могуће просечне дужинебезгубитачнекомпресијекодирања података произведених у извору, и ако је ентропија извора мања одканалног капацитетакомуникационог канала, подаци генерисани у извору се могу поздано пренети до примаоца (бар у теорији, евентуално занемарујући неке практичне аспекте, као што је сложеност система потребног за пренос података и времена које може бити неопходно).Информациона ентропија се типично мери убитовима(алтернативно званим „шенони”) или понекад у „природним јединицама” (натови) или децималним бројевима (званим „дитови”, „банови”, или „хартлији”).', 'sr-cyrlЈединица мера зависи од базе логаритма који се користи за дефинисање ентропије.Логаритам вероватноће је користан као мера ентропије јер је адитиван за независне изворе.', 'sr-cyrlНа пример, ентропија бацања новчића је један бит, и ентропијаmбацања јеmбитова.', 'sr-cyrlУ једноставном приказу,log2(n)битова је потребно за приказивање променљиве која може да попримиnвредности, ако јеnстепен од 2.', 'sr-cyrlАко су те вредности једнако вероватне, ентропија (у битовима) је једнака том броју.', 'sr-cyrlАко је једна од вредности вероватнија од других, запажање да се та вредност јавила је мање информативна него да се неки мање заступљени исход одвио.', 'sr-cyrlКонсеквентно, ређи догађаји пружају више информација кад су уочени.', 'sr-cyrlПошто се уочавања мање вероватних догађаја дешавају ређе, нето ефекат је да је ентропија (која се сматра просечном информацијом) добијена из неуниформне дистрибуције података увек мања или једнака одlog2(n).', 'sr-cyrlЕнтропија је нула кад је извесно да ће се један исход догодити.', 'sr-cyrlЕнтропија квантификује ова разматрања када је позната дистрибуција вероватноће изворних података.Смисаоуоченог догађаја (смисаопоруке) није важан у дефиницији ентропије.', 'sr-cyrlЕнтропија једино узима у обзир вероватноћу уочавања специфичног догађаја, тако да информација коју она садржи представља информацију о исходишној дистрибуцији вероватноће, а не о значењу самих догађаја.Дефиниција[уреди|уреди извор]Шенон је дефинисао ентропијуHдискретног извора без меморије (дискретне случајне променљиве)Xнад коначним алфабетомZ=,z_,\\\\dots ,z_\\\\}}на следећи начин.', 'sr-cyrlПрво се додели свакојвероватноћиpнеког догађаја њена количина информацијеI(p)=−log2\\u2061pp}.', 'sr-cyrlТада је ентропија по симболу дефинисана као очекивана вредност (математичко очекивање) количине информацијаH1=∑z∈Zpz⋅I(pz)=−∑z∈Zpz⋅log2\\u2061pz=\\\\sum _p_\\\\cdot I(p_)=-\\\\sum _p_\\\\cdot \\\\log _p_},Нека јеz∈Z, тада јеpz=P(X=z)=P(X=z)}вероватноћа да се догоди симболzиз алфабета, или еквивалентно[6]\\u200d:11[7]\\u200d:19–20(1)H1=−∑i=1mpi⋅log2\\u2061pi=-\\\\sum _^p_\\\\cdot \\\\log _p_},За граничну вредностlimpi→0pi⋅log2\\u2061pi\\\\rightarrow 0}p_\\\\cdot \\\\log _p_}се може показати да је нула.', 'sr-cyrlПрема томе, сабирци са вероватноћом једнаком нули не доприносе укупном збиру (тј.', 'sr-cyrlентропији).ЕнтропијаHn}за речиwдужинеnје дата саHn=−∑w∈Znpw⋅log2\\u2061pw=-\\\\sum _}p_\\\\cdot \\\\log _p_},где јеpw=P(X=w)=P(X=w)}вероватноћа да ће се догодити речw.', 'sr-cyrlЕнтропијаHје тада гранична вредност:(2)H=limn→∞Hnn}}}.Ако су симболи статистички независни, тадаHn=nH1=nH_}за свакоn, као иH=H1}.Елерманова дефиниција[уреди|уреди извор]Дејвид Елерманје желео да објасни заштоусловна ентропијаи друге функције имају својства слична функцијама у теорији вероватноће.', 'sr-cyrlОн тврди да су претходне дефиниције засноване на теорији мера функционисале само са степеном 2.', 'sr-cyrl[8]Интерпретација[уреди|уреди извор]Ентропија представља меру просечног информационог садржаја по симболу извора који представља неки систем или информациону секвенцу.', 'sr-cyrlУ теорији информација, као мера за количину информације у некој поруци може послужити колико се променилавероватноћадогађаја под утицајем те поруке.', 'sr-cyrlНа пример, ако за време лета временска прогноза за сутрашњи дан најави 30° C, то нам неће донети много информација, јер не садржи ништа неочекивано.', 'sr-cyrlМеђутим, ако се иста таква прогноза догоди зими, то представља потпуно неочекивану вест и самим тим садржи много информација.', 'sr-cyrlПод информацијом се може сматрати и мера уклоњене несигурности.', 'sr-cyrlШто се више симбола прими од извора, добијамо више информација и опада несигурност око онога шта је могло бити послато.Дефиниција информационог садржаја се може на следећи начин описати: ако се деси догађај чија је вероватноћаpi}, тада је кроз то изабран један конкретан догађај из хипотетичког скупа од1pi}}}једнако вероватних догађаја.', 'sr-cyrlДа би се ови догађаји међусобно разликовали, потребно им је доделитиlog2\\u2061(1pi)=−log2\\u2061(pi)\\\\left(}}\\\\right)=-\\\\log _(p_)}бита.', 'sr-cyrlОва вредност представља количину информације једног одређеног догађаја у битима.', 'sr-cyrlКада се количина информације сваког догађаја пондерише са вероватноћом истог догађаја, и када се саберу (тј.', 'sr-cyrlпронађе се математичко очекивање количине информације), добијамо средњу или очекивану количину информације по симболу.Јединица Шенон дефинише количину информације коју садржи порука о догађају са вероватноћомp=0,5.', 'sr-cyrlНа пример, саопштење о томе да је метални новчић пао на одређену страну доноси информацију од 1 Шенона.', 'sr-cyrlИли, ако на пријемној странибинарногтелекомуникационог система, чији је сигнал састављен од биполарних импулса константне амплитуде, меримо поларност долазећих импулса, и ако је вероватноћа појављивања позитивних и негативних амплитуда једнака, онда сваки импулс носи информацију од 1 Шенона.', 'sr-cyrlМеђутим, ако вероватноће појављивања позитивних и негативних амплитуда нису једнаке, онда један импулс доноси пријемнику неку другу количину информација која није 1 Шенон.', 'sr-cyrlТреба разликовати појамбинарни дигит, скраћенобит, од појмаШенон.', 'sr-cyrlБит је само носилац информације и физички је представљен импулсом који може да има два стања.', 'sr-cyrlУ зависности од вероватноће појављивања стања, бит може да доноси пријемнику већу или мању количину информација.', 'sr-cyrlСамо кад су вероватноће појављивања два стања код бита једнаке, онда бит носи количину информације од 1 Шенона.', 'sr-cyrl[9]Види још[уреди|уреди извор]ЕнтропијаРеференце[уреди|уреди извор]^Shannon, Claude E.(1948).', 'sr-cyrl„A Mathematical Theory of Communication”.Bell System Technical Journal.27(3): 379—423.doi:10.1002/j.1538-7305.1948.tb01338.x.hdl:11858/00-001M-0000-002C-4314-2.', 'sr-cyrl(PDF, archived fromhere)^Schneier, B:Applied Cryptography, Second edition, page 234.', 'sr-cyrlJohn Wiley and Sons.^Shannon, Claude E.: Prediction and entropy of printed English,The Bell System Technical Journal, 30:50-64, January 1951.^Pathria 2011, стр.', 'sr-cyrl51harvnb грешка: no target: CITEREFPathria2011 (help)^Shannon, Claude E. (1948).„A Mathematical Theory of Communication”(PDF).Bell System Technical Journal.27., July and October^Borda, Monica (2011).Fundamentals in Information Theory and Coding.', 'sr-cyrlSpringer.ISBN978-3-642-20346-6.^Han, Te Sun; Kobayashi, Kingo (2002).Mathematics of Information and Coding.', 'sr-cyrlAmerican Mathematical Society.ISBN978-0-8218-4256-0.^Ellerman, David (октобар 2017).„Logical Information Theory: New Logical Foundations for Information Theory”(PDF).Logic Journal of the IGPL.25(5): 806—835.doi:10.1093/jigpal/jzx022.', 'sr-cyrlПриступљено2.', 'sr-cyrl11.', 'sr-cyrl2022.CS1 одржавање: Формат датума (веза)^Лукатела, Г:Статистичка теорија телекомуникација и теорија информација, pp.', 'sr-cyrl258-259.', 'sr-cyrlГрађевинска књига, Београд, 1981.Литература[уреди|уреди извор]Han, Te Sun; Kobayashi, Kingo (2002).Mathematics of Information and Coding.', 'sr-cyrlAmerican Mathematical Society.ISBN978-0-8218-4256-0.Arndt, C (2004).Information Measures: Information and its Description in Science and Engineering.', 'sr-cyrlSpringer.ISBN978-3-540-40855-0.Borda, Monica (2011).Fundamentals in Information Theory and Coding.', 'sr-cyrlSpringer.ISBN978-3-642-20346-6.Pathria, R. K.; Beale, Paul (2011).Statistical Mechanics (Third Edition).', 'sr-cyrlAcademic Press.', 'sr-cyrlстр.', 'sr-cyrl51.ISBN978-0123821881.Cover, T. M., Thomas, J. A.Elements of information theory, Wiley-Interscience.', 'sr-cyrl(2nd изд.).', 'sr-cyrl2006.ISBN978-0-471-24195-9..Gray, R. M. (2011),Entropy and Information Theory, Springer.MacKay, David J.', 'sr-cyrlC..Information Theory, Inference, and Learning AlgorithmsCambridge.. Cambridge University Press.', 'sr-cyrl2003.ISBN978-0-521-64298-9.Pathria, R. K.; Beale, Paul (2011).Statistical Mechanics (Third Edition).', 'sr-cyrlAcademic Press.ISBN978-0123821881.Martin, Nathaniel F.G.; England, James W. (2011).Mathematical Theory of Entropy.', 'sr-cyrlCambridge University Press.ISBN978-0-521-17738-2.Shannon, C.E.,Weaver, W.(1949).The Mathematical Theory of Communication.', 'sr-cyrlUniversity of Illinois Press.ISBN978-0-252-72548-7.CS1 одржавање: Вишеструка имена: списак аутора (веза)Stone, J. V. Chapter 1 ofInformation Theory: A Tutorial Introduction, University of Sheffield, England.', 'sr-cyrl2014.ISBN978-0956372857..Спољашње везе[уреди|уреди извор]ЕнтропијанаВикимедијиној остави.Hazewinkel Michiel, ур.', 'sr-cyrl(2001).', 'sr-cyrl„Entropy”.Encyclopaedia of Mathematics.', 'sr-cyrlSpringer.ISBN978-1556080104.Introduction to entropy and informationonPrincipia Cybernetica WebEntropyan interdisciplinary journal on all aspects of the entropy concept.', 'sr-cyrlOpen access.Description of information entropy from \"Tools for Thought\" by Howard RheingoldA java applet representing Shannon\\'s Experiment to Calculate the Entropy of EnglishSlides on information gain and entropyAn Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science– a wikibook on the interpretation of the concept of entropy.Network Event Detection With Entropy Measures, Dr. Raimund Eimann, University of Auckland, PDF; 5993 kB – a PhD thesis demonstrating how entropy measures may be used in network anomaly detection.Rosetta Coderepository of implementations of Shannon entropy in different programming languages.', 'sr-cyrlInformation Theory for Intelligent PeopleАрхивиранона сајтуWayback Machine(13. јун 2020).', 'sr-cyrlShort introduction to the axioms of information theory, entropy, mutual information, Kullback–Liebler divergence, and Jensen–Shannon distance.Online tool for calculating entropy (plain text)Online tool for calculating entropy (binary)Нормативна контролаМеђународнеFASTДржавнеШпанијаФранцускаBnF подациНемачкаИзраелСједињене ДржавеЈапанЧешкаОсталеЕнциклопедија Британика2Преузето из „https://sr.wikipedia.org/w/index.php?title=Ентропија_(теорија_информација)&oldid=27557839”Категорија:Теорија информацијеСакривене категорије:Harv and Sfn no-target errorsCS1 одржавање: Формат датумаCite bookCS1 одржавање: Вишеструка имена: списак аутораКатегорија на Остави са локалним линком различитим од оног на ВикиподацимаЧланци са FAST идентификаторимаЧланци са BNE идентификаторимаЧланци са BNF идентификаторимаЧланци са BNFdata идентификаторимаЧланци са GND идентификаторимаЧланци са J9U идентификаторимаЧланци са LCCN идентификаторимаЧланци са NDL идентификаторимаЧланци са NKC идентификаторимаЧланци са EBO идентификаторимаДатум и време последње измене странице: 8. април 2024. у 00:10.Текст је доступан под лиценцомCreative Commons Ауторство—Делити под истим условима; могући су и додатни услови.', 'sr-cyrlПогледајтеуслове коришћењаза детаље.Политика приватностиО ВикипедијиОдрицање одговорностиКодекс понашањаЗа програмереСтатистикаИзјава о колачићимаМобилни приказСмањи ширину садржаја']\n"
     ]
    }
   ],
   "source": [
    "# Afficher les valeurs de la nouvelle colonne\n",
    "print(mon_nouveau_dataset['train']['nouvelle_colonne'])\n",
    "#print(mon_nouveau_dataset['nouvelle_colonne'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ee0f7-6463-4e10-b44c-30086a35aed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
